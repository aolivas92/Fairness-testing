{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../models/census/test.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0602 12:17:56.216112 140693737744192 saver.py:1399] Restoring parameters from ../models/census/test.model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Num Instances  0\n",
      "2\n",
      "Back\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "##import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf \n",
    "tf.compat.v1.disable_eager_execution()\n",
    "import sys, os\n",
    "sys.path.append(\"../\")\n",
    "import copy\n",
    "import pandas as pd\n",
    "from tensorflow.python.platform import flags\n",
    "from scipy.optimize import basinhopping\n",
    "import time\n",
    "from adf_data.census import census_data\n",
    "from adf_data.credit import credit_data\n",
    "from adf_data.bank import bank_data\n",
    "from adf_model.tutorial_models import dnn\n",
    "from adf_utils.utils_tf import model_prediction, model_argmax\n",
    "from adf_utils.config import census, credit, bank\n",
    "from adf_tutorial.utils import cluster, gradient_graph\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "#tot_labels=[]\n",
    "# step size of perturbation\n",
    "perturbation_size = 1\n",
    "\n",
    "def check_for_error_condition(conf, sess, x, preds, t, sens1, sens2, sens3):\n",
    "    global tot_labels\n",
    "    global loc_samples\n",
    "    \n",
    "    \"\"\"\n",
    "    Check whether the test case is an individual discriminatory instance\n",
    "    :param conf: the configuration of dataset\n",
    "    :param sess: TF session\n",
    "    :param x: input placeholder\n",
    "    :param preds: the model's symbolic output\n",
    "    :param t: test case\n",
    "    :param sens: the index of sensitive feature\n",
    "    :return: whether it is an individual discriminatory instance\n",
    "    \"\"\"\n",
    "    \n",
    "    t = [t.astype('int')]\n",
    "    \n",
    "    samples = m_instance( np.array(t), sens1, sens2, sens3 )\n",
    "    pred = pred_prob(sess, x, preds, samples )\n",
    "    partition = clustering(pred,samples, sens1, sens2, sens3)\n",
    "    dis_sample =t.copy()\n",
    "    dis_sample[0][sens1 - 1] = 0\n",
    "    dis_sample[0][sens2 - 1] = 0\n",
    "    dis_sample[0][sens3 - 1] = 0\n",
    "    if tuple(dis_sample[0]) not in loc_samples:\n",
    "            \n",
    "        label = model_argmax(sess, x, preds, np.array(t))\n",
    "        labels = pred_prob_train(sess, x, preds, samples)\n",
    "        \n",
    "        if (1 - label) in labels:\n",
    "            tot_labels+=1\n",
    "            loc_samples.add(tuple(dis_sample[0]))\n",
    "   \n",
    "    return max(list(partition.keys())[1:]) - min(list(partition.keys())[1:])#(len(partition) -1)\n",
    "    \n",
    "def seed_test_input(clusters, limit):\n",
    "    \"\"\"\n",
    "    Select the seed inputs for fairness testing\n",
    "    :param clusters: the results of K-means clustering\n",
    "    :param limit: the size of seed inputs wanted\n",
    "    :return: a sequence of seed inputs\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    rows = []\n",
    "    max_size = max([len(c[0]) for c in clusters])\n",
    "    while i < max_size:\n",
    "        if len(rows) == limit:\n",
    "            break\n",
    "        for c in clusters:\n",
    "            if i >= len(c[0]):\n",
    "                continue\n",
    "            row = c[0][i]\n",
    "            rows.append(row)\n",
    "            if len(rows) == limit:\n",
    "                break\n",
    "        i += 1\n",
    "    return np.array(rows)\n",
    "\n",
    "def clip(input, conf):\n",
    "    \"\"\"\n",
    "    Clip the generating instance with each feature to make sure it is valid\n",
    "    :param input: generating instance\n",
    "    :param conf: the configuration of dataset\n",
    "    :return: a valid generating instance\n",
    "    \"\"\"\n",
    "    for i in range(len(input)):\n",
    "        input[i] = max(input[i], conf.input_bounds[i][0])\n",
    "        input[i] = min(input[i], conf.input_bounds[i][1])\n",
    "    return input\n",
    "\n",
    "class Local_Perturbation(object):\n",
    "    \"\"\"\n",
    "    The  implementation of local perturbation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sess, grad, x, n_value1, n_value2, n_value3, sens1, sens2, sens3, input_shape, conf):\n",
    "        \"\"\"\n",
    "        Initial function of local perturbation\n",
    "        :param sess: TF session\n",
    "        :param grad: the gradient graph\n",
    "        :param x: input placeholder\n",
    "        :param n_value: the discriminatory value of sensitive feature\n",
    "        :param sens_param: the index of sensitive feature\n",
    "        :param input_shape: the shape of dataset\n",
    "        :param conf: the configuration of dataset\n",
    "        \"\"\"\n",
    "        self.sess = sess\n",
    "        self.grad = grad\n",
    "        self.x = x\n",
    "        self.n_value1 = n_value1\n",
    "        self.n_value2 = n_value2\n",
    "        self.n_value3 = n_value3\n",
    "        self.input_shape = input_shape\n",
    "        self.sens1 = sens1\n",
    "        self.sens2 = sens2\n",
    "        self.sens3 = sens3\n",
    "        self.conf = conf\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Local perturbation\n",
    "        :param x: input instance for local perturbation\n",
    "        :return: new potential individual discriminatory instance\n",
    "        \"\"\"\n",
    "\n",
    "        # perturbation\n",
    "        s = np.random.choice([1.0, -1.0]) * perturbation_size\n",
    "\n",
    "        n_x = x.copy()\n",
    "        n_x[self.sens1 - 1] = self.n_value1\n",
    "        n_x[self.sens2 - 1] = self.n_value2\n",
    "        n_x[self.sens3 - 1] = self.n_value3\n",
    "        \n",
    "\n",
    "        # compute the gradients of an individual discriminatory instance pairs\n",
    "        ind_grad = self.sess.run(self.grad, feed_dict={self.x:np.array([x])})\n",
    "        n_ind_grad = self.sess.run(self.grad, feed_dict={self.x:np.array([n_x])})\n",
    "\n",
    "        if np.zeros(self.input_shape).tolist() == ind_grad[0].tolist() and np.zeros(self.input_shape).tolist() == \\\n",
    "                n_ind_grad[0].tolist():\n",
    "            probs = 1.0 / (self.input_shape-1) * np.ones(self.input_shape)\n",
    "            probs[self.sens1 - 1] = 0\n",
    "            probs[self.sens2 - 1] = 0\n",
    "            probs[self.sens3 - 1] = 0\n",
    "        else:\n",
    "            # nomalize the reciprocal of gradients (prefer the low impactful feature)\n",
    "            grad_sum = 1.0 / (abs(ind_grad[0]) + abs(n_ind_grad[0]))\n",
    "            grad_sum[self.sens1 - 1] = 0\n",
    "            grad_sum[self.sens2 - 1] = 0\n",
    "            grad_sum[self.sens3 - 1] = 0\n",
    "            \n",
    "            probs = grad_sum / np.sum(grad_sum)\n",
    "        probs = probs/probs.sum()\n",
    "\n",
    "        # randomly choose the feature for local perturbation\n",
    "        index = np.random.choice(range(self.input_shape) , p=probs)\n",
    "        local_cal_grad = np.zeros(self.input_shape)\n",
    "        local_cal_grad[index] = 1.0\n",
    "\n",
    "        x = clip(x + s * local_cal_grad, self.conf).astype(\"int\")\n",
    "\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "#--------------------------------------\n",
    "def m_instance(sample, sensitive_param1, sensitive_param2, sensitive_param3):\n",
    "    #global m_sample\n",
    "    #global index\n",
    "    index = []\n",
    "    m_sample = []\n",
    "    \n",
    "    for i in range(census.input_bounds[sensitive_param1 - 1][0], census.input_bounds[sensitive_param1 -1 ][1] + 1):\n",
    "            for j in range(census.input_bounds[sensitive_param2 - 1][0], census.input_bounds[sensitive_param2 -1 ][1] + 1):\n",
    "                for k in range(census.input_bounds[sensitive_param3 - 1][0], census.input_bounds[sensitive_param3 -1 ][1] + 1):\n",
    "                    index.append([i,j,k])\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "    \n",
    "    \n",
    "    for ind in range(len(index)):\n",
    "        temp=sample.copy()\n",
    "        temp[0][sensitive_param1-1]=index[ind][0]\n",
    "        temp[0][sensitive_param2-1]=index[ind][1]\n",
    "        temp[0][sensitive_param3-1]= index[ind][2]\n",
    "        m_sample.append(temp)\n",
    "    return np.array(m_sample)\n",
    "\n",
    "def global_sample_select(clus_dic, sens1, sens2, sens3):\n",
    "    leng = 0\n",
    "    for key in clus_dic.keys():\n",
    "        if key == 'Seed':\n",
    "            continue\n",
    "        if len(clus_dic[key]) > leng:\n",
    "            leng = len(clus_dic[key])\n",
    "            largest = key\n",
    "    \n",
    "    sample_ind = np.random.randint(len(clus_dic[largest]))\n",
    "    n_sample_ind = np.random.randint(len(clus_dic[largest]))\n",
    "    \n",
    "    sample = clus_dic['Seed']\n",
    "    sample[sens3 -1] = clus_dic[largest][sample_ind][0]\n",
    "    sample[sens2 -1] = clus_dic[largest][sample_ind][1]\n",
    "    sample[sens1 -1] = clus_dic[largest][sample_ind][2]\n",
    "    \n",
    "    # returns one sample of largest partition and its pair\n",
    "    return np.array([sample]),clus_dic[largest][n_sample_ind]\n",
    "\n",
    "def local_sample_select(clus_dic, sens1, sens2, sens3):\n",
    "      \n",
    "    k_1 = min(list(clus_dic.keys())[1:])\n",
    "    k_2 = max(list(clus_dic.keys())[1:])\n",
    "    \n",
    "    sample_ind = np.random.randint(len(clus_dic[k_1]))\n",
    "    n_sample_ind = np.random.randint(len(clus_dic[k_2]))\n",
    "\n",
    "    sample = clus_dic['Seed']\n",
    "    sample[sens3 -1] = clus_dic[k_1][sample_ind][0]\n",
    "    sample[sens2 -1] = clus_dic[k_1][sample_ind][1]\n",
    "    sample[sens1 -1] = clus_dic[k_1][sample_ind][2]\n",
    "\n",
    "\n",
    "    return np.array([sample]),clus_dic[k_2][n_sample_ind]\n",
    "    \n",
    "def clustering(probs,m_sample, sens1, sens2, sens3):\n",
    "    epsillon=0.025\n",
    "    cluster_dic = {}\n",
    "    cluster_dic['Seed']=m_sample[0][0]\n",
    "    \n",
    "        \n",
    "    for i in range(len(probs)):\n",
    "        #  to avoid k = 11\n",
    "        if probs[i] == 1.0:\n",
    "            if (int( probs[i] / epsillon ) -1) not in cluster_dic.keys():\n",
    "                cluster_dic[ (int( probs[i] / epsillon ) -1)] = [ [m_sample[i][0][sens3 - 1]] + [ m_sample[i][0][sens2 - 1] ] + [ m_sample[i][0][sens1 - 1] ] ]\n",
    "           \n",
    "            else:\n",
    "                cluster_dic[ (int( probs[i] / epsillon ) -1)].append( [m_sample[i][0][sens3 - 1]] + [ m_sample[i][0][sens2 - 1] ] + [ m_sample[i][0][sens1 - 1] ]  )\n",
    "\n",
    "            \n",
    "           \n",
    "        elif int( probs[i] / epsillon ) not in cluster_dic.keys():\n",
    "                cluster_dic[ int( probs[i] / epsillon )] = [ [m_sample[i][0][sens3 - 1]] + [ m_sample[i][0][sens2 - 1] ] + [ m_sample[i][0][sens1 - 1] ] ]\n",
    "           \n",
    "        else:\n",
    "                cluster_dic[ int( probs[i] / epsillon)].append( [m_sample[i][0][sens3 - 1]] + [ m_sample[i][0][sens2 - 1] ] + [ m_sample[i][0][sens1 - 1] ]  )\n",
    "\n",
    "    return cluster_dic  \n",
    "\n",
    "def pred_prob_train(sess, x, preds, m_sample):\n",
    "        \n",
    "        labels= []\n",
    "        for sample in m_sample:\n",
    "            \n",
    "            pred = model_prediction(sess, x, preds, np.array(sample))\n",
    "            label= np.argmax(pred, axis=1)\n",
    "            labels.append(list(label)[0])\n",
    "            \n",
    "            #print('probs',probs[0])\n",
    "        return labels\n",
    "    \n",
    "def pred_prob(sess, x, preds, m_sample):\n",
    "        global probs\n",
    "        probs = []\n",
    "        for sample in m_sample:\n",
    "            \n",
    "            pred = model_prediction(sess, x, preds, np.array(sample))[0][1]\n",
    "            \n",
    "            probs.append(pred)\n",
    "            #print('probs',probs[0])\n",
    "        return probs \n",
    "#-------------------------------------------\n",
    "    \n",
    "def dnn_fair_testing(dataset, sensitive_param, sensitive_param2, sensitive_param3, model_path, cluster_num, max_global, max_local, max_iter):\n",
    "    \"\"\"\n",
    "    The implementation of ADF\n",
    "    :param dataset: the name of testing dataset\n",
    "    :param sensitive_param: the index of sensitive feature\n",
    "    :param model_path: the path of testing model\n",
    "    :param cluster_num: the number of clusters to form as well as the number of\n",
    "            centroids to generate\n",
    "    :param max_global: the maximum number of samples for global search\n",
    "    :param max_local: the maximum number of samples for local search\n",
    "    :param max_iter: the maximum iteration of global perturbation\n",
    "    \"\"\"\n",
    "    data = {\"census\":census_data, \"credit\":credit_data, \"bank\":bank_data}\n",
    "    data_config = {\"census\":census, \"credit\":credit, \"bank\":bank}\n",
    "    # prepare the testing data and model\n",
    "    X, Y, input_shape, nb_classes = data[dataset]()\n",
    "    tf.set_random_seed(1234)\n",
    "    \n",
    "    #config = tf.ConfigProto()\n",
    "    #config.gpu_options.per_process_gpu_memory_fraction = 0.8\n",
    "    config = tf.ConfigProto(device_count = {'GPU': 0})\n",
    "    config.allow_soft_placement= True\n",
    "    \n",
    "    sess = tf.Session(config=config)\n",
    "    x = tf.placeholder(tf.float32, shape=input_shape)\n",
    "    y = tf.placeholder(tf.float32, shape=(None, nb_classes))\n",
    "    model = dnn(input_shape, nb_classes)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    preds = model(x)\n",
    "    saver = tf.train.Saver()\n",
    "    model_path = model_path + dataset + \"/test.model\"\n",
    "    saver.restore(sess, model_path)\n",
    "    \n",
    "    # construct the gradient graph\n",
    "    grad_0 = gradient_graph(x, preds)\n",
    "\n",
    "    # build the clustering model\n",
    "    clf = cluster(dataset, cluster_num)\n",
    "    clusters = [np.where(clf.labels_==i) for i in range(cluster_num)]\n",
    "\n",
    "    # store the result of fairness testing\n",
    "    tot_inputs = set()\n",
    "    global_disc_inputs = set()\n",
    "    global_disc_inputs_list = []\n",
    "    local_disc_inputs = set()\n",
    "    local_disc_inputs_list = []\n",
    "    value_list = []\n",
    "    global training_data\n",
    "    global tot_labels\n",
    "    global loc_samples\n",
    "    training_data=set()\n",
    "    loc_samples=set()\n",
    "    tot_labels = 0\n",
    "    suc_idx = []\n",
    "    global training_df\n",
    "    training_df= pd.DataFrame(columns=['sample','#K', 'max_num_elem', 'min_max_dist', 'initial K', 'label' ])\n",
    "    #-----------------------\n",
    "    def print_fun(x, f, accepted):\n",
    "\n",
    "        print(\"at minimum %f accepted %d\" % (-1 * f, int(accepted)))\n",
    "    def evaluate_local(inp):\n",
    "        \n",
    "        \"\"\"\n",
    "        Evaluate whether the test input after local perturbation is an individual discriminatory instance\n",
    "        :param inp: test input\n",
    "        :return: whether it is an individual discriminatory instance\n",
    "        \"\"\"\n",
    "        \n",
    "        result = check_for_error_condition(data_config[dataset], sess, x, preds, inp, \n",
    "                                           sensitive_param, sensitive_param2, sensitive_param3)\n",
    "\n",
    "        return (-1 * result)\n",
    "\n",
    "    # select the seed input for fairness testing\n",
    "    inputs = seed_test_input(clusters, min(max_global, len(X)))\n",
    "    num_seed=0\n",
    "    #time4=time.time()\n",
    "    for num in range(len(inputs)):\n",
    "        #time1=time.time()\n",
    "        num_seed+=1\n",
    "        print(num_seed)\n",
    "        #if num_seed > 6: break\n",
    "      \n",
    "        index = inputs[num]\n",
    "        global clus_dic\n",
    "        global list_dic\n",
    "        sample = X[index:index+1]       \n",
    "        #---------------------------   \n",
    "        m_sample = m_instance( sample, sensitive_param, sensitive_param2, sensitive_param3 )        \n",
    "        pred = pred_prob(sess, x, preds, m_sample )             \n",
    "        clus_dic = clustering( pred, m_sample, sensitive_param, sensitive_param2, sensitive_param3 )         \n",
    "        init_k = len( clus_dic ) - 1     \n",
    "        #print( 'ini K-->', init_k )           \n",
    "        list_dic = []\n",
    "        max_k = init_k\n",
    "        max_k_iter = 30\n",
    "        #----------------------------\n",
    "        # start global perturbation\n",
    "        for iter in range( max_iter + 1 ):\n",
    "            #time1 = time.time()\n",
    "            m_sample = m_instance( sample , sensitive_param, sensitive_param2, sensitive_param3 )          \n",
    "            pred = pred_prob(sess, x, preds, m_sample )\n",
    "            clus_dic = clustering( pred, m_sample, sensitive_param, sensitive_param2, sensitive_param3 )            \n",
    "            cur_k = len( clus_dic ) - 1\n",
    "            if len( clus_dic ) - 1 >= max_k:\n",
    "                max_k = len( clus_dic ) - 1\n",
    "                max_sample = sample.copy()\n",
    "\n",
    "            sample,n_values = global_sample_select( clus_dic, sensitive_param, sensitive_param2, sensitive_param3 )\n",
    "\n",
    "            if len(clus_dic)-1 >= 2:\n",
    "\n",
    "                loc_x,n_values = local_sample_select(clus_dic ,sensitive_param, sensitive_param2, sensitive_param3 )                                \n",
    "                n_value1 = n_values[2]\n",
    "                n_value2 = n_values[1]\n",
    "                n_value3 = n_values[0]\n",
    "                           \n",
    "                minimizer = {\"method\": \"L-BFGS-B\"}\n",
    "                #minimizer = {\"method\":\"L-BFGS-B\", \"jac\":True}\n",
    "                #minimizer = {\"method\": \"SLSQP\"}\n",
    "                #minimizer = {'method':'nelder-mead'}\n",
    "                local_perturbation = Local_Perturbation(sess, grad_0, x, n_value1, n_value2, n_value3, \n",
    "                                                        sensitive_param, sensitive_param2,\n",
    "                                                        sensitive_param3, input_shape[1], \n",
    "                                                        data_config[dataset])\n",
    "                \n",
    "                basinhopping(evaluate_local, loc_x,stepsize=1.0, take_step=local_perturbation, \n",
    "                              minimizer_kwargs=minimizer, niter=max_local)#callback=print_fun, niter=max_local)\n",
    "\n",
    "                \n",
    "                dis_sample =sample.copy()\n",
    "                dis_sample[0][sensitive_param  - 1] = 0\n",
    "                dis_sample[0][sensitive_param2 - 1] = 0\n",
    "                dis_sample[0][sensitive_param3 - 1] = 0\n",
    "                if tuple(dis_sample[0].astype('int')) not in global_disc_inputs:\n",
    "\n",
    "                    num_disc+= tot_labels  \n",
    "                    min_max_dist = max(list(clus_dic.keys())[1:]) - min(list(clus_dic.keys())[1:])\n",
    "                    max_num_elem = 0\n",
    "                    for k in list(clus_dic.keys())[1:]:\n",
    "                        if len(clus_dic[k]) > max_num_elem:\n",
    "                            max_num_elem = len(clus_dic[k]) \n",
    "                    \n",
    "                    global_disc_inputs.add(tuple(dis_sample[0].astype('int')))\n",
    "                    training_df = training_df.append({'sample': tuple(dis_sample[0].astype('int')), \n",
    "                                                      '#K': len( clus_dic ) - 1, \n",
    "                                                      'min_max_dist':min_max_dist,\n",
    "                                                      'max_num_elem':max_num_elem,\n",
    "                                                      'initial K':init_k,\n",
    "                                                      'label': num_disc}, ignore_index=True)\n",
    "                \n",
    "            \n",
    "            num_disc = 0    \n",
    "            tot_labels = 0\n",
    "            list_dic.append(clus_dic)\n",
    "            clus_dic={}\n",
    "\n",
    "            if iter == max_iter:\n",
    "                break\n",
    "            if iter >= max_k_iter:\n",
    "                if init_k>=max_k:\n",
    "                    #print('break')\n",
    "                    break\n",
    "            if cur_k < max_k -5:\n",
    "                print('Back')\n",
    "                sample = max_sample.copy()\n",
    "            \n",
    "            #Making up n_sample\n",
    "            n_sample = sample.copy()\n",
    "            n_sample[0][sensitive_param3 - 1] = n_values[0]\n",
    "            n_sample[0][sensitive_param2 - 1] = n_values[1]\n",
    "            n_sample[0][sensitive_param  - 1] = n_values[2]\n",
    "            \n",
    "            \n",
    "            # global perturbation\n",
    "            s_grad = sess.run(tf.sign(grad_0), feed_dict={x: sample})\n",
    "            n_grad = sess.run(tf.sign(grad_0), feed_dict={x: n_sample})\n",
    "            # find the feature with same impact\n",
    "            if np.zeros(data_config[dataset].params).tolist() == s_grad[0].tolist():\n",
    "                g_diff = n_grad[0]\n",
    "            elif np.zeros(data_config[dataset].params).tolist() == n_grad[0].tolist():\n",
    "                g_diff = s_grad[0]\n",
    "            else:\n",
    "                g_diff = np.array(s_grad[0] == n_grad[0], dtype=float)\n",
    "            g_diff[sensitive_param - 1] = 0\n",
    "            g_diff[sensitive_param2 -1] = 0\n",
    "            g_diff[sensitive_param3 -1] = 0\n",
    "\n",
    "\n",
    "                \n",
    "              \n",
    "            cal_grad = s_grad * g_diff\n",
    "            if np.zeros(input_shape[1]).tolist() == cal_grad.tolist()[0]:\n",
    "                index  = np.random.randint(len(cal_grad[0]) - 1)\n",
    "                index1 = np.random.randint(len(cal_grad[0]) - 1)\n",
    "                index2 = np.random.randint(len(cal_grad[0]) - 1)\n",
    "                \n",
    "                if index == sensitive_param3 - 1:\n",
    "                    index = index + 1\n",
    "                if index == sensitive_param2 - 1:\n",
    "                    index = index + 1\n",
    "                if index == sensitive_param - 1:\n",
    "                    index = index + 1\n",
    "                \n",
    "                if index1 == sensitive_param3 - 1:\n",
    "                    index1 = index + 1\n",
    "                if index1 == sensitive_param2 - 1:\n",
    "                    index1 = index + 1\n",
    "                if index1 == sensitive_param - 1:\n",
    "                    index1 = index + 1\n",
    "                    \n",
    "                if index2 == sensitive_param3 - 1:\n",
    "                    index2 = index + 1\n",
    "                if index2 == sensitive_param2 - 1:\n",
    "                    index2 = index + 1\n",
    "                if index2 == sensitive_param - 1:\n",
    "                    index2 = index + 1\n",
    "                #print('s grad       ',s_grad)\n",
    "                cal_grad[0][index]  = np.random.choice([1.0, -1.0])\n",
    "                cal_grad[0][index1] = np.random.choice([1.0, -1.0])\n",
    "                cal_grad[0][index2] = np.random.choice([1.0, -1.0])\n",
    "\n",
    "            sample[0] = clip(sample[0] + perturbation_size * cal_grad[0], data_config[dataset]).astype(\"int\")\n",
    "            \n",
    "            #time2= time.time()\n",
    "            #print('Time ',time2 - time1)\n",
    "    \n",
    "      \n",
    "          \n",
    "        print('Num Instances ', len(training_df))\n",
    "        max_k=0\n",
    "        for k in list_dic:\n",
    "            if len(k)-1 >max_k:\n",
    "                max_k=len(k) -1 \n",
    "        #print('max k-->',max_k,'\\n')\n",
    "        \n",
    "        \n",
    "\n",
    "    training_df.to_csv('Traingni Data.csv')   \n",
    "        \n",
    "    # create the folder for storing the fairness testing result\n",
    "    if not os.path.exists('../results/'):\n",
    "        os.makedirs('../results/')\n",
    "    if not os.path.exists('../results/' + dataset + '/'):\n",
    "        os.makedirs('../results/' + dataset + '/')\n",
    "    if not os.path.exists('../results/'+ dataset + '/'+ str(sensitive_param) + '/'):\n",
    "        os.makedirs('../results/' + dataset + '/'+ str(sensitive_param) + '/')\n",
    "\n",
    "    # storing the fairness testing result\n",
    "    np.save('../results/'+dataset+'/'+ str(sensitive_param) + '/suc_idx.npy', np.array(suc_idx))\n",
    "    np.save('../results/'+dataset+'/'+ str(sensitive_param) + '/global_samples.npy', np.array(global_disc_inputs_list))\n",
    "    np.save('../results/'+dataset+'/'+ str(sensitive_param) + '/local_samples.npy', np.array(local_disc_inputs_list))\n",
    "    np.save('../results/'+dataset+'/'+ str(sensitive_param) + '/disc_value.npy', np.array(value_list))\n",
    "\n",
    "    # print the overview information of result\n",
    "    print(\"Total Inputs are \" + str(len(tot_inputs)))\n",
    "    print(\"Total discriminatory inputs of global search- \" + str(len(global_disc_inputs)))\n",
    "    print(\"Total discriminatory inputs of local search- \" + str(len(local_disc_inputs)))\n",
    "\n",
    "def main(argv=None):\n",
    "    dnn_fair_testing(dataset = FLAGS.dataset,\n",
    "                     sensitive_param = FLAGS.sens_param,\n",
    "                     sensitive_param2 = FLAGS.sens_param2,\n",
    "                     sensitive_param3 = FLAGS.sens_param3,\n",
    "                     \n",
    "  \n",
    "                     model_path = FLAGS.model_path,\n",
    "                     cluster_num=FLAGS.cluster_num,\n",
    "                     max_global=FLAGS.max_global,\n",
    "                     max_local=FLAGS.max_local,\n",
    "                     max_iter = FLAGS.max_iter)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    flags.DEFINE_string(\"dataset\", \"census\", \"the name of dataset\")\n",
    "    flags.DEFINE_integer('sens_param', 9, 'sensitive index, index start from 1 for age, 9 for gender, 8 for race')\n",
    "    flags.DEFINE_integer('sens_param2', 8, 'sensitive index, index start from 1 for age, 9 for gender, 8 for race')\n",
    "    flags.DEFINE_integer('sens_param3', 1, 'sensitive index, index start from 1 for age, 9 for gender, 8 for race')\n",
    "    flags.DEFINE_string('model_path', '../models/', 'the path for testing model')\n",
    "    flags.DEFINE_integer('cluster_num', 4, 'the number of clusters to form as well as the number of centroids to generate')\n",
    "    flags.DEFINE_integer('max_global', 1000, 'maximum number of samples for global search')#1000\n",
    "    flags.DEFINE_integer('max_local', 1000, 'maximum number of samples for local search')#1000\n",
    "    flags.DEFINE_integer('max_iter', 10, 'maximum iteration of global perturbation')\n",
    "\n",
    "    tf.app.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(loc_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 1.])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import tree\n",
    "\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "i = 0\n",
    "\n",
    "with open(\"../datasets/census\", \"r\") as ins:\n",
    "    for line in ins:\n",
    "        line = line.strip()\n",
    "        line1 = line.split(',')\n",
    "        if (i == 0):\n",
    "            i += 1\n",
    "            continue\n",
    "        # L = map(int, line1[:-1])\n",
    "        L = [int(i) for i in line1[:-1]]\n",
    "        X.append(L)\n",
    "        if int(line1[-1]) == 0:\n",
    "            Y.append([0])\n",
    "        else:\n",
    "            Y.append([1])\n",
    "X = np.array(X, dtype=float)\n",
    "Y = np.array(Y, dtype=float)\n",
    "model = tree.DecisionTreeClassifier()\n",
    "#model =  LogisticRegression(random_state =1)\n",
    "model.fit(X,Y)\n",
    "model.predict(X)\n",
    "tree.plot_tree(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process id 0 - Complete - Task time: 5s\n",
      "Process id 1 - Complete - Task time: 2s\n",
      "Process id 2 - Complete - Task time: 4s\n",
      "Process id 3 - Complete - Task time: 1s\n",
      "Process id 4 - Complete - Task time: 5s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from multiprocessing import Process, Queue, Event\n",
    "from queue import Empty\n",
    "from threading import Thread\n",
    "import time\n",
    "import pandas as pd\n",
    "import random\n",
    "import tempfile\n",
    "\n",
    "\n",
    "fake_runtimes = [5,2,4,1,5]\n",
    "# Simulate a long running process with a runtime that is variable\n",
    "def long_running_process(process_id: int, task_time: int):\n",
    "    time.sleep(task_time)\n",
    "    print(\"Process id {} - Complete - Task time: {}s\".format(process_id, task_time))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process id 0 - Complete - Task time: 5s\n",
      "Process id 1 - Complete - Task time: 2s\n",
      "Process id 2 - Complete - Task time: 4s\n",
      "Process id 3 - Complete - Task time: 1s\n",
      "Process id 4 - Complete - Task time: 5s\n",
      "17 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r1 -n1\n",
    "# Running this job 5 times in serial\n",
    "for i, task_time in zip(range(5), fake_runtimes):\n",
    "    long_running_process(i, task_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process id 3 - Complete - Task time: 1s\n",
      "Process id 1 - Complete - Task time: 2s\n",
      "Process id 2 - Complete - Task time: 4s\n",
      "Process id 0 - Complete - Task time: 5s\n",
      "Process id 4 - Complete - Task time: 5s\n",
      "5.14 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r1 -n1\n",
    "# Use multiple processes to do all 5 concurently\n",
    "processes = []\n",
    "for i, task_time in zip(range(5), fake_runtimes):\n",
    "    p = Process(target=long_running_process, args=(i, task_time))\n",
    "    p.start()\n",
    "    processes.append(p)\n",
    "\n",
    "for p in processes:\n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
