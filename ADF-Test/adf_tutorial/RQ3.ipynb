{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../models/census/test.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0722 11:40:30.492418 139931636864832 saver.py:1399] Restoring parameters from ../models/census/test.model\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "from itertools import product, combinations\n",
    "import tensorflow.compat.v1 as tf \n",
    "tf.compat.v1.disable_eager_execution()\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import copy\n",
    "import pandas as pd\n",
    "from tensorflow.python.platform import flags\n",
    "import time\n",
    "from adf_data.census import census_data\n",
    "from adf_data.credit import credit_data\n",
    "from adf_data.compas import compas_data\n",
    "from adf_data.default import default_data\n",
    "from adf_data.bank import bank_data\n",
    "from adf_data.heart import heart_data\n",
    "from adf_model.tutorial_models import dnn\n",
    "from adf_utils.utils_tf import model_prediction, model_argmax , layer_out\n",
    "from adf_utils.config import census, credit, bank, compas, default, heart\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "       \n",
    "def m_instance( sample, sens_params, conf):\n",
    "    index = []\n",
    "    m_sample = []\n",
    "    for sens in sens_params:\n",
    "        index.append([i for i in range(conf.input_bounds[sens-1][0], conf.input_bounds[sens-1][1]+1)])\n",
    "      \n",
    "    for ind in list(product(*index)):     \n",
    "        temp = sample.copy()\n",
    "        for i in range(len(sens_params)):\n",
    "            temp[0][sens_params[i]-1] = ind[i]\n",
    "        m_sample.append(temp)\n",
    "    return np.array(m_sample)\n",
    "    \n",
    "def clustering(probs,m_sample, sens_params):\n",
    "    epsillon=0.0125\n",
    "    cluster_dic = {}\n",
    "    cluster_dic['Seed']=m_sample[0][0]\n",
    "\n",
    "        #  to avoid k = Max + 1 \n",
    "    for i in range(len(probs)):\n",
    "    \n",
    "        if probs[i] == 1.0:\n",
    "            if (int( probs[i] / epsillon ) -1) not in cluster_dic.keys():\n",
    "             \n",
    "                cluster_dic[ (int( probs[i] / epsillon ) -1)] = [ [m_sample[i][0][j-1] for j in sens_params]]\n",
    "           \n",
    "            else:\n",
    "                cluster_dic[ (int( probs[i] / epsillon ) -1)].append( [m_sample[i][0][j-1] for j in sens_params] )\n",
    "\n",
    "                       \n",
    "        elif int( probs[i] / epsillon ) not in cluster_dic.keys():\n",
    "                cluster_dic[ int( probs[i] / epsillon )] = [ [m_sample[i][0][j-1] for j in sens_params] ]\n",
    "           \n",
    "        else:\n",
    "                cluster_dic[ int( probs[i] / epsillon)].append( [m_sample[i][0][j-1] for j in sens_params] )\n",
    "\n",
    "    return cluster_dic  \n",
    "\n",
    "    \n",
    "def pred_prob(sess, x, preds, m_sample, input_shape):\n",
    "        #global probs\n",
    "        probs = model_prediction(sess, x, preds, np.array(m_sample).reshape(len(m_sample),\n",
    "                                                                            input_shape[1]))[:,1:2].reshape(len(m_sample))\n",
    "        return probs        \n",
    "        \n",
    "def neuron_locator(samples, layer_number,model_path, input_shape, nb_classes, dataset, sens_params, update_list ):\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.per_process_gpu_memory_fraction = 0.8\n",
    "        sess = tf.Session(config = config)\n",
    "        x = tf.placeholder(tf.float32, shape = input_shape)\n",
    "        y = tf.placeholder(tf.float32, shape = (None, nb_classes))\n",
    "        model = dnn(input_shape, nb_classes)   \n",
    "\n",
    "        preds = model(x)\n",
    "        saver = tf.train.Saver()\n",
    "        model_path = model_path + dataset + \"/test.model\"\n",
    "        saver.restore(sess, model_path)\n",
    "#         l1 = model.layers[0].fprop(np.array([samples[0]]).astype('float32'))\n",
    "#         r1 = model.layers[1].fprop(l1,[[0,0,0]],[1000])\n",
    "#         l2 = model.layers[2].fprop(r1)\n",
    "#         r2 = model.layers[3].fprop(l2)\n",
    "#         init_out = r2.eval(session=sess)\n",
    "        feed_dic = {}\n",
    "        for neuron in range(len(update_list)):\n",
    "            time1 = time.time()              \n",
    "            for layer in range(0,11,2):\n",
    "                if layer == 0:\n",
    "                    l = model.layers[layer].fprop(samples.astype('float32'))\n",
    "                else:\n",
    "                    l = model.layers[layer].fprop(r)                   \n",
    "                if layer + 1 == (layer_number * 2) - 1:\n",
    "                    indices = []\n",
    "                    for instance in range(l.shape[0]):                       \n",
    "                        indices.append([ instance, 0, neuron])       \n",
    "                    updates = [ update_list[ neuron ] ] * l.shape[0]                    \n",
    "                    r = model.layers[layer + 1].fprop(l , indices, updates)\n",
    "                else:\n",
    "                    r = model.layers[layer + 1].fprop(l)             \n",
    "            feed_dic[neuron] = r\n",
    "        all_probs = sess.run(feed_dic)\n",
    "        out_dic={}\n",
    "        for key in all_probs.keys():\n",
    "            probs = np.array(all_probs[key]).reshape((90,2))[:,1:].reshape((90))\n",
    "            clus = clustering(probs,samples, sens_params)\n",
    "            out_dic[key]= [len(clus) - 1 ]     \n",
    "        sess.close()\n",
    "        tf.reset_default_graph()\n",
    "        return out_dic\n",
    "\n",
    "def get_rate(sess, model, model_path, input_shape, nb_classes,\n",
    "              dataset, lay_name, layer_output):\n",
    "        def get_distance(vec1, vec2, size):\n",
    "            return abs(vec1 - vec2).sum() / size\n",
    "        if  sess._closed:\n",
    "#                 config = tf.ConfigProto()\n",
    "#                 config.gpu_options.per_process_gpu_memory_fraction = 0.8\n",
    "                config = tf.ConfigProto(device_count = {'GPU': 0})\n",
    "                config.allow_soft_placement= True\n",
    "                sess   = tf.Session(config = config)\n",
    "                x      = tf.placeholder(tf.float32, shape = input_shape)\n",
    "                y      = tf.placeholder(tf.float32, shape = (None, nb_classes))\n",
    "                model  = dnn(input_shape, nb_classes)   \n",
    "                preds  = model(x)\n",
    "                saver  = tf.train.Saver()\n",
    "                saver.restore(sess, model_path)    \n",
    "\n",
    "        max_dis = 0\n",
    "        epsillon = 10 ** -7\n",
    "        num_samples = len(layer_output[lay_name])\n",
    "\n",
    "        layer_ind = np.where(np.array(list(layer_output.keys())) == lay_name)[0][0]\n",
    "        for ind in range(layer_ind):\n",
    "            temp_dis = 0\n",
    "            if 'ReLU' in np.array(list(layer_output.keys()))[ind]:\n",
    "                layer_name = np.array(list(layer_output.keys()))[ind]\n",
    "                layer_size  = len(layer_output[layer_name][0][0])\n",
    "                distances = np.zeros((num_samples,num_samples))\n",
    "                for i in combinations(range(num_samples),2):\n",
    "                    distances[i[0],i[1]] = get_distance(layer_output[layer_name][i[0]],layer_output[layer_name][i[1]],layer_size)\n",
    "                if distances.max()> max_dis:\n",
    "                    max_dis = distances.max()\n",
    "        distances = np.zeros((num_samples,num_samples))\n",
    "        for i in combinations(range(num_samples),2):\n",
    "            distances[i[0],i[1]] = get_distance(layer_output[lay_name][i[0]],layer_output[lay_name][i[1]],len(layer_output[lay_name][0][0]))\n",
    "        cur_dis = distances.max()\n",
    "\n",
    "        change_rate = (cur_dis - max_dis ) / (max_dis + epsillon)      \n",
    "        return change_rate \n",
    "#-------------------------------------------\n",
    "    \n",
    "def dnn_fair_testing(dataset, sens_params, model_path):\n",
    "    \"\"\"\n",
    "    \n",
    "    The implementation of ADF\n",
    "    :param dataset: the name of testing dataset\n",
    "    :param sensitive_param: the index of sensitive feature\n",
    "    :param model_path: the path of testing model\n",
    "    :param cluster_num: the number of clusters to form as well as the number of\n",
    "            centroids to generate\n",
    "    :param max_global: the maximum number of samples for global search\n",
    "    :param max_local: the maximum number of samples for local search\n",
    "    :param max_iter: the maximum iteration of global perturbation\n",
    "    \"\"\"\n",
    "    data = {\"census\":census_data, \"credit\":credit_data, \"bank\":bank_data, \"compas\":compas_data, \n",
    "            \"default\": default_data, \"heart\":heart_data}\n",
    "    data_config = {\"census\":census, \"credit\":credit, \"bank\":bank, \"compas\":compas, \"default\":default,\n",
    "                  \"heart\":heart}\n",
    "    # prepare the testing data and model\n",
    "    X, Y, input_shape, nb_classes = data[dataset]()\n",
    "    tf.set_random_seed(1234)\n",
    "    \n",
    "    \n",
    "\n",
    "#     config = tf.ConfigProto()\n",
    "#     config.gpu_options.per_process_gpu_memory_fraction = 0.8\n",
    "    config = tf.ConfigProto(device_count = {'GPU': 0})\n",
    "    config.allow_soft_placement= True\n",
    "    sess = tf.Session(config=config)\n",
    "    x = tf.placeholder(tf.float32, shape=input_shape)\n",
    "    y = tf.placeholder(tf.float32, shape=(None, nb_classes))\n",
    "    model = dnn(input_shape, nb_classes)   \n",
    "\n",
    "    preds = model(x)\n",
    "    saver = tf.train.Saver()\n",
    "    model_path = model_path + dataset + \"/test.model\"\n",
    "    saver.restore(sess, model_path)\n",
    "\n",
    "    sample_df=pd.read_csv('../results/census/local_samples.csv',header='infer')\n",
    "    loc_samples=np.array(sample_df.drop(columns=['sample']))\n",
    "    for sample in loc_samples:\n",
    "\n",
    "        samples = m_instance( np.array([sample]) , sens_params, data_config[dataset])\n",
    "        layer_output = layer_out(sess,model,np.array(samples).astype('float32'))  \n",
    "        temp={}\n",
    "        for layer in layer_output.keys():\n",
    "            if 'ReLU' in layer:\n",
    "                temp[layer]=get_rate(sess, model, model_path, input_shape, nb_classes,\n",
    "                                      dataset,layer, layer_output)\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "#    zz=sess.run(tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,'linear/kernel:0'))\n",
    "    #input([n.name for n in tf.get_default_graph().as_graph_def().node])\n",
    "#    current_graph = tf.compat.v1.get_default_graph()   \n",
    "\n",
    "    \n",
    "#     all_names = [var for var in tf.trainable_variables()]\n",
    "#     for x in all_names:\n",
    "#         print(x)\n",
    "#         if x.name == 'linear/kernel:0':\n",
    "#             print(x.eval(session=sess))\n",
    "#             op=x.assign(np.zeros((13,64)))\n",
    "#             sess.run(op)\n",
    "#             print('-------------------')\n",
    "#             print(x.numpy())\n",
    "#             print(x.eval(session=sess))\n",
    "#     input('dd')\n",
    "    \n",
    "    #loc_samples = np.load('../results/census/local_samples.npy')\n",
    "#     input(pd.DataFrame(loc_samples).to_csv('../results/census/local_samples.csv'))\n",
    "    sample_df=pd.read_csv('../results/census/local_samples.csv',header='infer')\n",
    "    loc_samples=np.array(sample_df.drop(columns=['sample']))\n",
    "    update_df=pd.read_csv('../results/census/dataset_layer2_out.csv')\n",
    "    update_list_mean = update_df.mean()\n",
    "    update_list_max = update_df.max()\n",
    "    update_list_mode = update_df.mode()\n",
    "#---------------------------------------    \n",
    "#     Layer 2 output for analysis\n",
    "\n",
    "#     test_dic={}\n",
    "#     ind=0\n",
    "#     global zz\n",
    "#     for sample in loc_samples:\n",
    "#         samples = m_instance( np.array([sample]) , sens_params, data_config[dataset])\n",
    "#         l1 = model.layers[0].fprop(samples.astype('float32'))\n",
    "#         r1 = model.layers[1].fprop(l1)\n",
    "#         l2 = model.layers[2].fprop(r1)\n",
    "#         r2 = model.layers[3].fprop(l2)\n",
    "#         test_dic[ind]=r2\n",
    "#         ind+=1\n",
    "#     zz=sess.run(test_dic)\n",
    "#     df=pd.DataFrame(np.zeros((1,32))).drop(index=0)\n",
    "#     for key in zz.keys():\n",
    "#         df=df.append(pd.DataFrame(zz[key].reshape((90,32))),ignore_index=True)\n",
    "#     df.to_csv('../results/census/dataset_layer2_out.csv',index=False) \n",
    "#-------------------------------------------------\n",
    "#     input()\n",
    "#         r1 = model.layers[1].fprop(l1,[[0,0,0]],[1000])\n",
    "#         l2 = model.layers[2].fprop(r1)\n",
    "#         r2 = model.layers[3].fprop(l2)\n",
    "#     df_layer_rate =pd.DataFrame(np.zeros((1,5)),columns=['ReLU1','ReLU3','ReLU5','ReLU7','ReLU9'])\n",
    "#     for sample in loc_samples:\n",
    "\n",
    "#         samples = m_instance( np.array([sample]) , sens_params, data_config[dataset])\n",
    "#         layer_output = layer_out(sess,model,np.array(samples).astype('float32'))  \n",
    "#         temp={}\n",
    "#         for layer in layer_output.keys():\n",
    "#             if 'ReLU' in layer:\n",
    "#                 #print(layer, get_rate(layer, layer_output)) \n",
    "#                 temp[layer]=get_rate(layer, layer_output)\n",
    "#                 #df_layer_rate[ layer ] = get_rate(layer, layer_output) \n",
    "#         df_layer_rate = df_layer_rate.append(temp,ignore_index=True)\n",
    "\n",
    "#     #df_layer_rate.to_csv('../results/census/layer_locator.csv',index=False) \n",
    "   \n",
    "    layer_number = 2\n",
    "    column_list=['ReLU3 N0','ReLU3 N1','ReLU3 N2','ReLU3 N3',\n",
    "                 'ReLU3 N4','ReLU3 N5','ReLU3 N6','ReLU3 N7',\n",
    "                 'ReLU3 N8','ReLU3 N9','ReLU3 N10','ReLU3 N11',\n",
    "                 'ReLU3 N12','ReLU3 N13','ReLU3 N14','ReLU3 N15',\n",
    "                 'ReLU3 N16','ReLU3 N17','ReLU3 N18','ReLU3 N19',\n",
    "                 'ReLU3 N20','ReLU3 N21','ReLU3 N22','ReLU3 N23',\n",
    "                 'ReLU3 N24','ReLU3 N25','ReLU3 N26','ReLU3 N27',\n",
    "                 'ReLU3 N28','ReLU3 N29','ReLU3 N30','ReLU3 N31']\n",
    "    df=pd.DataFrame(np.zeros((1,32)),dtype='int32').drop(index=0)\n",
    "    for sample in loc_samples:\n",
    "        samples = m_instance( np.array([sample]) , sens_params, data_config[dataset])\n",
    "        ss=neuron_locator(samples, layer_number,model_path,input_shape,nb_classes,dataset,sens_params,update_list_max)\n",
    "        df1 = pd.DataFrame(ss)\n",
    "        df=df.append(df1,ignore_index=False)\n",
    "    df.columns=column_list\n",
    "    df.to_csv('../results/census/neuron_locator_max.csv',index=False)   \n",
    "#----------------------------\n",
    "                \n",
    "           \n",
    "        \n",
    "def main(argv=None):\n",
    "    dnn_fair_testing(dataset = FLAGS.dataset, \n",
    "                     sens_params = FLAGS.sens_params,\n",
    "                     model_path  = FLAGS.model_path)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    flags.DEFINE_string(\"dataset\", \"census\", \"the name of dataset\")\n",
    "    flags.DEFINE_string('model_path', '../models/', 'the path for testing model')\n",
    "    flags.DEFINE_integer('cluster_num', 4, 'the number of clusters to form as well as the number of centroids to generate')\n",
    "    flags.DEFINE_list('sens_params',[9,8,1],'sensitive parameters index.1 for age, 9 for gender, 8 for race')\n",
    "    tf.app.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
