{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../models/census/test.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0728 11:16:47.091549 140397840320320 saver.py:1399] Restoring parameters from ../models/census/test.model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/lib/python3/dist-packages/tensorflow/python/util/dispatch.py:1096: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0728 11:16:47.136642 140397840320320 deprecation.py:341] From /usr/lib/python3/dist-packages/tensorflow/python/util/dispatch.py:1096: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242 Percentage discriminatory inputs of local search- 34.036568213783404\n",
      "362 Percentage discriminatory inputs of local search- 25.156358582348854\n",
      "519 Percentage discriminatory inputs of local search- 23.107747105966162\n",
      "688 Percentage discriminatory inputs of local search- 23.4412265758092\n",
      "866 Percentage discriminatory inputs of local search- 23.336028024791162\n",
      "1042 Percentage discriminatory inputs of local search- 23.31096196868009\n",
      "1190 Percentage discriminatory inputs of local search- 22.968538892105773\n",
      "1450 Percentage discriminatory inputs of local search- 24.332941768753148\n",
      "1732 Percentage discriminatory inputs of local search- 25.76230849323219\n",
      "1905 Percentage discriminatory inputs of local search- 25.43730805180932\n",
      "1959 Percentage discriminatory inputs of local search- 23.754092397235357\n",
      "2260 Percentage discriminatory inputs of local search- 24.95858641634456\n",
      "2303 Percentage discriminatory inputs of local search- 23.39258506856272\n",
      "2474 Percentage discriminatory inputs of local search- 23.35725075528701\n",
      "2567 Percentage discriminatory inputs of local search- 22.662664430122714\n",
      "2732 Percentage discriminatory inputs of local search- 22.700457000415454\n",
      "2959 Percentage discriminatory inputs of local search- 23.086525708044004\n",
      "3216 Percentage discriminatory inputs of local search- 23.67316893632683\n",
      "3381 Percentage discriminatory inputs of local search- 23.633440514469452\n",
      "3571 Percentage discriminatory inputs of local search- 23.65684001324942\n",
      "3641 Percentage discriminatory inputs of local search- 23.012261408165845\n",
      "3726 Percentage discriminatory inputs of local search- 22.43226971703793\n",
      "3813 Percentage discriminatory inputs of local search- 21.965551011002937\n",
      "3908 Percentage discriminatory inputs of local search- 21.58281327663335\n",
      "4109 Percentage discriminatory inputs of local search- 21.77299703264095\n",
      "4232 Percentage discriminatory inputs of local search- 21.532512465655845\n",
      "4381 Percentage discriminatory inputs of local search- 21.441855912294443\n",
      "4522 Percentage discriminatory inputs of local search- 21.330188679245282\n",
      "4642 Percentage discriminatory inputs of local search- 21.13169754631948\n",
      "4812 Percentage discriminatory inputs of local search- 21.20103978499361\n",
      "4855 Percentage discriminatory inputs of local search- 20.629727203195376\n",
      "5135 Percentage discriminatory inputs of local search- 21.136906231991436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-d50df68d4713>:130: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  grad_sum = 1.0 / (abs(ind_grad[0]) + abs(n_ind_grad[0]))\n",
      "<ipython-input-1-d50df68d4713>:132: RuntimeWarning: invalid value encountered in true_divide\n",
      "  probs = grad_sum / np.sum(grad_sum)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "probabilities contain NaN",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d50df68d4713>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    327\u001b[0m     \u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFINE_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'max_iter'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'maximum iteration of global perturbation'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow/python/platform/app.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, argv)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0mmain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'__main__'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags_parser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_parse_flags_tolerate_undef\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3/dist-packages/absl/app.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, argv, flags_parser)\u001b[0m\n\u001b[1;32m    301\u001b[0m       \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m       \u001b[0m_run_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mUsageError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m       \u001b[0musage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshorthelp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetailed_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexitcode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexitcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/absl/app.py\u001b[0m in \u001b[0;36m_run_main\u001b[0;34m(main, argv)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-d50df68d4713>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(argv)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m     dnn_fair_testing(dataset = FLAGS.dataset,\n\u001b[0m\u001b[1;32m    313\u001b[0m                      \u001b[0msensitive_param\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msens_param\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                      \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-d50df68d4713>\u001b[0m in \u001b[0;36mdnn_fair_testing\u001b[0;34m(dataset, sensitive_param, model_path, cluster_num, max_global, max_local, max_iter)\u001b[0m\n\u001b[1;32m    255\u001b[0m                 local_perturbation = Local_Perturbation(sess, grad_0, x, n_value, sensitive_param, input_shape[1],\n\u001b[1;32m    256\u001b[0m                                                         data_config[dataset])\n\u001b[0;32m--> 257\u001b[0;31m                 basinhopping(evaluate_local, sample, stepsize=1.0, take_step=local_perturbation,\n\u001b[0m\u001b[1;32m    258\u001b[0m                              \u001b[0mminimizer_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mminimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m                              niter=max_local)\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/scipy/optimize/_basinhopping.py\u001b[0m in \u001b[0;36mbasinhopping\u001b[0;34m(func, x0, niter, T, stepsize, minimizer_kwargs, take_step, accept_test, callback, interval, disp, niter_success, seed)\u001b[0m\n\u001b[1;32m    674\u001b[0m                \" successfully\"]\n\u001b[1;32m    675\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mniter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0mnew_global_min\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_cycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/scipy/optimize/_basinhopping.py\u001b[0m in \u001b[0;36mone_cycle\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mnew_global_min\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         \u001b[0maccept\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_monte_carlo_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maccept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/scipy/optimize/_basinhopping.py\u001b[0m in \u001b[0;36m_monte_carlo_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;31m# algorithm might change x in place\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0mx_after_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0mx_after_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_taking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_after_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;31m# do a local minimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-d50df68d4713>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;31m# randomly choose the feature for local perturbation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m         \u001b[0mlocal_cal_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0mlocal_cal_grad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: probabilities contain NaN"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf \n",
    "tf.compat.v1.disable_eager_execution()\n",
    "import sys, os\n",
    "sys.path.append(\"../\")\n",
    "import copy\n",
    "import pandas as pd\n",
    "from tensorflow.python.platform import flags\n",
    "from scipy.optimize import basinhopping\n",
    "import time\n",
    "from adf_data.census import census_data\n",
    "from adf_data.credit import credit_data\n",
    "from adf_data.bank import bank_data\n",
    "from adf_model.tutorial_models import dnn\n",
    "from adf_utils.utils_tf import model_prediction, model_argmax , layer_out\n",
    "from adf_utils.config import census, credit, bank\n",
    "from adf_tutorial.utils import cluster, gradient_graph\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "\n",
    "# step size of perturbation\n",
    "perturbation_size = 1\n",
    "\n",
    "def check_for_error_condition(conf, sess, x, preds, t, sens):\n",
    "    \"\"\"\n",
    "    Check whether the test case is an individual discriminatory instance\n",
    "    :param conf: the configuration of dataset\n",
    "    :param sess: TF session\n",
    "    :param x: input placeholder\n",
    "    :param preds: the model's symbolic output\n",
    "    :param t: test case\n",
    "    :param sens: the index of sensitive feature\n",
    "    :return: whether it is an individual discriminatory instance\n",
    "    \"\"\"\n",
    "    t = t.astype('int')\n",
    "    label = model_argmax(sess, x, preds, np.array([t]))\n",
    "\n",
    "    # check for all the possible values of sensitive feature\n",
    "    for val in range(conf.input_bounds[sens-1][0], conf.input_bounds[sens-1][1]+1):\n",
    "        if val != t[sens-1]:\n",
    "            tnew = copy.deepcopy(t)\n",
    "            tnew[sens-1] = val\n",
    "            label_new = model_argmax(sess, x, preds, np.array([tnew]))\n",
    "            if label_new != label:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def seed_test_input(clusters, limit):\n",
    "    \"\"\"\n",
    "    Select the seed inputs for fairness testing\n",
    "    :param clusters: the results of K-means clustering\n",
    "    :param limit: the size of seed inputs wanted\n",
    "    :return: a sequence of seed inputs\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    rows = []\n",
    "    max_size = max([len(c[0]) for c in clusters])\n",
    "    while i < max_size:\n",
    "        if len(rows) == limit:\n",
    "            break\n",
    "        for c in clusters:\n",
    "            if i >= len(c[0]):\n",
    "                continue\n",
    "            row = c[0][i]\n",
    "            rows.append(row)\n",
    "            if len(rows) == limit:\n",
    "                break\n",
    "        i += 1\n",
    "    return np.array(rows)\n",
    "\n",
    "def clip(input, conf):\n",
    "    \"\"\"\n",
    "    Clip the generating instance with each feature to make sure it is valid\n",
    "    :param input: generating instance\n",
    "    :param conf: the configuration of dataset\n",
    "    :return: a valid generating instance\n",
    "    \"\"\"\n",
    "    for i in range(len(input)):\n",
    "        input[i] = max(input[i], conf.input_bounds[i][0])\n",
    "        input[i] = min(input[i], conf.input_bounds[i][1])\n",
    "    return input\n",
    "\n",
    "class Local_Perturbation(object):\n",
    "    \"\"\"\n",
    "    The  implementation of local perturbation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sess, grad, x, n_value, sens, input_shape, conf):\n",
    "        \"\"\"\n",
    "        Initial function of local perturbation\n",
    "        :param sess: TF session\n",
    "        :param grad: the gradient graph\n",
    "        :param x: input placeholder\n",
    "        :param n_value: the discriminatory value of sensitive feature\n",
    "        :param sens_param: the index of sensitive feature\n",
    "        :param input_shape: the shape of dataset\n",
    "        :param conf: the configuration of dataset\n",
    "        \"\"\"\n",
    "        self.sess = sess\n",
    "        self.grad = grad\n",
    "        self.x = x\n",
    "        self.n_value = n_value\n",
    "        self.input_shape = input_shape\n",
    "        self.sens = sens\n",
    "        self.conf = conf\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Local perturbation\n",
    "        :param x: input instance for local perturbation\n",
    "        :return: new potential individual discriminatory instance\n",
    "        \"\"\"\n",
    "\n",
    "        # perturbation\n",
    "        s = np.random.choice([1.0, -1.0]) * perturbation_size\n",
    "\n",
    "        n_x = x.copy()\n",
    "        n_x[self.sens - 1] = self.n_value\n",
    "\n",
    "        # compute the gradients of an individual discriminatory instance pairs\n",
    "        ind_grad = self.sess.run(self.grad, feed_dict={self.x:np.array([x])})\n",
    "        n_ind_grad = self.sess.run(self.grad, feed_dict={self.x:np.array([n_x])})\n",
    "\n",
    "        if np.zeros(self.input_shape).tolist() == ind_grad[0].tolist() and np.zeros(self.input_shape).tolist() == \\\n",
    "                n_ind_grad[0].tolist():\n",
    "            probs = 1.0 / (self.input_shape-1) * np.ones(self.input_shape)\n",
    "            probs[self.sens - 1] = 0\n",
    "        else:\n",
    "            # nomalize the reciprocal of gradients (prefer the low impactful feature)\n",
    "            grad_sum = 1.0 / (abs(ind_grad[0]) + abs(n_ind_grad[0]))\n",
    "            grad_sum[self.sens - 1] = 0\n",
    "            probs = grad_sum / np.sum(grad_sum)\n",
    "        probs = probs/probs.sum()\n",
    "\n",
    "        # randomly choose the feature for local perturbation\n",
    "        index = np.random.choice(range(self.input_shape) , p=probs)\n",
    "        local_cal_grad = np.zeros(self.input_shape)\n",
    "        local_cal_grad[index] = 1.0\n",
    "\n",
    "        x = clip(x + s * local_cal_grad, self.conf).astype(\"int\")\n",
    "\n",
    "        return x\n",
    "\n",
    "def dnn_fair_testing(dataset, sensitive_param, model_path, cluster_num, max_global, max_local, max_iter):\n",
    "    \"\"\"\n",
    "    The implementation of ADF\n",
    "    :param dataset: the name of testing dataset\n",
    "    :param sensitive_param: the index of sensitive feature\n",
    "    :param model_path: the path of testing model\n",
    "    :param cluster_num: the number of clusters to form as well as the number of\n",
    "            centroids to generate\n",
    "    :param max_global: the maximum number of samples for global search\n",
    "    :param max_local: the maximum number of samples for local search\n",
    "    :param max_iter: the maximum iteration of global perturbation\n",
    "    \"\"\"\n",
    "    data = {\"census\":census_data, \"credit\":credit_data, \"bank\":bank_data}\n",
    "    data_config = {\"census\":census, \"credit\":credit, \"bank\":bank}\n",
    "\n",
    "    # prepare the testing data and model\n",
    "    X, Y, input_shape, nb_classes = data[dataset]()\n",
    "    tf.set_random_seed(1234)\n",
    "#     config = tf.ConfigProto()\n",
    "#     config.gpu_options.per_process_gpu_memory_fraction = 0.8\n",
    "    config = tf.ConfigProto(device_count = {'GPU': 0})\n",
    "    config.allow_soft_placement= True\n",
    "    \n",
    "    sess = tf.Session(config=config)\n",
    "    x = tf.placeholder(tf.float32, shape=input_shape)\n",
    "    y = tf.placeholder(tf.float32, shape=(None, nb_classes))\n",
    "    model = dnn(input_shape, nb_classes)\n",
    "    preds = model(x)\n",
    "    saver = tf.train.Saver()\n",
    "    model_path = model_path + dataset + \"/test.model\"\n",
    "    saver.restore(sess, model_path)\n",
    "\n",
    "    # construct the gradient graph\n",
    "    grad_0 = gradient_graph(x, preds)\n",
    "\n",
    "    # build the clustering model\n",
    "    clf = cluster(dataset, cluster_num)\n",
    "    clusters = [np.where(clf.labels_==i) for i in range(cluster_num)]\n",
    "\n",
    "    # store the result of fairness testing\n",
    "    tot_inputs = set()\n",
    "    global_disc_inputs = set()\n",
    "    global_disc_inputs_list = []\n",
    "    local_disc_inputs = set()\n",
    "    local_disc_inputs_list = []\n",
    "    value_list = []\n",
    "    suc_idx = []\n",
    "\n",
    "    def evaluate_local(inp):\n",
    "        \"\"\"\n",
    "        Evaluate whether the test input after local perturbation is an individual discriminatory instance\n",
    "        :param inp: test input\n",
    "        :return: whether it is an individual discriminatory instance\n",
    "        \"\"\"\n",
    "        result = check_for_error_condition(data_config[dataset], sess, x, preds, inp, sensitive_param)\n",
    "\n",
    "        temp = copy.deepcopy(inp.astype('int').tolist())\n",
    "        temp = temp[:sensitive_param - 1] + temp[sensitive_param:]\n",
    "        tot_inputs.add(tuple(temp))\n",
    "        if result and (tuple(temp) not in global_disc_inputs) and (tuple(temp) not in local_disc_inputs):\n",
    "            local_disc_inputs.add(tuple(temp))\n",
    "            local_disc_inputs_list.append(temp)\n",
    "\n",
    "        return not result\n",
    "\n",
    "    # select the seed input for fairness testing\n",
    "    inputs = seed_test_input(clusters, min(max_global, len(X)))\n",
    "    time1=time.time()\n",
    "    for num in range(len(inputs)):\n",
    "        if time.time()-time1 > 500:break\n",
    "        index = inputs[num]\n",
    "        sample = X[index:index+1]\n",
    "\n",
    "        # start global perturbation\n",
    "        for iter in range(max_iter+1):\n",
    "            if time.time()-time1 > 500:break\n",
    "            probs = model_prediction(sess, x, preds, sample)[0]\n",
    "            label = np.argmax(probs)\n",
    "            prob = probs[label]\n",
    "            max_diff = 0\n",
    "            n_value = -1\n",
    "\n",
    "            # search the instance with maximum probability difference for global perturbation\n",
    "            for i in range(census.input_bounds[sensitive_param-1][0], census.input_bounds[sensitive_param-1][1] + 1):\n",
    "                if i != sample[0][sensitive_param-1]:\n",
    "                    n_sample = sample.copy()\n",
    "                    n_sample[0][sensitive_param-1] = i\n",
    "                    n_probs = model_prediction(sess, x, preds, n_sample)[0]\n",
    "                    n_label = np.argmax(n_probs)\n",
    "                    n_prob = n_probs[n_label]\n",
    "                    if label != n_label:\n",
    "                        n_value = i\n",
    "                        break\n",
    "                    else:\n",
    "                        prob_diff = abs(prob - n_prob)\n",
    "                        if prob_diff > max_diff:\n",
    "                            max_diff = prob_diff\n",
    "                            n_value = i\n",
    "\n",
    "            temp = copy.deepcopy(sample[0].astype('int').tolist())\n",
    "            temp = temp[:sensitive_param - 1] + temp[sensitive_param:]\n",
    "\n",
    "            # if get an individual discriminatory instance\n",
    "            if label != n_label and (tuple(temp) not in global_disc_inputs) and (tuple(temp) not in local_disc_inputs):\n",
    "                global_disc_inputs_list.append(temp)\n",
    "                global_disc_inputs.add(tuple(temp))\n",
    "                value_list.append([sample[0, sensitive_param - 1], n_value])\n",
    "                suc_idx.append(index)\n",
    "                #print(len(suc_idx), num)\n",
    "                # start local perturbation\n",
    "                minimizer = {\"method\": \"L-BFGS-B\"}\n",
    "                local_perturbation = Local_Perturbation(sess, grad_0, x, n_value, sensitive_param, input_shape[1],\n",
    "                                                        data_config[dataset])\n",
    "                basinhopping(evaluate_local, sample, stepsize=1.0, take_step=local_perturbation,\n",
    "                             minimizer_kwargs=minimizer,\n",
    "                             niter=max_local)\n",
    "                \n",
    "                print(len(local_disc_inputs_list),\n",
    "                      \"Percentage discriminatory inputs of local search- \" + str(\n",
    "                          float(len(local_disc_inputs)) / float(len(tot_inputs)) * 100))\n",
    "                break\n",
    "\n",
    "            n_sample[0][sensitive_param - 1] = n_value\n",
    "\n",
    "            if iter == max_iter:\n",
    "                break\n",
    "\n",
    "            # global perturbation\n",
    "            s_grad = sess.run(tf.sign(grad_0), feed_dict={x: sample})\n",
    "            n_grad = sess.run(tf.sign(grad_0), feed_dict={x: n_sample})\n",
    "\n",
    "            # find the feature with same impact\n",
    "            if np.zeros(data_config[dataset].params).tolist() == s_grad[0].tolist():\n",
    "                g_diff = n_grad[0]\n",
    "            elif np.zeros(data_config[dataset].params).tolist() == n_grad[0].tolist():\n",
    "                g_diff = s_grad[0]\n",
    "            else:\n",
    "                g_diff = np.array(s_grad[0] == n_grad[0], dtype=float)\n",
    "            g_diff[sensitive_param - 1] = 0\n",
    "            if np.zeros(input_shape[1]).tolist() == g_diff.tolist():\n",
    "                index = np.random.randint(len(g_diff) - 1)\n",
    "                if index > sensitive_param - 2:\n",
    "                    index = index + 1\n",
    "                g_diff[index] = 1.0\n",
    "\n",
    "            cal_grad = s_grad * g_diff\n",
    "            sample[0] = clip(sample[0] + perturbation_size * cal_grad[0], data_config[dataset]).astype(\"int\")\n",
    "\n",
    "    # create the folder for storing the fairness testing result\n",
    "    if not os.path.exists('../results/'):\n",
    "        os.makedirs('../results/')\n",
    "    if not os.path.exists('../results/' + dataset + '/'):\n",
    "        os.makedirs('../results/' + dataset + '/')\n",
    "    if not os.path.exists('../results/'+ dataset + '/'+ str(sensitive_param) + '/'):\n",
    "        os.makedirs('../results/' + dataset + '/'+ str(sensitive_param) + '/')\n",
    "\n",
    "    # storing the fairness testing result\n",
    "    np.save('../results/'+dataset+'/'+ str(sensitive_param) + '/suc_idx.npy', np.array(suc_idx))\n",
    "    np.save('../results/'+dataset+'/'+ str(sensitive_param) + '/global_samples.npy', np.array(global_disc_inputs_list))\n",
    "    np.save('../results/'+dataset+'/'+ str(sensitive_param) + '/local_samples.npy', np.array(local_disc_inputs_list))\n",
    "    np.save('../results/'+dataset+'/'+ str(sensitive_param) + '/disc_value.npy', np.array(value_list))\n",
    "\n",
    "    # print the overview information of result\n",
    "    print(\"Total Inputs are \" + str(len(tot_inputs)))\n",
    "    print(\"Total discriminatory inputs of global search- \" + str(len(global_disc_inputs)))\n",
    "    print(\"Total discriminatory inputs of local search- \" + str(len(local_disc_inputs)))\n",
    "\n",
    "def main(argv=None):\n",
    "    dnn_fair_testing(dataset = FLAGS.dataset,\n",
    "                     sensitive_param = FLAGS.sens_param,\n",
    "                     model_path = FLAGS.model_path,\n",
    "                     cluster_num=FLAGS.cluster_num,\n",
    "                     max_global=FLAGS.max_global,\n",
    "                     max_local=FLAGS.max_local,\n",
    "                     max_iter = FLAGS.max_iter)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    flags.DEFINE_string(\"dataset\", \"census\", \"the name of dataset\")\n",
    "    flags.DEFINE_integer('sens_param', 9, 'sensitive index, index start from 1, 9 for gender, 8 for race')\n",
    "    flags.DEFINE_string('model_path', '../models/', 'the path for testing model')\n",
    "    flags.DEFINE_integer('cluster_num', 4, 'the number of clusters to form as well as the number of centroids to generate')\n",
    "    flags.DEFINE_integer('max_global', 1000, 'maximum number of samples for global search')\n",
    "    flags.DEFINE_integer('max_local', 1000, 'maximum number of samples for local search')\n",
    "    flags.DEFINE_integer('max_iter', 10, 'maximum iteration of global perturbation')\n",
    "\n",
    "    tf.app.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Total discriminatory inputs of global search- 67\n",
    "Total discriminatory inputs of local search- 11269\n",
    "\n",
    "Total discriminatory inputs of global search- 28\n",
    "Total discriminatory inputs of local search- 696"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
