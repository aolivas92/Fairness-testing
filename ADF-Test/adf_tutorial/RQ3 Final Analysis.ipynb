{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "146.35195899009705\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py:3339: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from itertools import product, combinations\n",
    "import tensorflow.compat.v1 as tf \n",
    "tf.compat.v1.disable_eager_execution()\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import copy\n",
    "import time\n",
    "import pandas as pd\n",
    "from tensorflow.python.platform import flags\n",
    "from adf_data.census import census_data\n",
    "from adf_data.credit import credit_data\n",
    "from adf_data.compas import compas_data\n",
    "from adf_data.default import default_data\n",
    "from adf_data.bank import bank_data\n",
    "from adf_data.heart import heart_data\n",
    "from adf_model.tutorial_models import dnn\n",
    "from adf_utils.utils_tf import model_prediction, model_argmax , layer_out\n",
    "from adf_utils.config import census, credit, bank, compas, default, heart\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "       \n",
    "def m_instance( sample, sens_params, conf):\n",
    "    index = []\n",
    "    m_sample = []\n",
    "    for sens in sens_params:\n",
    "        index.append([i for i in range(conf.input_bounds[sens - 1][0], conf.input_bounds[sens-1][1] + 1)])\n",
    "      \n",
    "    for ind in list(product(*index)):     \n",
    "        temp = sample.copy()\n",
    "        for i in range(len(sens_params)):\n",
    "            temp[0][sens_params[i]-1] = ind[i]\n",
    "        m_sample.append(temp)\n",
    "    return np.array(m_sample)\n",
    "    \n",
    "def clustering(probs,m_sample, sens_params):\n",
    "    epsillon = 0.025\n",
    "    cluster_dic = {}\n",
    "    cluster_dic['Seed'] = m_sample[0][0]\n",
    "         \n",
    "    for i in range(len(probs)):\n",
    "        #  to avoid k = Max + 1\n",
    "        if probs[i] == 1.0:\n",
    "            if (int( probs[i] / epsillon ) -1) not in cluster_dic.keys():\n",
    "             \n",
    "                cluster_dic[ (int( probs[i] / epsillon ) - 1)] = [ [m_sample[i][0][j - 1] for j in sens_params] ]\n",
    "           \n",
    "            else:\n",
    "                cluster_dic[ (int( probs[i] / epsillon ) - 1)].append( [m_sample[i][0][j - 1] for j in sens_params] )\n",
    "\n",
    "                       \n",
    "        elif int( probs[i] / epsillon ) not in cluster_dic.keys():\n",
    "                cluster_dic[ int( probs[i] / epsillon )] = [ [m_sample[i][0][j - 1] for j in sens_params] ]\n",
    "           \n",
    "        else:\n",
    "                cluster_dic[ int( probs[i] / epsillon)].append( [m_sample[i][0][j - 1] for j in sens_params] )\n",
    "\n",
    "    return cluster_dic  \n",
    "\n",
    "    \n",
    "def pred_prob(sess, x, preds, m_sample, input_shape):\n",
    "        probs = model_prediction(sess, x, preds, np.array(m_sample).reshape(len(m_sample),\n",
    "                                    input_shape[1]))[:,1:2].reshape(len(m_sample))\n",
    "        return probs        \n",
    "        \n",
    "def neuron_locator(sess, model, samples, layer_number,model_path, input_shape, \n",
    "                   nb_classes, dataset, sens_params, update_list ):\n",
    "        if  sess._closed:\n",
    "#             config = tf.ConfigProto()\n",
    "#             config.gpu_options.per_process_gpu_memory_fraction = 0.8\n",
    "            config = tf.ConfigProto(device_count = {'GPU': 0})\n",
    "            config.allow_soft_placement= True            \n",
    "            sess   = tf.Session(config = config)\n",
    "            x      = tf.placeholder(tf.float32, shape = input_shape)\n",
    "            y      = tf.placeholder(tf.float32, shape = (None, nb_classes))\n",
    "            model  = dnn(input_shape, nb_classes)   \n",
    "            preds  = model(x)\n",
    "            saver  = tf.train.Saver()\n",
    "            saver.restore(sess, model_path)\n",
    "            \n",
    "        num_layers = len(model.layers)\n",
    "        feed_dic = {}\n",
    "        for neuron in range(len(update_list)):\n",
    "            time1 = time.time()              \n",
    "            for layer in range(0,num_layers - 1,2):\n",
    "                if layer == 0:\n",
    "                    l = model.layers[layer].fprop(samples.astype('float32'))\n",
    "                else:\n",
    "                    l = model.layers[layer].fprop(r)                   \n",
    "                if layer + 1 == (layer_number * 2) - 1:\n",
    "                    indices = []\n",
    "                    for instance in range(l.shape[0]):                       \n",
    "                        indices.append([ instance, 0, neuron])       \n",
    "                    updates = [ update_list[ neuron ] ] * l.shape[0]\n",
    "                    r = model.layers[layer + 1].fprop(l , indices, updates)\n",
    "                else:\n",
    "                    r = model.layers[layer + 1].fprop(l)\n",
    "            feed_dic[neuron] = r\n",
    "        all_probs = sess.run(feed_dic)\n",
    "        out_dic   = {}\n",
    "        for key in all_probs.keys():\n",
    "            probs = np.array(all_probs[key]).reshape((90,2))[:,1:].reshape((90))\n",
    "            clus  = clustering(probs,samples, sens_params)\n",
    "            out_dic[key] = [len(clus) - 1 ]\n",
    "        sess.close()\n",
    "        tf.reset_default_graph()\n",
    "        return out_dic\n",
    "    \n",
    "def model_acc(sess, model,model_path,input_shape, nb_classes,\n",
    "              dataset, sens_params,neuron,X,Y,layer_number,num_layers,update_list):\n",
    "        if  sess._closed:\n",
    "#                 config = tf.ConfigProto()\n",
    "#                 config.gpu_options.per_process_gpu_memory_fraction = 0.8\n",
    "                config = tf.ConfigProto(device_count = {'GPU': 0})\n",
    "                config.allow_soft_placement= True\n",
    "                sess   = tf.Session(config = config)\n",
    "                x      = tf.placeholder(tf.float32, shape = input_shape)\n",
    "                y      = tf.placeholder(tf.float32, shape = (None, nb_classes))\n",
    "                model  = dnn(input_shape, nb_classes)   \n",
    "                preds  = model(x)\n",
    "                saver  = tf.train.Saver()\n",
    "                saver.restore(sess, model_path)\n",
    "        feed_dic = {}        \n",
    "        for layer in range(0,num_layers - 1,2):\n",
    "            if layer == 0:\n",
    "                l = model.layers[layer].fprop(X.astype('float32'))\n",
    "            else:\n",
    "                l = model.layers[layer].fprop(r)          \n",
    "            if layer + 1 == (layer_number * 2) - 1:\n",
    "                indices = []\n",
    "                for instance in range(l.shape[0]):                       \n",
    "                    indices.append([ instance, neuron])                \n",
    "                updates = [ update_list[ neuron ] ] * l.shape[0]                \n",
    "                r = model.layers[layer + 1].fprop(l , indices, updates)\n",
    "            else:\n",
    "                r = model.layers[layer + 1].fprop(l)             \n",
    "        all_probs = sess.run(r)\n",
    "        out_class = []\n",
    "        for out in all_probs:\n",
    "            out_class.append(np.argmax(out))\n",
    "        truth_val = []\n",
    "        for tr in Y:\n",
    "                truth_val.append(np.argmax(tr))\n",
    "        acc = 0\n",
    "        for i in range(len(out_class)):\n",
    "            if out_class[i] == truth_val[i]:\n",
    "                acc += 1\n",
    "        accuracy = round(acc/len(out_class),3)\n",
    "        sess.close()\n",
    "        tf.reset_default_graph()\n",
    "        return accuracy \n",
    "\n",
    "def get_rate(sess, model, model_path, input_shape, nb_classes,\n",
    "              dataset, lay_name, layer_output):\n",
    "        def get_distance(vec1, vec2, size):\n",
    "            return abs(vec1 - vec2).sum() / size\n",
    "        if  sess._closed:\n",
    "#                 config = tf.ConfigProto()\n",
    "#                 config.gpu_options.per_process_gpu_memory_fraction = 0.8\n",
    "                config = tf.ConfigProto(device_count = {'GPU': 0})\n",
    "                config.allow_soft_placement= True\n",
    "                sess   = tf.Session(config = config)\n",
    "                x      = tf.placeholder(tf.float32, shape = input_shape)\n",
    "                y      = tf.placeholder(tf.float32, shape = (None, nb_classes))\n",
    "                model  = dnn(input_shape, nb_classes)   \n",
    "                preds  = model(x)\n",
    "                saver  = tf.train.Saver()\n",
    "                saver.restore(sess, model_path)    \n",
    "\n",
    "        max_dis = 0\n",
    "        epsillon = 10 ** -7\n",
    "        num_samples = len(layer_output[lay_name])\n",
    "\n",
    "        layer_ind = np.where(np.array(list(layer_output.keys())) == lay_name)[0][0]\n",
    "        for ind in range(layer_ind):\n",
    "            temp_dis = 0\n",
    "            if 'ReLU' in np.array(list(layer_output.keys()))[ind]:\n",
    "                layer_name = np.array(list(layer_output.keys()))[ind]\n",
    "                layer_size  = len(layer_output[layer_name][0][0])\n",
    "                distances = np.zeros((num_samples,num_samples))\n",
    "                for i in combinations(range(num_samples),2):\n",
    "                    distances[i[0],i[1]] = get_distance(layer_output[layer_name][i[0]],layer_output[layer_name][i[1]],layer_size)\n",
    "                if distances.max()> max_dis:\n",
    "                    max_dis = distances.max()\n",
    "        distances = np.zeros((num_samples,num_samples))\n",
    "        for i in combinations(range(num_samples),2):\n",
    "            distances[i[0],i[1]] = get_distance(layer_output[lay_name][i[0]],layer_output[lay_name][i[1]],len(layer_output[lay_name][0][0]))\n",
    "        cur_dis = distances.max()\n",
    "\n",
    "        change_rate = (cur_dis - max_dis ) / (max_dis + epsillon)      \n",
    "        return change_rate \n",
    "#-------------------------------------------\n",
    "    \n",
    "def dnn_fair_testing(dataset, sens_params, model_path):\n",
    "\n",
    "    data = {\"census\":census_data, \"credit\":credit_data, \"bank\":bank_data, \"compas\":compas_data, \n",
    "            \"default\": default_data, \"heart\":heart_data}\n",
    "    data_config = {\"census\":census, \"credit\":credit, \"bank\":bank, \"compas\":compas, \"default\":default,\n",
    "                  \"heart\":heart}\n",
    "    # prepare the testing data and model\n",
    "    X, Y, input_shape, nb_classes = data[dataset]()\n",
    "    tf.set_random_seed(1234)\n",
    "\n",
    "#     config = tf.ConfigProto()\n",
    "#     config.gpu_options.per_process_gpu_memory_fraction = 0.8\n",
    "    config = tf.ConfigProto(device_count = {'GPU': 0})\n",
    "    config.allow_soft_placement= True\n",
    "\n",
    "\n",
    "    sess  = tf.Session(config = config)\n",
    "    x     = tf.placeholder(tf.float32, shape = input_shape)\n",
    "    y     = tf.placeholder(tf.float32, shape = (None, nb_classes))\n",
    "    model = dnn(input_shape, nb_classes)   \n",
    "\n",
    "    preds = model(x)\n",
    "    saver = tf.train.Saver()\n",
    "    model_path = model_path + dataset + \"/test.model\"\n",
    "    saver.restore(sess, model_path)\n",
    "    \n",
    "    num_trial = 1\n",
    "    num_rand_point_1 = 4 # number of random points in range(0 to mean)\n",
    "    num_rand_point_2 = 3 # number of random points in range(mean to std)\n",
    "    num_rand_point_3 = 1 # number of random points in range(mean + std to  mean + 2 * std)\n",
    "    \n",
    "    for trial in range(num_trial):\n",
    "        # maybe we can use a file selected randomly from those 30 runs\n",
    "        input_df  = pd.read_csv('../results/'+dataset+'/final_disc_instances_k_maxk_40.csv',header='infer')\n",
    "        sample_df = input_df.copy()\n",
    "        sample_df_rand = sample_df.sample(n = 900,axis = 0,random_state = np.random.RandomState())\n",
    "        sample_df_maxk = sample_df.sort_values(by = '#k',ascending=False).head(100)\n",
    "        sample_df = pd.concat([sample_df_rand,sample_df_maxk])\n",
    "        ini_k_samples = sample_df['#k']\n",
    "        sample_df = sample_df.drop(columns = ['#disc','#k']) \n",
    "        samples   = sample_df.to_numpy()\n",
    "        num_samples = len(samples)\n",
    "        print(num_samples)\n",
    "        np.save('../results/'+dataset+'/RQ3/samples_'+str(num_samples)+'_'+str(trial)+'.npy', samples)\n",
    "        update_df = pd.read_csv('../results/'+dataset+'/dataset_layer2_out.csv')\n",
    "        #-----------------------------\n",
    "        update_min  = update_df.min()\n",
    "        update_max  = update_df.max()\n",
    "        update_mean = update_df.mean()\n",
    "        update_std  = update_df.std()\n",
    "        update_list = []\n",
    "        update_list.append(update_min)\n",
    "        rand_point_1 = np.sort(np.random.random(num_rand_point_1))\n",
    "        for i in range(len(rand_point_1)):\n",
    "            update_list.append(rand_point_1[i] * update_mean)\n",
    "        update_list.append(update_mean)\n",
    "        rand_point_2 = np.sort(np.random.random(num_rand_point_2))\n",
    "        for i in range(len(rand_point_2)):\n",
    "            update_list.append(update_mean + rand_point_2[i] * update_std)\n",
    "        update_list.append(update_mean + update_std +  np.random.random(num_rand_point_3)[0] * update_std)        \n",
    "        update_list.append(update_max)\n",
    "        #----------------------------------\n",
    "        layer_number = 2 #supposed to come from layer locator\n",
    "        layer_size   = model.layers[(layer_number*2) - 1].input_shape[1]\n",
    "        layer_name   = model.layers[(layer_number*2) - 1]\n",
    "        num_layers   = len(model.layers)\n",
    "\n",
    "        all_dic = {}\n",
    "        accu_neuron = {}\n",
    "        acc_try = {}\n",
    "        sample_ind = 0\n",
    "        for sample in samples:       \n",
    "            update_list_man = np.array([0] * layer_size)\n",
    "            m_samples  = m_instance( np.array([sample]), sens_params, data_config[dataset])\n",
    "            change_dic = {}\n",
    "            for i in range(11):\n",
    "                update_list_man = update_list[i]               \n",
    "                x = neuron_locator(sess, model, m_samples, layer_number,model_path,\n",
    "                               input_shape, nb_classes, dataset, sens_params, update_list_man )\n",
    "                if sample_ind == 0:\n",
    "                    accu_neuron = {}\n",
    "                    for neuron in range(len(update_list_man)):\n",
    "                        accu_neuron[neuron] = model_acc(sess, model,model_path,\n",
    "                                         input_shape, nb_classes, dataset, sens_params,\n",
    "                                         neuron,X,Y,layer_number,num_layers,update_list_man)\n",
    "                    acc_try[i] = accu_neuron                 \n",
    "                change_dic[i] = x  \n",
    "            all_dic[sample_ind] = change_dic\n",
    "            clear_output(wait=True)\n",
    "            print(sample_ind)\n",
    "            sample_ind += 1\n",
    "\n",
    "        np.save('../results/'+dataset+'/RQ3/inik_'+str(num_samples)+'_'+str(trial)+'.npy',\n",
    "                np.array(ini_k_samples))\n",
    "        np.save('../results/'+dataset+'/accu_dic_'+str(num_samples)+'_'+str(trial)+'.npy',\n",
    "                acc_try) \n",
    "        np.save('../results/'+dataset+'/all_dic_'+str(num_samples)+'_'+str(trial)+'.npy', \n",
    "                all_dic)   \n",
    "              \n",
    "        accu_dic = dict(np.load('../results/'+dataset+'/accu_dic_'+str(num_samples)+'_'+str(trial)+'.npy',\n",
    "                                allow_pickle=True).item())  \n",
    "        all_dic  = dict(np.load('../results/'+dataset+'/all_dic_'+str(num_samples)+'_'+str(trial)+'.npy',\n",
    "                               allow_pickle=True).item())\n",
    "        ini_k = np.load('../results/'+dataset+'/RQ3/inik_'+str(num_samples)+'_'+str(trial)+'.npy')\n",
    "        \n",
    "        num_samples = len(all_dic.keys())\n",
    "        num_force   = len(all_dic[0].keys())\n",
    "        num_neuron  = len(all_dic[0][0].keys())\n",
    "        ini_k = np.repeat(ini_k, (num_force * num_neuron))\n",
    "        data  = np.zeros(((num_samples * num_force * num_neuron) ,4) , dtype = 'int32')\n",
    "        df    = pd.DataFrame(data,columns = ['sample','force','neuron','K'],dtype = 'int32')\n",
    "        sample_col = np.repeat(np.array([i for i in range(num_samples)]),(num_neuron * num_force))\n",
    "        force_col  = np.array([ int(i/num_neuron ) for i in range( num_neuron * num_force ) ] * num_samples)\n",
    "        neuron_col = np.array([i for i in range( num_neuron )] * ( num_samples*num_force ))\n",
    "        df['sample'] = sample_col\n",
    "        df['force']  = force_col\n",
    "        df['neuron'] = neuron_col\n",
    "        df['acc'] = 0\n",
    "        acc = pd.DataFrame(accu_dic).transpose().to_numpy()\n",
    "        acc = acc.reshape(acc.shape[0] * acc.shape[1],)\n",
    "        for i in range(len(all_dic.keys())):\n",
    "            temp = pd.DataFrame(all_dic[i]).transpose().to_numpy()\n",
    "            temp = temp.reshape(((len(all_dic[0][0].keys())) * len(all_dic[0].keys()),))   \n",
    "            df.loc[df.loc[(df['sample'] == i) ].index,'acc'] = acc\n",
    "            df.loc[df.loc[(df['sample'] == i) ].index,'K'] = temp\n",
    "        df['K'] = df['K'].transform(lambda x:x[0])\n",
    "        df['init_k'] = ini_k\n",
    "\n",
    "        R_act   = []\n",
    "        R_deact = []\n",
    "        diff_R  = []\n",
    "        ini_acc = 0.881\n",
    "        acc_e   = 0.05\n",
    "        for neuron in range(num_neuron):\n",
    "            k_deact = df.loc[(df['neuron'] == neuron) & (df['force'] <= 1) & \\\n",
    "                                  (df['acc'] >= ini_acc - acc_e)]['K'].mean()\n",
    "            k_act   = df.loc[(df['neuron'] == neuron) & (df['force'] > 1) & \\\n",
    "                                  (df['acc'] >= ini_acc - acc_e)]['K'].mean()\n",
    "            k_init  = df.loc[(df['neuron'] == neuron) & (df['acc'] >= ini_acc - acc_e)]['init_k'].mean()\n",
    "            R_act_temp   = (k_act - k_init) / k_init\n",
    "            R_deact_temp = (k_deact - k_init) / k_init\n",
    "            diff_R_temp  = R_act_temp - R_deact_temp\n",
    "            R_act.append(R_act_temp)\n",
    "            R_deact.append(R_deact_temp)\n",
    "            diff_R.append(diff_R_temp)\n",
    "  \n",
    "        df.to_csv('../results/'+dataset+'/RQ3/df_'+str(num_samples)+'_'+str(trial)+'.csv',index=False)\n",
    "        np.save('../results/'+dataset+'/RQ3/R_act_'+str(num_samples)+'_'+str(trial)+'.npy',R_act)\n",
    "        np.save('../results/'+dataset+'/RQ3/R_deact_'+str(num_samples)+'_'+str(trial)+'.npy',R_deact)\n",
    "        np.save('../results/'+dataset+'/RQ3/R_diffR_'+str(num_samples)+'_'+str(trial)+'.npy',diff_R)\n",
    "\n",
    "def main(argv = None):\n",
    "    time1 = time.time()\n",
    "    dnn_fair_testing(dataset = FLAGS.dataset, \n",
    "                     sens_params = FLAGS.sens_params,\n",
    "                     model_path  = FLAGS.model_path)\n",
    "    print(time.time() - time1 )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    flags.DEFINE_string(\"dataset\", \"census\", \"the name of dataset\")\n",
    "    flags.DEFINE_string('model_path', '../models/', 'the path for testing model')\n",
    "    flags.DEFINE_list('sens_params',[9,8,1],'sensitive parameters index.1 for age, 9 for gender, 8 for race')\n",
    "    tf.app.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "time1 = time.time()\n",
    "accu_dic = dict(np.load('../results/census/accu_dic_900_100.npy',allow_pickle=True).item())  \n",
    "all_dic = dict(np.load('../results/census/all_dic_900_100.npy',allow_pickle=True).item())\n",
    "ini_k = np.load('../results/census/ini_k_samples_900_100.npy')\n",
    "num_samples = len(all_dic.keys())\n",
    "num_force = len(all_dic[0].keys())\n",
    "num_neuron = len(all_dic[0][0].keys())\n",
    "ini_k = np.repeat(ini_k, (num_force * num_neuron))\n",
    "data = np.zeros(((num_samples * num_force *num_neuron) ,4) , dtype='int32')\n",
    "df = pd.DataFrame(data,columns=['sample','force','neuron','K'],dtype='int32')\n",
    "sample_col = np.repeat(np.array([i for i in range(num_samples)]),(num_neuron*num_force))\n",
    "force_col = np.array([ int(i/num_neuron ) for i in range(num_neuron*num_force)] * num_samples)\n",
    "neuron_col = np.array([i for i in range( num_neuron )] * ( num_samples*num_force ))\n",
    "df['sample'] = sample_col\n",
    "df['force'] = force_col\n",
    "df['neuron'] = neuron_col\n",
    "df['acc'] = 0\n",
    "acc = pd.DataFrame(accu_dic).transpose().to_numpy()\n",
    "acc = acc.reshape(acc.shape[0] * acc.shape[1],)\n",
    "for i in range(len(all_dic.keys())):\n",
    "    temp = pd.DataFrame(all_dic[i]).transpose().to_numpy()\n",
    "    temp = temp.reshape(((len(all_dic[0][0].keys())) * len(all_dic[0].keys()),))   \n",
    "    df.loc[df.loc[(df['sample']==i) ].index,'acc'] = acc\n",
    "    df.loc[df.loc[(df['sample']==i) ].index,'K'] = temp\n",
    "df['K']=df['K'].transform(lambda x:x[0])\n",
    "df['init_k'] = ini_k\n",
    "\n",
    "R_act   = []\n",
    "R_deact = []\n",
    "diff_R  = []\n",
    "ini_acc = 0.881\n",
    "acc_e = 0.05\n",
    "for neuron in range(num_neuron):\n",
    "    k_deact = df.loc[(df['neuron']==neuron) & (df['force']==0) & \\\n",
    "                          (df['acc'] >= ini_acc - acc_e)]['K'].mean()\n",
    "    k_act   = df.loc[(df['neuron']==neuron) & (df['force'] > 0) & \\\n",
    "                          (df['acc'] >= ini_acc - acc_e)]['K'].mean()\n",
    "    k_init  = df.loc[(df['neuron']==neuron) & (df['acc'] >= ini_acc - acc_e)]['init_k'].mean()\n",
    "    R_act_temp   = (k_act - k_init) / k_init\n",
    "    R_deact_temp = (k_deact - k_init) / k_init\n",
    "    diff_R_temp  = R_act_temp - R_deact_temp\n",
    "    R_act.append(R_act_temp)\n",
    "    R_deact.append(R_deact_temp)\n",
    "    diff_R.append(diff_R_temp)\n",
    "\n",
    "df.to_csv('../results/census/RQ3/df_'+str(num_samples)+'_2.csv',index=False)\n",
    "np.save('../results/census/RQ3/R_act_'+str(num_samples)+'_2.npy',R_act)\n",
    "np.save('../results/census/RQ3/R_deact_'+str(num_samples)+'_2.npy',R_deact)\n",
    "np.save('../results/census/RQ3/R_diffR_'+str(num_samples)+'_2.npy',diff_R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(len(diff_R)):\n",
    "    print(i,'-->',round(diff_R1[i],3),'    ',round(diff_R[i],3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
