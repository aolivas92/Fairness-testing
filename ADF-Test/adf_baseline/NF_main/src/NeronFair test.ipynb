{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/lib/python3/dist-packages/tensorflow/python/util/dispatch.py:1096: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0812 10:59:03.795379 140661202327360 deprecation.py:545] From /usr/lib/python3/dist-packages/tensorflow/python/util/dispatch.py:1096: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../models/census/test.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0812 10:59:03.848797 140661202327360 saver.py:1399] Restoring parameters from ../models/census/test.model\n",
      "`np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "`np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "802 0 234 Percentage discriminatory inputs of local search- 29.140722291407222\n",
      "1580 1 331 Percentage discriminatory inputs of local search- 20.936116382036683\n",
      "2392 2 436 Percentage discriminatory inputs of local search- 18.219807772670286\n",
      "3185 3 765 Percentage discriminatory inputs of local search- 24.01129943502825\n",
      "3992 4 998 Percentage discriminatory inputs of local search- 24.993739043325817\n",
      "4825 5 1112 Percentage discriminatory inputs of local search- 23.04185661002901\n",
      "5627 6 1307 Percentage discriminatory inputs of local search- 23.22316986496091\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf \n",
    "tf.compat.v1.disable_eager_execution()\n",
    "import sys, os\n",
    "sys.path.append(\"../\")\n",
    "import copy,time\n",
    "from tensorflow.python.platform import flags\n",
    "from scipy.optimize import basinhopping\n",
    "from nf_data.census import census_data\n",
    "from nf_data.credit import credit_data\n",
    "from nf_data.compas import compas_data\n",
    "from nf_data.default import default_data\n",
    "from nf_data.bank import bank_data\n",
    "from nf_data.heart import heart_data\n",
    "from nf_data.diabetes import diabetes_data\n",
    "from nf_data.students import students_data\n",
    "from nf_data.meps15 import meps15_data\n",
    "from nf_data.meps16 import meps16_data\n",
    "from nf_model.dnn_models import dnn\n",
    "from utils.utils_tf import model_prediction, model_argmax, model_loss\n",
    "from utils.config import census, credit, bank, compas, default, heart, diabetes, students , meps15, meps16\n",
    "from src.nf_utils import cluster, gradient_graph_neuron\n",
    "\n",
    "olderr = np.seterr(all='ignore')\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "FLAGS = flags.FLAGS\n",
    "perturbation_size = 1\n",
    "\n",
    "def check_for_error_condition(conf, sess, x, preds, t, sens):\n",
    "    \"\"\"\n",
    "    Check whether the test case is an individual discriminatory instance\n",
    "    :param conf: the configuration of dataset\n",
    "    :param sess: TF session\n",
    "    :param x: input placeholder\n",
    "    :param preds: the model's symbolic output\n",
    "    :param t: test case\n",
    "    :param sens: the index of sensitive feature\n",
    "    :return: whether it is an individual discriminatory instance\n",
    "    \"\"\"\n",
    "    t = t.astype('int')\n",
    "    label = model_argmax(sess, x, preds, np.array([t]))\n",
    "    # check for all the possible values of sensitive feature\n",
    "    for val in range(conf.input_bounds[sens-1][0], conf.input_bounds[sens-1][1]+1):\n",
    "        if val != t[sens-1]:\n",
    "            tnew = copy.deepcopy(t)\n",
    "            tnew[sens-1] = val\n",
    "            label_new = model_argmax(sess, x, preds, np.array([tnew]))\n",
    "            if label_new != label:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def seed_test_input(clusters, limit):\n",
    "    \"\"\"\n",
    "    Select the seed inputs for fairness testing\n",
    "    :param clusters: the results of K-means clustering\n",
    "    :param limit: the size of seed inputs wanted\n",
    "    :return: a sequence of seed inputs\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    rows = []\n",
    "    max_size = max([len(c[0]) for c in clusters])\n",
    "    while i < max_size:\n",
    "        if len(rows) == limit:\n",
    "            break\n",
    "        for c in clusters:\n",
    "            if i >= len(c[0]):\n",
    "                continue\n",
    "            row = c[0][i]\n",
    "            rows.append(row)\n",
    "            if len(rows) == limit:\n",
    "                break\n",
    "        i += 1\n",
    "    return np.array(rows)\n",
    "\n",
    "def clip(input, conf):\n",
    "    \"\"\"\n",
    "    Clip the generating instance with each feature to make sure it is valid\n",
    "    :param input: generating instance\n",
    "    :param conf: the configuration of dataset\n",
    "    :return: a valid generating instance\n",
    "    \"\"\"\n",
    "    for i in range(len(input)):\n",
    "        input[i] = max(input[i], conf.input_bounds[i][0])\n",
    "        input[i] = min(input[i], conf.input_bounds[i][1])\n",
    "    return input\n",
    "\n",
    "class Local_Perturbation(object):\n",
    "    \"\"\"\n",
    "    The  implementation of local perturbation\n",
    "    \"\"\"\n",
    "    def __init__(self, sess,  x, nx, x_grad, nx_grad, n_value, sens_param, input_shape, conf):\n",
    "        \"\"\"\n",
    "        Initial function of local perturbation\n",
    "        :param sess: TF session\n",
    "        :param x: input placeholder for x\n",
    "        :param nx: input placeholder for nx (sensitive attributes of nx and x are different)\n",
    "        :param x_grad: the gradient graph for x\n",
    "        :param nx_grad: the gradient graph for nx\n",
    "        :param n_value: the discriminatory value of sensitive feature\n",
    "        :param sens_param: the index of sensitive feature\n",
    "        :param input_shape: the shape of dataset\n",
    "        :param conf: the configuration of dataset\n",
    "        \"\"\"\n",
    "        self.sess = sess\n",
    "        self.grad = x_grad\n",
    "        self.ngrad = nx_grad\n",
    "        self.x = x\n",
    "        self.nx = nx\n",
    "        self.n_value = n_value\n",
    "        self.input_shape = input_shape\n",
    "        self.sens_param = sens_param\n",
    "        self.conf = conf\n",
    "\n",
    "    def softmax(self, m):\n",
    "        probs = np.exp(m - np.max(m))\n",
    "        probs /= np.sum(probs)\n",
    "        return probs\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Local perturbation\n",
    "        :param x: input instance for local perturbation\n",
    "        :return: new potential individual discriminatory instance\n",
    "        \"\"\"\n",
    "        # perturbation\n",
    "        s = np.random.choice([1.0, -1.0]) * perturbation_size\n",
    "        n_x = x.copy()\n",
    "        n_x[self.sens_param - 1] = self.n_value\n",
    "        # compute the gradients of an individual discriminatory instance pairs\n",
    "        ind_grad,n_ind_grad = self.sess.run([self.grad,self.ngrad], feed_dict={self.x:np.array([x]), self.nx: np.array([n_x])})\n",
    "\n",
    "        if np.zeros(self.input_shape).tolist() == ind_grad[0].tolist() and np.zeros(self.input_shape).tolist() == \\\n",
    "                n_ind_grad[0].tolist():\n",
    "            probs = 1.0 / (self.input_shape-1) * np.ones(self.input_shape)\n",
    "            probs[self.sens_param - 1] = 0\n",
    "        else:\n",
    "            # nomalize the reciprocal of gradients (prefer the low impactful feature)\n",
    "            grad_sum = 1.0 / (abs(ind_grad[0]) + abs(n_ind_grad[0]))\n",
    "            grad_sum[self.sens_param - 1] = 0\n",
    "            probs = grad_sum / np.sum(grad_sum)\n",
    "        probs = probs / probs.sum()\n",
    "        # probs = self.softmax(probs)\n",
    "\n",
    "        # randomly choose the feature for local perturbation\n",
    "        try:\n",
    "            index = np.random.choice(range(self.input_shape) , p=probs)\n",
    "        except:\n",
    "            index = 0\n",
    "        local_cal_grad = np.zeros(self.input_shape)\n",
    "        local_cal_grad[index] = 1.0\n",
    "        x = clip(x + s * local_cal_grad, self.conf).astype(\"int\")\n",
    "        return x\n",
    "\n",
    "def dnn_fair_testing(dataset, sensitive_param, model_path, cluster_num, max_global, max_local, max_iter, ReLU_name):\n",
    "    \"\"\"\n",
    "    The implementation of NF\n",
    "    :param dataset: the name of testing dataset\n",
    "    :param sensitive_param: the index of sensitive feature\n",
    "    :param model_path: the path of testing model\n",
    "    :param cluster_num: the number of clusters to form as well as the number of\n",
    "            centroids to generate\n",
    "    :param max_global: the maximum number of samples for global search\n",
    "    :param max_local: the maximum number of samples for local search\n",
    "    :param max_iter: the maximum iteration of global perturbation\n",
    "    :param ReLU_name: the name of bias layer of dnn model\n",
    "    \"\"\"\n",
    "    data = {\"census\":census_data, \"credit\":credit_data, \"bank\":bank_data, \"compas\":compas_data, \n",
    "            \"default\": default_data, \"heart\":heart_data, \"diabetes\":diabetes_data, \n",
    "            \"students\":students_data, \"meps15\":meps15_data, \"meps16\":meps16_data}\n",
    "    data_config = {\"census\":census, \"credit\":credit, \"bank\":bank, \"compas\":compas, \"default\":default,\n",
    "                  \"heart\":heart , \"diabetes\":diabetes,\"students\":students, \"meps15\":meps15, \"meps16\":meps16}\n",
    "\n",
    "    # prepare the testing data and model\n",
    "    X, Y, input_shape, nb_classes = data[dataset]()\n",
    "\n",
    "    def get_weights(X, sensitive_param, sess, x, nx, x_hidden, nx_hidden, alpha = 0.5):\n",
    "        nX = copy.copy(X)\n",
    "        senss = data_config[dataset].input_bounds[sensitive_param - 1]\n",
    "        eq = np.array(nX[:, sensitive_param - 1] == senss[0]).astype(np.int)\n",
    "        neq = -eq + 1\n",
    "        nX[:, sensitive_param - 1] = eq * senss[-1] + neq * senss[0]\n",
    "        sa, nsa = sess.run([x_hidden, nx_hidden], feed_dict={x: X, nx: nX})\n",
    "        sf = np.mean(np.abs(sa) + np.abs(nsa), axis=0)\n",
    "        # print(sf)\n",
    "        num = 0 if int(alpha * len(sf)) - 1 < 0 else int(alpha * len(sf)) - 1\n",
    "        ti = np.argsort(sf)[len(sf) - num - 1]\n",
    "        alpha = sf[ti]\n",
    "        weights = np.array(sf >= alpha).astype(np.int)\n",
    "        return weights\n",
    "\n",
    "    tf.set_random_seed(2020)\n",
    "    config = tf.ConfigProto(device_count = {'GPU': 0})\n",
    "    config.allow_soft_placement= True\n",
    "#     config = tf.ConfigProto()\n",
    "#     config.gpu_options.per_process_gpu_memory_fraction = 1\n",
    "    sd = 0\n",
    "    with tf.Graph().as_default():\n",
    "        sess = tf.Session(config=config)\n",
    "        x = tf.placeholder(tf.float32, shape=input_shape)\n",
    "        nx = tf.placeholder(tf.float32, shape=input_shape)\n",
    "        model = dnn(input_shape, nb_classes)\n",
    "        \n",
    "        preds = model(x)\n",
    "        x_hidden = model.get_layer(x, ReLU_name)\n",
    "        nx_hidden = model.get_layer(nx, ReLU_name)\n",
    "        \n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, model_path)\n",
    "        weights = get_weights(X,sensitive_param, sess,x,nx,x_hidden,nx_hidden)\n",
    "        x_grad,nx_grad = gradient_graph_neuron(x, nx, x_hidden, nx_hidden, weights)\n",
    "\n",
    "        clf = cluster(dataset, cluster_num)\n",
    "        clusters = [np.where(clf.labels_==i) for i in range(cluster_num)]\n",
    "        # store the result of fairness testing\n",
    "        tot_inputs = set()\n",
    "        global_disc_inputs = set()\n",
    "        global_disc_inputs_list = []\n",
    "        local_disc_inputs = set()\n",
    "        local_disc_inputs_list = []\n",
    "        value_list = []\n",
    "        suc_idx = []\n",
    "\n",
    "        def evaluate_local(inp):\n",
    "            \"\"\"\n",
    "            Evaluate whether the test input after local perturbation is an individual discriminatory instance\n",
    "            :param inp: test input\n",
    "            :return: whether it is an individual discriminatory instance\n",
    "            \"\"\"\n",
    "            result = check_for_error_condition(data_config[dataset], sess, x, preds, inp, sensitive_param)\n",
    "            temp = copy.deepcopy(inp.astype('int').tolist())\n",
    "            temp = temp[:sensitive_param - 1] + temp[sensitive_param:]\n",
    "            tot_inputs.add(tuple(temp))\n",
    "            if result and (tuple(temp) not in global_disc_inputs) and (tuple(temp) not in local_disc_inputs):\n",
    "                local_disc_inputs.add(tuple(temp))\n",
    "                local_disc_inputs_list.append(temp)\n",
    "            return not result\n",
    "        # select the seed input for fairness testing\n",
    "        time1 = time.time()\n",
    "        inputs = seed_test_input(clusters, min(max_global, len(X)))\n",
    "        # global flag, n_sample, n_label\n",
    "        for num in range(len(inputs)):\n",
    "            if time.time() - time1 >300:break\n",
    "            index = inputs[num]\n",
    "            sample = X[index:index+1]\n",
    "            memory1 = sample[0] * 0\n",
    "            memory2 = sample[0] * 0 + 1\n",
    "            memory3 = sample[0] * 0 - 1\n",
    "            # start global perturbation\n",
    "            for iter in range(max_iter+1):\n",
    "                if time.time() - time1 >300:break\n",
    "                probs = model_prediction(sess, x, preds, sample)[0]\n",
    "                label = np.argmax(probs)\n",
    "                prob = probs[label]\n",
    "                max_diff = 0\n",
    "                n_value = -1\n",
    "                # search the instance with maximum probability difference for global perturbation\n",
    "                for i in range(census.input_bounds[sensitive_param-1][0], census.input_bounds[sensitive_param-1][1] + 1):\n",
    "                    if i != sample[0][sensitive_param-1]:\n",
    "                        n_sample = sample.copy()\n",
    "                        n_sample[0][sensitive_param-1] = i\n",
    "                        n_probs = model_prediction(sess, x, preds, n_sample)[0]\n",
    "                        n_label = np.argmax(n_probs)\n",
    "                        n_prob = n_probs[n_label]\n",
    "                        if label != n_label:\n",
    "                            n_value = i\n",
    "                            break\n",
    "                        else:\n",
    "                            prob_diff = abs(prob - n_prob)\n",
    "                            if prob_diff > max_diff:\n",
    "                                max_diff = prob_diff\n",
    "                                n_value = i\n",
    "\n",
    "                temp = copy.deepcopy(sample[0].astype('int').tolist())\n",
    "                temp = temp[:sensitive_param - 1] + temp[sensitive_param:]\n",
    "                # if get an individual discriminatory instance\n",
    "                if label != n_label and (tuple(temp) not in global_disc_inputs) and (tuple(temp) not in local_disc_inputs):\n",
    "                    global_disc_inputs_list.append(temp)\n",
    "                    global_disc_inputs.add(tuple(temp))\n",
    "                    value_list.append([sample[0, sensitive_param - 1], n_value])\n",
    "                    suc_idx.append(index)\n",
    "                    # start local perturbation\n",
    "                    minimizer = {\"method\": \"L-BFGS-B\"}\n",
    "                    local_perturbation = Local_Perturbation(sess,  x, nx, x_grad, nx_grad,n_value, sensitive_param, input_shape[1], data_config[dataset])\n",
    "                    basinhopping(evaluate_local, sample, stepsize=1.0, take_step=local_perturbation,\n",
    "                                 minimizer_kwargs=minimizer,\n",
    "                                 niter=max_local)\n",
    "                    print(len(tot_inputs),num,len(local_disc_inputs),\"Percentage discriminatory inputs of local search- \" + str(\n",
    "                              float(len(local_disc_inputs)) / float(len(tot_inputs)+1) * 100))\n",
    "                    break\n",
    "\n",
    "                n_sample[0][sensitive_param - 1] = n_value\n",
    "                s_grad,n_grad ,sn_grad= sess.run([tf.sign(x_grad),tf.sign(nx_grad),tf.sign(x_grad+nx_grad)], feed_dict={x: sample,nx:n_sample})\n",
    "                # find the feature with same impact\n",
    "                if np.zeros(data_config[dataset].params).tolist() == s_grad[0].tolist():\n",
    "                    g_diff = n_grad[0]\n",
    "                elif np.zeros(data_config[dataset].params).tolist() == n_grad[0].tolist():\n",
    "                    g_diff = s_grad[0]\n",
    "                else:\n",
    "                    g_diff = np.array(s_grad[0] == n_grad[0], dtype=float)\n",
    "\n",
    "                g_diff[sensitive_param - 1] = 0\n",
    "                if np.zeros(input_shape[1]).tolist() == g_diff.tolist():\n",
    "                    g_diff = sn_grad[0]\n",
    "                    g_diff[sensitive_param - 1] = 0\n",
    "                if np.zeros(data_config[dataset].params).tolist() == s_grad[0].tolist() or np.array(memory1[0]).tolist()==np.array(memory3[0]).tolist():\n",
    "                    np.random.seed(seed = 2020+sd)\n",
    "                    sd += 1\n",
    "                    delta = perturbation_size\n",
    "                    s_grad[0] = np.random.randint(-delta, delta+1, (np.shape(s_grad[0])))\n",
    "\n",
    "                g_diff = np.ones(data_config[dataset].params)\n",
    "                g_diff[sensitive_param - 1] = 0\n",
    "                cal_grad = s_grad * g_diff  # g_diff:\n",
    "                memory1 = memory2\n",
    "                memory2 = memory3\n",
    "                memory3 = cal_grad\n",
    "                sample[0] = clip(sample[0] + perturbation_size * cal_grad[0], data_config[dataset]).astype(\"int\")\n",
    "                if iter == max_iter:\n",
    "                    break\n",
    "        print(\"Total Inputs are \" + str(len(tot_inputs)))\n",
    "        print(\"Total discriminatory inputs of global search- \" + str(len(global_disc_inputs)))\n",
    "        print(\"Total discriminatory inputs of local search- \" + str(len(local_disc_inputs)))\n",
    "\n",
    "        # storing the fairness testing result\n",
    "        base_path = './output/' + dataset + '/' + FLAGS.sens_name + '/'\n",
    "        if not os.path.exists(base_path):\n",
    "            os.makedirs(base_path)\n",
    "\n",
    "        np.save(base_path + 'global_samples.npy', np.array(global_disc_inputs_list))\n",
    "        np.save(base_path + 'local_samples.npy', np.array(local_disc_inputs_list))\n",
    "        np.save(base_path + 'suc_idx.npy', np.array(suc_idx))\n",
    "        np.save(base_path + 'suc_idx.npy', np.array(value_list))\n",
    "        print(len(global_disc_inputs_list),len(local_disc_inputs_list))\n",
    "        print(\"Total discriminatory inputs of global search- \" + str(len(global_disc_inputs)))\n",
    "        print(\"Total discriminatory inputs of local search- \" + str(len(local_disc_inputs)))\n",
    "\n",
    "def main(argv=None):\n",
    "    dnn_fair_testing(dataset=FLAGS.dataset,\n",
    "                     sensitive_param=FLAGS.sens_param,\n",
    "                     model_path=FLAGS.model_path,\n",
    "                     cluster_num=FLAGS.cluster_num,\n",
    "                     max_global=FLAGS.max_global,\n",
    "                     max_local=FLAGS.max_local,\n",
    "                     max_iter=FLAGS.max_iter,\n",
    "                     ReLU_name=FLAGS.ReLU_name)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    flags.DEFINE_string(\"dataset\", \"census\", \"the name of dataset\")\n",
    "    flags.DEFINE_string(\"sens_name\", \"gender\", \"the name of sens_param\")\n",
    "    flags.DEFINE_integer(\"sens_param\", 9, \"sensitive index, index start from 1, 9 for gender, 8 for race\")\n",
    "    flags.DEFINE_string(\"model_path\", \"../models/census/test.model\", \"the path for testing model\")\n",
    "    flags.DEFINE_integer(\"cluster_num\", 4, \"the number of clusters to form as well as the number of centroids to generate\")\n",
    "    flags.DEFINE_integer(\"max_global\", 1000, \"maximum number of samples for global search\")\n",
    "    flags.DEFINE_integer(\"max_local\", 1000, \"maximum number of samples for local search\")\n",
    "    flags.DEFINE_integer(\"max_iter\", 100, \"maximum iteration of global perturbation\")\n",
    "    flags.DEFINE_string(\"ReLU_name\", \"ReLU5\", \"the name of bias layer of dnn model\")\n",
    "    tf.app.run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-a64e90033483>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-a64e90033483>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Total discriminatory inputs of global search- 43\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "NF\n",
    "40-1000-9-census\n",
    "Total discriminatory inputs of global search- 43\n",
    "Total discriminatory inputs of local search- 6290\n",
    "----------------\n",
    "10-1000-9-census\n",
    "Total discriminatory inputs of global search- 42\n",
    "Total discriminatory inputs of local search- 5977\n",
    "---------------------------\n",
    "100-100-9-census\n",
    "Total discriminatory inputs of global search- 165\n",
    "Total discriminatory inputs of local search- 4644\n",
    "\n",
    "\n",
    "ADF\n",
    "10-1000-9-census\n",
    "Total discriminatory inputs of global search- 41\n",
    "Total discriminatory inputs of local search- 6075\n",
    "-----------------------------\n",
    "40-1000-9-census\n",
    "Total discriminatory inputs of global search- 36\n",
    "Total discriminatory inputs of local search- 5385\n",
    "-----------------------------\n",
    "100-100-9-census\n",
    "Total discriminatory inputs of global search- 19\n",
    "Total discriminatory inputs of local search- 449\n",
    "\n",
    "\n",
    "Ourtool\n",
    "10-1000-9-census\n",
    "Total discriminatory inputs of global search- 10\n",
    "Total discriminatory inputs of local search- 21402\n",
    "-----------------------------\n",
    "40-1000-9-census\n",
    "Total discriminatory inputs of global search- 9\n",
    "Total discriminatory inputs of local search- 22630\n",
    "-----------------------------\n",
    "100-100-9-census\n",
    "Total discriminatory inputs of global search- 61 \n",
    "Total discriminatory inputs of local search- 7785\n",
    "------------------------------\n",
    "100-1000-9\n",
    "Total discriminatory inputs of global search- 13 \n",
    "Total discriminatory inputs of local search- 24351"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
