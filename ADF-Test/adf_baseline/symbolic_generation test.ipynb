{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../models/census/test.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0725 12:58:32.600752 140628324833088 saver.py:1399] Restoring parameters from ../models/census/test.model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Inputs are 10\n",
      "Total discriminatory inputs of global search- 0 127\n",
      "Total discriminatory inputs of local search- 0 0\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py:3339: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import tensorflow.compat.v1 as tf \n",
    "tf.compat.v1.disable_eager_execution()\n",
    "from tensorflow.python.platform import flags\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "if sys.version_info.major==2:\n",
    "    from Queue import PriorityQueue\n",
    "else:\n",
    "    from queue import PriorityQueue\n",
    "from z3 import *\n",
    "import os\n",
    "import copy\n",
    "\n",
    "from adf_utils.config import census, credit, bank\n",
    "from adf_baseline.lime import lime_tabular\n",
    "from adf_model.tutorial_models import dnn\n",
    "from adf_data.census import census_data\n",
    "from adf_data.credit import credit_data\n",
    "from adf_data.bank import bank_data\n",
    "from adf_utils.utils_tf import model_argmax\n",
    "from adf_tutorial.utils import cluster\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "def seed_test_input(dataset, cluster_num, limit):\n",
    "    \"\"\"\n",
    "    Select the seed inputs for fairness testing\n",
    "    :param dataset: the name of dataset\n",
    "    :param clusters: the results of K-means clustering\n",
    "    :param limit: the size of seed inputs wanted\n",
    "    :return: a sequence of seed inputs\n",
    "    \"\"\"\n",
    "    # build the clustering model\n",
    "    clf = cluster(dataset, cluster_num)\n",
    "    clusters = [np.where(clf.labels_ == i) for i in range(cluster_num)]  # len(clusters[0][0])==32561\n",
    "\n",
    "    i = 0\n",
    "    rows = []\n",
    "    max_size = max([len(c[0]) for c in clusters])\n",
    "    while i < max_size:\n",
    "        if len(rows) == limit:\n",
    "            break\n",
    "        for c in clusters:\n",
    "            if i >= len(c[0]):\n",
    "                continue\n",
    "            row = c[0][i]\n",
    "            rows.append(row)\n",
    "        i += 1\n",
    "    return np.array(rows)\n",
    "\n",
    "def getPath(X, sess, x, preds, input, conf):\n",
    "    \"\"\"\n",
    "    Get the path from Local Interpretable Model-agnostic Explanation Tree\n",
    "    :param X: the whole inputs\n",
    "    :param sess: TF session\n",
    "    :param x: input placeholder\n",
    "    :param preds: the model's symbolic output\n",
    "    :param input: instance to interpret\n",
    "    :param conf: the configuration of dataset\n",
    "    :return: the path for the decision of given instance\n",
    "    \"\"\"\n",
    "\n",
    "    # use the original implementation of LIME\n",
    "    explainer = lime_tabular.LimeTabularExplainer(X,\n",
    "                                                  feature_names=conf.feature_name, class_names=conf.class_name, categorical_features=conf.categorical_features,\n",
    "                                                  discretize_continuous=True)\n",
    "    g_data = explainer.generate_instance(input, num_samples=5000)\n",
    "    g_labels = model_argmax(sess, x, preds, g_data)\n",
    "\n",
    "    # build the interpretable tree\n",
    "    tree = DecisionTreeClassifier(random_state=2019) #min_samples_split=0.05, min_samples_leaf =0.01\n",
    "    tree.fit(g_data, g_labels)\n",
    "\n",
    "    # get the path for decision\n",
    "    path_index = tree.decision_path(np.array([input])).indices\n",
    "    path = []\n",
    "    for i in range(len(path_index)):\n",
    "        node = path_index[i]\n",
    "        i = i + 1\n",
    "        f = tree.tree_.feature[node]\n",
    "        if f != -2:\n",
    "            left_count = tree.tree_.n_node_samples[tree.tree_.children_left[node]]\n",
    "            right_count = tree.tree_.n_node_samples[tree.tree_.children_right[node]]\n",
    "            left_confidence = 1.0 * left_count / (left_count + right_count)\n",
    "            right_confidence = 1.0 - left_confidence\n",
    "            if tree.tree_.children_left[node] == path_index[i]:\n",
    "                path.append([f, \"<=\", tree.tree_.threshold[node], left_confidence])\n",
    "            else:\n",
    "                path.append([f, \">\", tree.tree_.threshold[node], right_confidence])\n",
    "    return path\n",
    "\n",
    "def check_for_error_condition(conf, sess, x, preds, t, sens):\n",
    "    \"\"\"\n",
    "    Check whether the test case is an individual discriminatory instance\n",
    "    :param conf: the configuration of dataset\n",
    "    :param sess: TF session\n",
    "    :param x: input placeholder\n",
    "    :param preds: the model's symbolic output\n",
    "    :param t: test case\n",
    "    :param sens: the index of sensitive feature\n",
    "    :return: whether it is an individual discriminatory instance\n",
    "    \"\"\"\n",
    "    label = model_argmax(sess, x, preds, np.array([t]))\n",
    "    for val in range(conf.input_bounds[sens-1][0], conf.input_bounds[sens-1][1]+1):\n",
    "        if val != t[sens-1]:\n",
    "            tnew = copy.deepcopy(t)\n",
    "            tnew[sens-1] = val\n",
    "            label_new = model_argmax(sess, x, preds, np.array([tnew]))\n",
    "            if label_new != label:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def global_solve(path_constraint, arguments, t, conf):\n",
    "    \"\"\"\n",
    "    Solve the constraint for global generation\n",
    "    :param path_constraint: the constraint of path\n",
    "    :param arguments: the name of features in path_constraint\n",
    "    :param t: test case\n",
    "    :param conf: the configuration of dataset\n",
    "    :return: new instance through global generation\n",
    "    \"\"\"\n",
    "    s = Solver()\n",
    "    for c in path_constraint:\n",
    "        s.add(arguments[c[0]] >= conf.input_bounds[c[0]][0])\n",
    "        s.add(arguments[c[0]] <= conf.input_bounds[c[0]][1])\n",
    "        if c[1] == \"<=\":\n",
    "            s.add(arguments[c[0]] <= c[2])\n",
    "        else:\n",
    "            s.add(arguments[c[0]] > c[2])\n",
    "\n",
    "    if s.check() == sat:\n",
    "        m = s.model()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    tnew = copy.deepcopy(t)\n",
    "    for i in range(len(arguments)):\n",
    "        if m[arguments[i]] == None:\n",
    "            continue\n",
    "        else:\n",
    "            tnew[i] = int(m[arguments[i]].as_long())\n",
    "    return tnew.astype('int').tolist()\n",
    "\n",
    "def local_solve(path_constraint, arguments, t, index, conf):\n",
    "    \"\"\"\n",
    "    Solve the constraint for local generation\n",
    "    :param path_constraint: the constraint of path\n",
    "    :param arguments: the name of features in path_constraint\n",
    "    :param t: test case\n",
    "    :param index: the index of constraint for local generation\n",
    "    :param conf: the configuration of dataset\n",
    "    :return: new instance through global generation\n",
    "    \"\"\"\n",
    "    c = path_constraint[index]\n",
    "    s = Solver()\n",
    "    s.add(arguments[c[0]] >= conf.input_bounds[c[0]][0])\n",
    "    s.add(arguments[c[0]] <= conf.input_bounds[c[0]][1])\n",
    "    for i in range(len(path_constraint)):\n",
    "        if path_constraint[i][0] == c[0]:\n",
    "            if path_constraint[i][1] == \"<=\":\n",
    "                s.add(arguments[path_constraint[i][0]] <= path_constraint[i][2])\n",
    "            else:\n",
    "                s.add(arguments[path_constraint[i][0]] > path_constraint[i][2])\n",
    "\n",
    "    if s.check() == sat:\n",
    "        m = s.model()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    tnew = copy.deepcopy(t)\n",
    "    tnew[c[0]] = int(m[arguments[c[0]]].as_long())\n",
    "    return tnew.astype('int').tolist()\n",
    "\n",
    "def average_confidence(path_constraint):\n",
    "    \"\"\"\n",
    "    The average confidence (probability) of path\n",
    "    :param path_constraint: the constraint of path\n",
    "    :return: the average confidence\n",
    "    \"\"\"\n",
    "    r = np.mean(np.array(path_constraint)[:,3].astype(float))\n",
    "    return r\n",
    "\n",
    "def gen_arguments(conf):\n",
    "    \"\"\"\n",
    "    Generate the argument for all the features\n",
    "    :param conf: the configuration of dataset\n",
    "    :return: a sequence of arguments\n",
    "    \"\"\"\n",
    "    arguments = []\n",
    "    for i in range(conf.params):\n",
    "        arguments.append(Int(conf.feature_name[i]))\n",
    "    return arguments\n",
    "\n",
    "def symbolic_generation(dataset, sensitive_param, model_path, cluster_num, limit):\n",
    "    \"\"\"\n",
    "    The implementation of symbolic generation\n",
    "    :param dataset: the name of dataset\n",
    "    :param sensitive_param: the index of sensitive feature\n",
    "    :param model_path: the path of testing model\n",
    "    :param cluster_num: the number of clusters to form as well as the number of\n",
    "            centroids to generate\n",
    "    :param limit: the maximum number of test case\n",
    "    \"\"\"\n",
    "    data = {\"census\":census_data, \"credit\":credit_data, \"bank\":bank_data}\n",
    "    data_config = {\"census\":census, \"credit\":credit, \"bank\":bank}\n",
    "\n",
    "    # the rank for priority queue, rank1 is for seed inputs, rank2 for local, rank3 for global\n",
    "    rank1 = 5\n",
    "    rank2 = 1\n",
    "    rank3 = 10\n",
    "    T1 = 0.3\n",
    "\n",
    "    # prepare the testing data and model\n",
    "    X, Y, input_shape, nb_classes = data[dataset]()\n",
    "    arguments = gen_arguments(data_config[dataset])\n",
    "    model = dnn(input_shape, nb_classes)\n",
    "    x = tf.placeholder(tf.float32, shape=input_shape)\n",
    "    y = tf.placeholder(tf.float32, shape=(None, nb_classes))\n",
    "    preds = model(x)\n",
    "    tf.set_random_seed(1234)\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = 0.8\n",
    "    sess = tf.Session(config=config)\n",
    "    saver = tf.train.Saver()\n",
    "    model_path = model_path + dataset + \"/test.model\"\n",
    "    saver.restore(sess, model_path)\n",
    "\n",
    "    # store the result of fairness testing\n",
    "    global_disc_inputs = set()\n",
    "    global_disc_inputs_list = []\n",
    "    local_disc_inputs = set()\n",
    "    local_disc_inputs_list = []\n",
    "    tot_inputs = set()\n",
    "\n",
    "    # select the seed input for fairness testing\n",
    "    inputs = seed_test_input(dataset, cluster_num, limit)\n",
    "    q = PriorityQueue() # low push first\n",
    "    for inp in inputs[::-1]:\n",
    "        q.put((rank1,X[inp].tolist()))\n",
    "\n",
    "    visited_path = []\n",
    "    l_count = 0\n",
    "    g_count = 0\n",
    "    while len(tot_inputs) < limit and q.qsize() != 0:\n",
    "        t = q.get()\n",
    "        t_rank = t[0]\n",
    "        t = np.array(t[1])\n",
    "        found = check_for_error_condition(data_config[dataset], sess, x, preds, t, sensitive_param)\n",
    "        p = getPath(X, sess, x, preds, t, data_config[dataset])\n",
    "        temp = copy.deepcopy(t.tolist())\n",
    "        temp = temp[:sensitive_param - 1] + temp[sensitive_param:]\n",
    "\n",
    "        tot_inputs.add(tuple(temp))\n",
    "        if found:\n",
    "            if (tuple(temp) not in global_disc_inputs) and (tuple(temp) not in local_disc_inputs):\n",
    "                if t_rank > 2:\n",
    "                    global_disc_inputs.add(tuple(temp))\n",
    "                    global_disc_inputs_list.append(temp)\n",
    "                else:\n",
    "                    local_disc_inputs.add(tuple(temp))\n",
    "                    local_disc_inputs_list.append(temp)\n",
    "                if len(tot_inputs) == limit:\n",
    "                    break\n",
    "\n",
    "            # local search\n",
    "            for i in range(len(p)):\n",
    "                path_constraint = copy.deepcopy(p)\n",
    "                c = path_constraint[i]\n",
    "                if c[0] == sensitive_param - 1:\n",
    "                    continue\n",
    "\n",
    "                if c[1] == \"<=\":\n",
    "                    c[1] = \">\"\n",
    "                    c[3] = 1.0 - c[3]\n",
    "                else:\n",
    "                    c[1] = \"<=\"\n",
    "                    c[3] = 1.0 - c[3]\n",
    "\n",
    "                if path_constraint not in visited_path:\n",
    "                    visited_path.append(path_constraint)\n",
    "                    input = local_solve(path_constraint, arguments, t, i, data_config[dataset])\n",
    "                    l_count += 1\n",
    "                    if input != None:\n",
    "                        r = average_confidence(path_constraint)\n",
    "                        q.put((rank2 + r, input))\n",
    "\n",
    "        # global search\n",
    "        prefix_pred = []\n",
    "        for c in p:\n",
    "            if c[0] == sensitive_param - 1:\n",
    "                    continue\n",
    "            if c[3] < T1:\n",
    "                break\n",
    "\n",
    "            n_c = copy.deepcopy(c)\n",
    "            if n_c[1] == \"<=\":\n",
    "                n_c[1] = \">\"\n",
    "                n_c[3] = 1.0 - c[3]\n",
    "            else:\n",
    "                n_c[1] = \"<=\"\n",
    "                n_c[3] = 1.0 - c[3]\n",
    "            path_constraint = prefix_pred + [n_c]\n",
    "\n",
    "            # filter out the path_constraint already solved before\n",
    "            if path_constraint not in visited_path:\n",
    "                visited_path.append(path_constraint)\n",
    "                input = global_solve(path_constraint, arguments, t, data_config[dataset])\n",
    "                g_count += 1\n",
    "                if input != None:\n",
    "                    r = average_confidence(path_constraint)\n",
    "                    q.put((rank3-r, input))\n",
    "\n",
    "            prefix_pred = prefix_pred + [c]\n",
    "\n",
    "    # create the folder for storing the fairness testing result\n",
    "    if not os.path.exists('../results/'):\n",
    "        os.makedirs('../results/')\n",
    "    if not os.path.exists('../results/' + dataset + '/'):\n",
    "        os.makedirs('../results/' + dataset + '/')\n",
    "    if not os.path.exists('../results/'+ dataset + '/'+ str(sensitive_param) + '/'):\n",
    "        os.makedirs('../results/' + dataset + '/'+ str(sensitive_param) + '/')\n",
    "\n",
    "    # storing the fairness testing result\n",
    "    np.save('../results/' + dataset + '/' + str(sensitive_param) + '/global_samples_symbolic.npy',\n",
    "            np.array(global_disc_inputs_list))\n",
    "    np.save('../results/' + dataset + '/' + str(sensitive_param) + '/local_samples_symbolic.npy',\n",
    "            np.array(local_disc_inputs_list))\n",
    "\n",
    "    # print the overview information of result\n",
    "    print(\"Total Inputs are \" + str(len(tot_inputs)))\n",
    "    print(\"Total discriminatory inputs of global search- \" + str(len(global_disc_inputs)), g_count)\n",
    "    print(\"Total discriminatory inputs of local search- \" + str(len(local_disc_inputs)), l_count)\n",
    "\n",
    "def main(argv=None):\n",
    "    symbolic_generation(dataset=FLAGS.dataset,\n",
    "                        sensitive_param=FLAGS.sens_param,\n",
    "                        model_path=FLAGS.model_path,\n",
    "                        cluster_num=FLAGS.cluster_num,\n",
    "                        limit=FLAGS.sample_limit)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    flags.DEFINE_string('dataset', 'census', 'the name of dataset')\n",
    "    flags.DEFINE_integer('sens_param', 9, 'sensitive index, index start from 1, 9 for gender, 8 for race.')\n",
    "    flags.DEFINE_string('model_path', '../models/', 'the path for testing model')\n",
    "    flags.DEFINE_integer('sample_limit', 10, 'number of samples to search')\n",
    "    flags.DEFINE_integer('cluster_num', 4, 'the number of clusters to form as well as the number of centroids to generate')\n",
    "\n",
    "    tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
