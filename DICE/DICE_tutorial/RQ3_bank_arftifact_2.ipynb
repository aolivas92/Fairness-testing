{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_neuron 2 32\n",
      "Time to intervene 102.46068620681763\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To exit: use 'exit', 'quit', or Ctrl-D.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import  os\n",
    "#import psutil\n",
    "# p = psutil.Process(os.getpid())\n",
    "# p.cpu_affinity(0)\n",
    "import numpy as np\n",
    "from itertools import product, combinations\n",
    "import tensorflow.compat.v1 as tf \n",
    "tf.compat.v1.disable_eager_execution()\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import copy\n",
    "import time\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from tensorflow.python.platform import flags\n",
    "from DICE_data.census import census_data\n",
    "from DICE_data.credit import credit_data\n",
    "from DICE_data.compas import compas_data\n",
    "from DICE_data.default import default_data\n",
    "from DICE_data.bank import bank_data\n",
    "from DICE_data.heart import heart_data\n",
    "from DICE_data.diabetes import diabetes_data\n",
    "from DICE_data.students import students_data\n",
    "from DICE_data.meps15 import meps15_data\n",
    "from DICE_data.meps16 import meps16_data\n",
    "from DICE_model.tutorial_models import dnn\n",
    "from DICE_utils.utils_tf import model_prediction, model_argmax , layer_out, model_eval\n",
    "from DICE_utils.config import census, credit, bank, compas, default, heart, diabetes, students , meps15, meps16\n",
    "from IPython.display import clear_output\n",
    "import csv\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "       \n",
    "def m_instance( sample, sens_params, conf):\n",
    "    index = []\n",
    "    m_sample = []\n",
    "    for sens in sens_params:\n",
    "        index.append([i for i in range(conf.input_bounds[sens - 1][0], conf.input_bounds[sens-1][1] + 1)])\n",
    "      \n",
    "    for ind in list(product(*index)):     \n",
    "        temp = sample.copy()\n",
    "        for i in range(len(sens_params)):\n",
    "            temp[0][sens_params[i]-1] = ind[i]\n",
    "        m_sample.append(temp)\n",
    "    return np.array(m_sample)\n",
    "    \n",
    "def clustering(probs,m_sample, sens_params, epsilon=0.025):\n",
    "    cluster_dic = {}\n",
    "    cluster_dic['Seed'] = m_sample[0][0]\n",
    "    bins= np.arange(0, 1, epsilon )\n",
    "    digitized = np.digitize(probs, bins) - 1\n",
    "    for  k in range(len(digitized)):\n",
    "\n",
    "        if digitized[k] not in cluster_dic.keys():        \n",
    "            cluster_dic[digitized[k]]=[ [m_sample[k][0][j - 1] for j in sens_params]]\n",
    "        else:\n",
    "            cluster_dic[digitized[k]].append( [m_sample[k][0][j - 1] for j in sens_params])\n",
    "    return cluster_dic \n",
    "\n",
    "    \n",
    "def pred_prob(sess, x, preds, m_sample, input_shape):\n",
    "        probs = model_prediction(sess, x, preds, np.array(m_sample).reshape(len(m_sample),\n",
    "                                    input_shape[1]))[:,1:2].reshape(len(m_sample))\n",
    "        return probs        \n",
    "        \n",
    "def neuron_locator(sess, model, samples, layer_number,model_path, input_shape, \n",
    "                   nb_classes, dataset, sens_params, update_list  ):\n",
    "        \n",
    "        if  sess._closed:\n",
    "#             config = tf.ConfigProto()\n",
    "#             config.gpu_options.per_process_gpu_memory_fraction = 0.8\n",
    "            config = tf.ConfigProto(device_count = {'GPU': 0})\n",
    "            config.allow_soft_placement= True            \n",
    "            sess   = tf.Session(config = config)\n",
    "            x      = tf.placeholder(tf.float32, shape = input_shape)\n",
    "            y      = tf.placeholder(tf.float32, shape = (None, nb_classes))\n",
    "            model  = dnn(input_shape, nb_classes)   \n",
    "            preds  = model(x)\n",
    "            saver  = tf.train.Saver()\n",
    "            saver.restore(sess, model_path)\n",
    "            \n",
    "        num_layers = len(model.layers)\n",
    "        feed_dic = {}\n",
    "        for neuron in range(len(update_list)):           \n",
    "            for layer in range(0,num_layers - 1,2):\n",
    "                if layer == 0:\n",
    "                    l = model.layers[layer].fprop(samples.astype('float32'))\n",
    "                else:\n",
    "                    l = model.layers[layer].fprop(r)                   \n",
    "                if layer + 1 == (layer_number * 2) - 1:\n",
    "                    indices = []\n",
    "                    for instance in range(l.shape[0]):                       \n",
    "                        indices.append([ instance, 0, neuron])       \n",
    "                    updates = [ update_list[ neuron ] ] * l.shape[0]\n",
    "                    r = model.layers[layer + 1].fprop(l , indices, updates)\n",
    "                else:\n",
    "                    r = model.layers[layer + 1].fprop(l)\n",
    "            feed_dic[neuron] = r\n",
    "        all_probs = sess.run(feed_dic)\n",
    "        out_dic   = {}\n",
    "        for key in all_probs.keys():\n",
    "\n",
    "            probs = np.array(all_probs[key]).reshape((np.array(all_probs[key]).shape[0],2))[:,1:].reshape((np.array(all_probs[key]).shape[0]))\n",
    "            clus  = clustering(probs,samples, sens_params)\n",
    "            out_dic[key] = [len(clus) - 1 ]\n",
    "        sess.close()\n",
    "        tf.reset_default_graph()\n",
    "        return out_dic\n",
    "    \n",
    "def model_acc(sess, model,model_path,input_shape, nb_classes,\n",
    "              dataset, sens_params,neuron,X,Y,layer_number,num_layers,update_list):\n",
    "        \n",
    "        if  sess._closed:\n",
    "#                 config = tf.ConfigProto()\n",
    "#                 config.gpu_options.per_process_gpu_memory_fraction = 0.8\n",
    "                config = tf.ConfigProto(device_count = {'GPU': 0})\n",
    "                config.allow_soft_placement= True\n",
    "                sess   = tf.Session(config = config)\n",
    "                x      = tf.placeholder(tf.float32, shape = input_shape)\n",
    "                y      = tf.placeholder(tf.float32, shape = (None, nb_classes))\n",
    "                model  = dnn(input_shape, nb_classes)   \n",
    "                preds  = model(x)\n",
    "                saver  = tf.train.Saver()\n",
    "                saver.restore(sess, model_path)\n",
    "        feed_dic = {}        \n",
    "        for layer in range(0,num_layers - 1,2):\n",
    "            if layer == 0:\n",
    "                l = model.layers[layer].fprop(X.astype('float32'))\n",
    "            else:\n",
    "                l = model.layers[layer].fprop(r)          \n",
    "            if layer + 1 == (layer_number * 2) - 1:\n",
    "                indices = []\n",
    "                for instance in range(l.shape[0]):                       \n",
    "                    indices.append([ instance, neuron])                \n",
    "                updates = [ update_list[ neuron ] ] * l.shape[0]                \n",
    "                r = model.layers[layer + 1].fprop(l , indices, updates)\n",
    "            else:\n",
    "                r = model.layers[layer + 1].fprop(l)             \n",
    "        all_probs = sess.run(r)\n",
    "        out_class = []\n",
    "        for out in all_probs:\n",
    "            out_class.append(np.argmax(out))\n",
    "        truth_val = []\n",
    "        for tr in Y:\n",
    "                truth_val.append(np.argmax(tr))\n",
    "        acc = 0\n",
    "        for i in range(len(out_class)):\n",
    "            if out_class[i] == truth_val[i]:\n",
    "                acc += 1\n",
    "        accuracy = round(acc/len(out_class),3)\n",
    "        sess.close()\n",
    "        tf.reset_default_graph()\n",
    "        return accuracy \n",
    "\n",
    "def get_rate(sess, model, model_path, input_shape, nb_classes,\n",
    "              dataset, lay_name, layer_output):\n",
    "        \n",
    "        def get_distance(vec1, vec2, size):\n",
    "            return abs(vec1 - vec2).sum() / size\n",
    "        \n",
    "        max_dis = 0\n",
    "        epsillon = 10 ** -7\n",
    "        num_samples = len(layer_output[lay_name])\n",
    "        #print('lay_name',lay_name)\n",
    "        layer_ind = np.where(np.array(list(layer_output.keys())) == lay_name)[0][0]\n",
    "\n",
    "        for ind in range(layer_ind):\n",
    "            temp_dis = 0\n",
    "            if 'ReLU' in np.array(list(layer_output.keys()))[ind]:\n",
    "                layer_name = np.array(list(layer_output.keys()))[ind]\n",
    "                layer_size  = len(layer_output[layer_name][0][0])\n",
    "                distances = np.zeros((num_samples,num_samples))\n",
    "                \n",
    "                for i in combinations(range(num_samples),2):\n",
    "                    distances[i[0],i[1]] = get_distance(layer_output[layer_name][i[0]],\n",
    "                                                        layer_output[layer_name][i[1]],layer_size)\n",
    "                if distances.max()> max_dis:\n",
    "                    max_dis = distances.max()                                      \n",
    "        distances = np.zeros((num_samples,num_samples))       \n",
    "        for i in combinations(range(num_samples),2):\n",
    "            distances[i[0],i[1]] = get_distance(layer_output[lay_name][i[0]],layer_output[lay_name][i[1]],len(layer_output[lay_name][0][0]))\n",
    "        cur_dis = distances.max()\n",
    "        change_rate = (cur_dis - max_dis ) / (max_dis + epsillon)\n",
    "        return change_rate\n",
    "    \n",
    "def layer_locator(sess, model, model_path,sens_params, input_shape, nb_classes,\n",
    "              dataset,conf, samples):\n",
    "        if  sess._closed:\n",
    "    #                 config = tf.ConfigProto()\n",
    "    #                 config.gpu_options.per_process_gpu_memory_fraction = 0.8\n",
    "            config = tf.ConfigProto(device_count = {'GPU': 0})\n",
    "            config.allow_soft_placement= True\n",
    "            sess   = tf.Session(config = config)\n",
    "            x      = tf.placeholder(tf.float32, shape = input_shape)\n",
    "            y      = tf.placeholder(tf.float32, shape = (None, nb_classes))\n",
    "            model  = dnn(input_shape, nb_classes)   \n",
    "            preds  = model(x)\n",
    "            saver  = tf.train.Saver()\n",
    "            saver.restore(sess, model_path)\n",
    "            \n",
    "        layer_list = []\n",
    "        layer_rate = []\n",
    "        for sample in samples:            \n",
    "            samples = m_instance( np.array([sample]) , sens_params, conf)\n",
    "            layer_output = layer_out(sess,model,np.array(samples).astype('float32')) \n",
    "            temp_list = []\n",
    "            for layer in layer_output.keys():\n",
    "                if 'ReLU' in layer:\n",
    "                    temp_rate = get_rate(sess, model, model_path, input_shape, nb_classes,\n",
    "                                          dataset,layer, layer_output)\n",
    "                    temp_list.append(temp_rate) \n",
    "            layer_rate.append(max(temp_list[1:]))\n",
    "                              \n",
    "            layer_list.append((np.argmax(np.array(temp_list[1:])) + 2 ))\n",
    "        sess.close()\n",
    "        tf.reset_default_graph()\n",
    "        print()\n",
    "        return stats.mode(layer_list)[0][0], np.array(layer_rate).mean()    \n",
    "#-------------------------------------------\n",
    "    \n",
    "def dnn_fair_testing(dataset, sens_params, model_path):\n",
    "\n",
    "    data = {\"census\":census_data, \"credit\":credit_data, \"bank\":bank_data, \"compas\":compas_data, \n",
    "            \"default\": default_data, \"heart\":heart_data, \"diabetes\":diabetes_data, \n",
    "            \"students\":students_data, \"meps15\":meps15_data, \"meps16\":meps16_data}\n",
    "    data_config = {\"census\":census, \"credit\":credit, \"bank\":bank, \"compas\":compas, \"default\":default,\n",
    "                  \"heart\":heart , \"diabetes\":diabetes,\"students\":students, \"meps15\":meps15, \"meps16\":meps16}\n",
    "    global df,RQ3_table\n",
    "    # prepare the testing data and model\n",
    "    X, Y, input_shape, nb_classes = data[dataset]()\n",
    "    tf.set_random_seed(1234)\n",
    "    layer_numbers=[]\n",
    "    layer_influence = []\n",
    "    RQ3_table = []\n",
    "    for trial in range(1):\n",
    "        config = tf.ConfigProto(device_count = {'GPU': 0})\n",
    "        config.allow_soft_placement= True\n",
    "        sess  = tf.Session(config = config)\n",
    "        x     = tf.placeholder(tf.float32, shape = input_shape)\n",
    "        y     = tf.placeholder(tf.float32, shape = (None, nb_classes))\n",
    "        model = dnn(input_shape, nb_classes)   \n",
    "        preds = model(x)\n",
    "        saver = tf.train.Saver()\n",
    "        model_path ='../models/'\n",
    "        model_path = model_path + dataset + \"/test.model\"\n",
    "        saver.restore(sess, model_path)\n",
    "        eval_params = {'batch_size': 128}\n",
    "        ini_acc = round(model_eval(sess, x, y, preds, X, Y, args=eval_params),3)\n",
    "        time1 = time.time()\n",
    "    # Loading the result of QID\n",
    "        layer_output = layer_out(sess,model,X.astype('float32'))\n",
    "        input_df  = pd.read_csv('../results/' + dataset + '/OurTool/RQ1&2/'+ ''.join(str(i) for i in sens_params)+'_10runs'+'/total_disc_'+str(trial)+'.csv',header='infer')\n",
    "        input_df = input_df.drop(columns=['Unnamed: 0'])       \n",
    "        sample_df = input_df.copy()\n",
    "        sample_df_rand = sample_df.sample(n = 90,axis = 0,random_state = np.random.RandomState())\n",
    "        sample_df_maxk = sample_df.sort_values(by = 'k',ascending=False).head(10)\n",
    "        sample_df = pd.concat([sample_df_rand,sample_df_maxk])\n",
    "        ini_k_samples = sample_df['k']\n",
    "        sample_df = sample_df.drop(columns = ['sh_entropy', 'k', 'disc', 'min_entropy','time']) \n",
    "        samples   = sample_df.to_numpy()\n",
    "        num_samples = len(samples)\n",
    "#         layer_number, layer_rate = layer_locator(sess, model, model_path, sens_params, input_shape, nb_classes,\n",
    "#               dataset,data_config[dataset], samples)\n",
    "        layer_number = 2\n",
    "        layer_rate =11\n",
    "        layer_numbers.append(layer_number)\n",
    "        layer_influence.append(layer_rate)\n",
    "        \n",
    "        #-----------------------------\n",
    "        update_df = layer_output['ReLU'+str((2*layer_number) - 1 )]\n",
    "        update_min  = np.min(update_df,axis=0)\n",
    "#         update_max  = np.max(update_df,axis=0)\n",
    "        update_mean = np.mean(update_df,axis=0)\n",
    "#         update_std  = np.std(update_df,axis=0)\n",
    "        update_list = []\n",
    "        update_list.append(update_min)      \n",
    "        update_list.append(update_mean)\n",
    "        layer_size   = model.layers[(layer_number*2) - 1].input_shape[1]\n",
    "        layer_name   = model.layers[(layer_number*2) - 1]\n",
    "        num_layers   = len(model.layers)\n",
    "        num_trial = len(update_list)\n",
    "        all_dic = {}\n",
    "        accu_neuron = {}\n",
    "        acc_try = {}\n",
    "        sample_ind = 0\n",
    "        for sample in samples:       \n",
    "            update_list_man = np.array([0] * layer_size)\n",
    "            m_samples  = m_instance( np.array([sample]), sens_params, data_config[dataset])\n",
    "            change_dic = {}\n",
    "            for i in range(num_trial):\n",
    "                update_list_man = update_list[i]\n",
    "                x = neuron_locator(sess, model, m_samples, layer_number,model_path,\n",
    "                               input_shape, nb_classes, dataset, sens_params, update_list_man )\n",
    "                if sample_ind == 0:\n",
    "                    accu_neuron = {}\n",
    "                    for neuron in range(len(update_list_man)):\n",
    "                        accu_neuron[neuron] = model_acc(sess, model,model_path,\n",
    "                                         input_shape, nb_classes, dataset, sens_params,\n",
    "                                         neuron,X,Y,layer_number,num_layers,update_list_man)\n",
    "                    acc_try[i] = accu_neuron                 \n",
    "                change_dic[i] = x  \n",
    "            all_dic[sample_ind] = change_dic\n",
    "            clear_output(wait=True)\n",
    "            sample_ind += 1\n",
    "\n",
    "        # create the folder for storing the fairness testing result\n",
    "        if not os.path.exists('../results/'):\n",
    "            os.makedirs('../results/')\n",
    "        if not os.path.exists('../results/' + dataset + '/'):\n",
    "            os.makedirs('../results/' + dataset + '/')\n",
    "        if not os.path.exists('../results/' + dataset + '/OurTool/'):\n",
    "            os.makedirs('../results/' + dataset + '/OurTool/')\n",
    "        if not os.path.exists('../results/' + dataset + '/OurTool/RQ3/'):\n",
    "            os.makedirs('../results/' + dataset + '/OurTool/RQ3/')          \n",
    "        if not os.path.exists('../results/' + dataset + '/OurTool/RQ3/table2/'):\n",
    "            os.makedirs('../results/' + dataset + '/OurTool/RQ3/table2/')\n",
    "#         np.save('../results/'+dataset+'/OurTool/RQ3/table2/inik_'+str(num_samples)+'_'+str(trial)+'.npy',\n",
    "#                 np.array(ini_k_samples))\n",
    "#         np.save('../results/'+dataset+'/OurTool/RQ3/table2/accu_dic_'+str(num_samples)+'_'+str(trial)+'.npy',\n",
    "#                 acc_try) \n",
    "#         np.save('../results/'+dataset+'/OurTool/RQ3/table2/all_dic_'+str(num_samples)+'_'+str(trial)+'.npy', \n",
    "#                 all_dic)   \n",
    "        accu_dic =  acc_try \n",
    "        ini_k = np.array(ini_k_samples)\n",
    "        num_samples = len(all_dic.keys())\n",
    "        num_force   = len(all_dic[0].keys())\n",
    "        num_neuron  = len(all_dic[0][0].keys())\n",
    "        print('num_neuron',layer_number,num_neuron)\n",
    "        ini_k = np.repeat(ini_k, (num_force * num_neuron))\n",
    "        data  = np.zeros(((num_samples * num_force * num_neuron) ,4) , dtype = 'int32')\n",
    "        df    = pd.DataFrame(data,columns = ['sample','force','neuron','K'],dtype = 'int32')\n",
    "        sample_col = np.repeat(np.array([i for i in range(num_samples)]),(num_neuron * num_force))\n",
    "        force_col  = np.array([ int(i/num_neuron ) for i in range( num_neuron * num_force ) ] * num_samples)\n",
    "        neuron_col = np.array([i for i in range( num_neuron )] * ( num_samples*num_force ))\n",
    "        df['sample'] = sample_col\n",
    "        df['force']  = force_col\n",
    "        df['neuron'] = neuron_col\n",
    "        df['acc'] = 0\n",
    "        acc = pd.DataFrame(accu_dic).transpose().to_numpy()\n",
    "        acc = acc.reshape(acc.shape[0] * acc.shape[1],)\n",
    "        for i in range(len(all_dic.keys())):\n",
    "            temp = pd.DataFrame(all_dic[i]).transpose().to_numpy()\n",
    "            temp = temp.reshape(((len(all_dic[0][0].keys())) * len(all_dic[0].keys()),))   \n",
    "            df.loc[df.loc[(df['sample'] == i) ].index,'acc'] = acc\n",
    "            df.loc[df.loc[(df['sample'] == i) ].index,'K'] = temp\n",
    "        df['K'] = df['K'].transform(lambda x:x[0])\n",
    "        df['init_k'] = ini_k\n",
    "\n",
    "        R_act   = []\n",
    "        R_deact = []\n",
    "        diff_R  = []\n",
    "        acc_e   = 0.05\n",
    "        for neuron in range(num_neuron):\n",
    "            k_deact = df.loc[(df['neuron'] == neuron) & (df['force'] == 0) & \\\n",
    "                                  (df['acc'] >= ini_acc - acc_e)]['K'].mean()\n",
    "            k_act   = df.loc[(df['neuron'] == neuron) & (df['force'] == 1) & \\\n",
    "                                  (df['acc'] >= ini_acc - acc_e)]['K'].mean()\n",
    "            k_init  = df.loc[(df['neuron'] == neuron) & (df['acc'] >= ini_acc - acc_e)]['init_k'].mean()\n",
    "            R_act_temp   = (k_act - k_init) / k_init\n",
    "            R_deact_temp = (k_deact - k_init) / k_init\n",
    "            diff_R_temp  = R_act_temp - R_deact_temp\n",
    "            R_act.append(R_act_temp)\n",
    "            R_deact.append(R_deact_temp)\n",
    "            diff_R.append(diff_R_temp)\n",
    "\n",
    "        RQ3_table.append(diff_R)\n",
    "#         df.to_csv('../results/'+dataset+'/OurTool/RQ3/table2/df_'+str(num_samples)+'_'+str(trial)+'.csv',index=False)\n",
    "#         np.save('../results/'+dataset+'/OurTool/RQ3/table2/R_act_'+str(num_samples)+'_'+str(trial)+'.npy',R_act)\n",
    "#         np.save('../results/'+dataset+'/OurTool/RQ3/table2/R_deact_'+str(num_samples)+'_'+str(trial)+'.npy',R_deact)\n",
    "#         np.save('../results/'+dataset+'/OurTool/RQ3/table2/R_diffR_'+str(num_samples)+'_'+str(trial)+'.npy',diff_R)\n",
    "    RQ3_table = np.mean(RQ3_table,axis=0)\n",
    "    #RQ3_table[np.where(np.isnan(RQ3_table)==True)[0]]=np.mean(RQ3_table)\n",
    "    with open('../results/'+dataset+'/OurTool/RQ3/table2/RQ3_table_'+str(num_samples)+'.csv', 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['Biased layer'] + [' Layer influence'] + ['Neuron+1', 'Neuron+2','Neuron+3', \n",
    "                                                                   'Neuron-1', 'Neuron-2', 'Neuron-3'])\n",
    "        \n",
    "        writer.writerow(np.append(RQ3_table, [stats.mode(layer_numbers)[0][0], np.mean(layer_influence)], axis=0) )\n",
    "\n",
    "        \n",
    "#     np.save('../results/'+dataset+'/OurTool/RQ3/table2/res_layers_'+str(num_samples)+'.npy',layer_numbers)\n",
    "#     np.save('../results/'+dataset+'/OurTool/RQ3/table2/layer_influence'+str(num_samples)+'.npy',layer_influence)\n",
    "#     np.save('../results/'+dataset+'/OurTool/RQ3/table2/RQ3_table'+str(num_samples)+'.npy',RQ3_table)\n",
    "    print('Time to intervene',time.time() - time1)\n",
    "def main(argv = None):\n",
    "    \n",
    "    dnn_fair_testing(dataset = FLAGS.dataset, \n",
    "                     sens_params = FLAGS.sens_params,\n",
    "                     model_path  = FLAGS.model_path)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    flags.DEFINE_string(\"dataset\", \"census\", \"the name of dataset\")\n",
    "    flags.DEFINE_string('model_path', '../models/', 'the path for testing model')\n",
    "    flags.DEFINE_list('sens_params',[9,8,1],'sensitive parameters index.1 for age, 9 for gender, 8 for race')\n",
    "    tf.app.run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00675676,  0.        ,  0.        , -0.10706861, -0.05509356,\n",
       "       -0.03950104,  0.00883576, -0.02494802,  0.        , -0.09355509,\n",
       "       -0.01611227, -0.00883576, -0.08523909, -0.06548857,  0.13877339,\n",
       "        0.27338877,  0.        , -0.02442827,         nan, -0.13357588,\n",
       "        0.        , -0.00363825, -0.1029106 , -0.04209979,  0.25051975,\n",
       "       -0.02754678, -0.02182952,         nan, -0.04209979,  0.        ,\n",
       "       -0.01455301, -0.00935551])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RQ_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_eff_ind = np.where(RQ_table<0)[0]\n",
    "neg_eff_ind = np.where(RQ_table>0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00883576, 0.13877339, 0.27338877, 0.25051975])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RQ_table[neg_eff]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 3, 2])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(RQ_table[neg_eff])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00883576, 0.13877339, 0.25051975, 0.27338877])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RQ_table[neg_eff_ind[np.argsort(RQ_table[neg_eff])]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
