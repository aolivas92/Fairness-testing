{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-8d4c1f7f7e7a>, line 666)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-8d4c1f7f7e7a>\"\u001b[0;36m, line \u001b[0;32m666\u001b[0m\n\u001b[0;31m    flags.DEFINE_integer('timeout', , 'search timeout')\u001b[0m\n\u001b[0m                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "from itertools import product\n",
    "import itertools\n",
    "import tensorflow.compat.v1 as tf \n",
    "tf.compat.v1.disable_eager_execution()\n",
    "import sys, os\n",
    "sys.path.append(\"../\")\n",
    "import copy\n",
    "import pandas as pd\n",
    "from tensorflow.python.platform import flags\n",
    "from scipy.optimize import basinhopping\n",
    "from scipy.stats import entropy\n",
    "import time\n",
    "\n",
    "from adf_data.census import census_data\n",
    "from adf_data.credit import credit_data\n",
    "from adf_data.compas import compas_data\n",
    "from adf_data.default import default_data\n",
    "from adf_data.bank import bank_data\n",
    "from adf_data.heart import heart_data\n",
    "from adf_data.diabetes import diabetes_data\n",
    "from adf_data.students import students_data\n",
    "from adf_data.meps15 import meps15_data\n",
    "from adf_data.meps16 import meps16_data\n",
    "\n",
    "from adf_model.tutorial_models import dnn\n",
    "from adf_utils.utils_tf import model_prediction, model_argmax , layer_out\n",
    "from adf_utils.config import census, credit, bank, compas, default, heart, diabetes, students , meps15, meps16\n",
    "from adf_tutorial.utils import cluster, gradient_graph\n",
    "#from IPython.display import clear_output\n",
    "\n",
    "# import argparse\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\"--dataset\", help='The name of dataset: census, credit, bank, default, meps21 ')\n",
    "# parser.add_argument(\"--sensitive_index\", help='The index for sensitive feature')\n",
    "# parser.add_argument(\"--time_out\", help='Max. running time', default = 14400, required=False)\n",
    "\n",
    "# args = parser.parse_args()\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "# step size of perturbation\n",
    "perturbation_size = 1\n",
    "\n",
    "def check_for_error_condition(conf, sess, x, preds, t, sens_params, input_shape, epsillon):\n",
    "    \"\"\"\n",
    "    Check whether the test case is an individual discriminatory instance\n",
    "    :param conf: the configuration of dataset\n",
    "    :param sess: TF session\n",
    "    :param x: input placeholder\n",
    "    :param preds: the model's symbolic output\n",
    "    :param t: test case\n",
    "    :param sens: the index of sensitive feature\n",
    "    :return: whether it is an individual discriminatory instance\n",
    "    \"\"\"\n",
    "\n",
    "    t = [t.astype('int')]   \n",
    "    samples = m_instance( np.array(t), sens_params, conf ) \n",
    "    pred = pred_prob(sess, x, preds, samples , input_shape )\n",
    "    partition = clustering(pred,samples, sens_params , epsillon)\n",
    "    #entropy_min = np.log2(len(partition)-1)#sh_entropy(pred, ent_tresh)\n",
    "    #entropy_sh = sh_entropy(pred, epsillon)\n",
    "    \n",
    "\n",
    "    return  max(list(partition.keys())[1:]) - min(list(partition.keys())[1:]), \\\n",
    "                                            len(partition)-1, conf#(len(partition) -1),\n",
    "    \n",
    "def seed_test_input(clusters, limit):\n",
    "    \"\"\"\n",
    "    Select the seed inputs for fairness testing\n",
    "    :param clusters: the results of K-means clustering\n",
    "    :param limit: the size of seed inputs wanted\n",
    "    :return: a sequence of seed inputs\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    rows = []\n",
    "    max_size = max([len(c[0]) for c in clusters])\n",
    "    while i < max_size:\n",
    "        if len(rows) == limit:\n",
    "            break\n",
    "        for c in clusters:\n",
    "            if i >= len(c[0]):\n",
    "                continue\n",
    "            row = c[0][i]\n",
    "            rows.append(row)\n",
    "            if len(rows) == limit:\n",
    "                break\n",
    "        i += 1\n",
    "    return np.array(rows)\n",
    "\n",
    "def clip(input, conf):\n",
    "    \"\"\"\n",
    "    Clip the generating instance with each feature to make sure it is valid\n",
    "    :param input: generating instance\n",
    "    :param conf: the configuration of dataset\n",
    "    :return: a valid generating instance\n",
    "    \"\"\"\n",
    "    for i in range(len(input)):\n",
    "        input[i] = max(input[i], conf.input_bounds[i][0])\n",
    "        input[i] = min(input[i], conf.input_bounds[i][1])\n",
    "    return input\n",
    "\n",
    "class Local_Perturbation(object):\n",
    "    \"\"\"\n",
    "    The  implementation of local perturbation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sess, grad, x, n_values, sens_params, input_shape, conf):\n",
    "        \"\"\"\n",
    "        Initial function of local perturbation\n",
    "        :param sess: TF session\n",
    "        :param grad: the gradient graph\n",
    "        :param x: input placeholder\n",
    "        :param n_value: the discriminatory value of sensitive feature\n",
    "        :param sens_param: the index of sensitive feature\n",
    "        :param input_shape: the shape of dataset\n",
    "        :param conf: the configuration of dataset\n",
    "        \"\"\"\n",
    "        self.sess = sess\n",
    "        self.grad = grad\n",
    "        self.x = x\n",
    "        self.n_values = n_values\n",
    "        self.input_shape = input_shape\n",
    "        self.sens = sens_params\n",
    "        self.conf = conf\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Local perturbation\n",
    "        :param x: input instance for local perturbation\n",
    "        :return: new potential individual discriminatory instance\n",
    "        \"\"\"\n",
    "\n",
    "        # perturbation\n",
    "        s = np.random.choice([1.0, -1.0]) * perturbation_size\n",
    "\n",
    "        n_x = x.copy()\n",
    "        for i in range(len(self.sens)):\n",
    "            n_x[self.sens[i] - 1] = self.n_values[i]\n",
    "       \n",
    "        # compute the gradients of an individual discriminatory instance pairs\n",
    "        ind_grad = self.sess.run(self.grad, feed_dict={self.x:np.array([x])})\n",
    "        n_ind_grad = self.sess.run(self.grad, feed_dict={self.x:np.array([n_x])})\n",
    "\n",
    "        if np.zeros(self.input_shape).tolist() == ind_grad[0].tolist() and \\\n",
    "           np.zeros(self.input_shape).tolist() == n_ind_grad[0].tolist():\n",
    "            \n",
    "            probs = 1.0 / (self.input_shape) * np.ones(self.input_shape)\n",
    "\n",
    "            for sens in self.sens :\n",
    "                probs[sens - 1] = 0\n",
    "\n",
    "                \n",
    "\n",
    "        else:\n",
    "            # nomalize the reciprocal of gradients (prefer the low impactful feature)\n",
    "            grad_sum = 1.0 / (abs(ind_grad[0]) + abs(n_ind_grad[0]))\n",
    "\n",
    "            for sens in self.sens :\n",
    "                grad_sum[ sens - 1 ] = 0\n",
    "\n",
    "            probs = grad_sum / np.sum(grad_sum)\n",
    "        probs = probs / probs.sum()\n",
    "        if True in np.isnan(probs):\n",
    "            probs = 1.0 / (self.input_shape) * np.ones(self.input_shape)\n",
    "\n",
    "            for sens in self.sens :\n",
    "                probs[sens - 1] = 0\n",
    "            probs = probs/probs.sum()\n",
    "\n",
    "\n",
    "        # randomly choose the feature for local perturbation\n",
    "        index = np.random.choice(range(self.input_shape) , p=probs)\n",
    "        local_cal_grad = np.zeros(self.input_shape)\n",
    "        local_cal_grad[index] = 1.0\n",
    "        x = clip(x + s * local_cal_grad, self.conf).astype(\"int\")\n",
    "        return x\n",
    "                \n",
    "#--------------------------------------\n",
    "def m_instance( sample, sens_params, conf):\n",
    "    index = []\n",
    "    m_sample = []\n",
    "    for sens in sens_params:\n",
    "        index.append([i for i in range(conf.input_bounds[sens - 1][0], conf.input_bounds[sens - 1][1] + 1)])\n",
    "      \n",
    "    for ind in list(product(*index)):     \n",
    "        temp = sample.copy()\n",
    "        for i in range(len(sens_params)):\n",
    "            temp[0][sens_params[i] - 1] = ind[i]\n",
    "        m_sample.append(temp)\n",
    "    return np.array(m_sample)\n",
    "\n",
    "def global_sample_select(clus_dic, sens_params):\n",
    "    leng = 0\n",
    "    for key in clus_dic.keys():\n",
    "        if key == 'Seed':\n",
    "            continue\n",
    "        if len(clus_dic[key]) > leng:\n",
    "            leng = len(clus_dic[key])\n",
    "            largest = key\n",
    "    \n",
    "    sample_ind = np.random.randint(len(clus_dic[largest]))\n",
    "    n_sample_ind = np.random.randint(len(clus_dic[largest]))\n",
    "    \n",
    "    sample = clus_dic['Seed']\n",
    "    for i in range(len(sens_params)):\n",
    "        sample[sens_params[i] -1] = clus_dic[largest][sample_ind][i]\n",
    "    # returns one sample of largest partition and its pair\n",
    "    return np.array([sample]),clus_dic[largest][n_sample_ind]\n",
    "\n",
    "\n",
    "def local_sample_select(clus_dic, sens_params):\n",
    "      \n",
    "    k_1 = min(list(clus_dic.keys())[1:])\n",
    "    k_2 = max(list(clus_dic.keys())[1:])\n",
    "    \n",
    "    sample_ind = np.random.randint(len(clus_dic[k_1]))\n",
    "    n_sample_ind = np.random.randint(len(clus_dic[k_2]))\n",
    "\n",
    "    sample = clus_dic['Seed']\n",
    "    for i in range(len(sens_params)):\n",
    "        sample[sens_params[i] -1] = clus_dic[k_1][sample_ind][i]\n",
    "    return np.array([sample]),clus_dic[k_2][n_sample_ind]\n",
    "    \n",
    "\n",
    "def clustering(probs,m_sample, sens_params, epsillon):\n",
    "    cluster_dic = {}\n",
    "    cluster_dic['Seed'] = m_sample[0][0]\n",
    "    bins= np.arange(0, 1, epsillon )\n",
    "    digitized = np.digitize(probs, bins) - 1\n",
    "    for  k in range(len(digitized)):\n",
    "\n",
    "        if digitized[k] not in cluster_dic.keys():        \n",
    "            cluster_dic[digitized[k]]=[ [m_sample[k][0][j - 1] for j in sens_params]]\n",
    "        else:\n",
    "            cluster_dic[digitized[k]].append( [m_sample[k][0][j - 1] for j in sens_params])\n",
    "    return cluster_dic \n",
    "    \n",
    "def pred_prob(sess, x, preds, m_sample, input_shape):\n",
    "        probs = model_prediction(sess, x, preds, np.array(m_sample).reshape(len(m_sample),\n",
    "                                                                input_shape[1]))[:,1:2].reshape(len(m_sample))\n",
    "        return probs\n",
    "\n",
    "def sh_entropy(probs,bin_thresh, base=2 ):\n",
    "    bins = np.arange(0, 1,bin_thresh )\n",
    "    digitized = np.digitize(probs, bins)\n",
    "    value,counts = np.unique(digitized, return_counts=True)   \n",
    "    return entropy(counts, base=base)\n",
    "     \n",
    "    \n",
    "def dnn_fair_testing(dataset, sens_params, model_path, cluster_num, \n",
    "                     max_global, max_local, max_iter, epsillon, timeout):\n",
    "    \"\"\"\n",
    "    \n",
    "    The implementation of ADF\n",
    "    :param dataset: the name of testing dataset\n",
    "    :param sensitive_param: the index of sensitive feature\n",
    "    :param model_path: the path of testing model\n",
    "    :param cluster_num: the number of clusters to form as well as the number of\n",
    "            centroids to generate\n",
    "    :param max_global: the maximum number of samples for global search\n",
    "    :param max_local: the maximum number of samples for local search\n",
    "    :param max_iter: the maximum iteration of global perturbation\n",
    "    \"\"\"\n",
    "    data = {\"census\":census_data, \"credit\":credit_data, \"bank\":bank_data, \"compas\":compas_data, \n",
    "            \"default\": default_data, \"heart\":heart_data, \"diabetes\":diabetes_data, \n",
    "            \"students\":students_data, \"meps15\":meps15_data, \"meps16\":meps16_data}\n",
    "    data_config = {\"census\":census, \"credit\":credit, \"bank\":bank, \"compas\":compas, \"default\":default,\n",
    "                  \"heart\":heart , \"diabetes\":diabetes,\"students\":students, \"meps15\":meps15, \"meps16\":meps16}\n",
    "    # prepare the testing data and model\n",
    "    X, Y, input_shape, nb_classes = data[dataset]()\n",
    "    tf.set_random_seed(1234)\n",
    "\n",
    "    config = tf.ConfigProto(device_count = {'GPU': 0})\n",
    "    config.allow_soft_placement= True\n",
    "\n",
    "    sess = tf.Session(config=config)\n",
    "    x = tf.placeholder(tf.float32, shape=input_shape)\n",
    "    y = tf.placeholder(tf.float32, shape=(None, nb_classes))\n",
    "    model = dnn(input_shape, nb_classes)   \n",
    "\n",
    "    preds = model(x)\n",
    "    saver = tf.train.Saver()\n",
    "    model_path = model_path + dataset + \"/test.model\"\n",
    "    saver.restore(sess, model_path)\n",
    "\n",
    "    # construct the gradient graph\n",
    "    grad_0 = gradient_graph(x, preds)\n",
    "\n",
    "    # build the clustering model\n",
    "    clf = cluster(dataset, cluster_num)\n",
    "    clusters = [np.where(clf.labels_ == i) for i in range(cluster_num)]\n",
    "\n",
    "    # store the result of fairness testing\n",
    "   \n",
    "    global max_k\n",
    "    global start_time\n",
    "    global max_k_time\n",
    "\n",
    "    print(dataset, sens_params)\n",
    "    RQ2_table = []\n",
    "    RQ1_table = []\n",
    "    for trial in range(2):\n",
    "        print('Trial', trial)\n",
    "        if  sess._closed:\n",
    "            sess = tf.Session(config=config)\n",
    "            sess = tf.Session(config=config)\n",
    "            x = tf.placeholder(tf.float32, shape=input_shape)\n",
    "            y = tf.placeholder(tf.float32, shape=(None, nb_classes))\n",
    "            model = dnn(input_shape, nb_classes)   \n",
    "            preds = model(x)\n",
    "            saver = tf.train.Saver()\n",
    "            saver.restore(sess, model_path)\n",
    "            grad_0 = gradient_graph(x, preds)\n",
    "        global_inputs = set()\n",
    "        tot_inputs =set()\n",
    "        global_inputs_list = []\n",
    "        local_inputs = set()\n",
    "        local_inputs_list = []\n",
    "        seed_num = 0          \n",
    "        max_k_time = 0\n",
    "        max_k_time_list = []\n",
    "        init_k_list  = []\n",
    "        max_k_list = []\n",
    "        #-----------------------\n",
    "        def evaluate_local(inp):\n",
    "\n",
    "            \"\"\"\n",
    "            Evaluate whether the test input after local perturbation is an individual discriminatory instance\n",
    "            :param inp: test input\n",
    "            :return: whether it is an individual discriminatory instance\n",
    "            \"\"\" \n",
    "            global max_k\n",
    "            global max_k_time\n",
    "            global start_time\n",
    "            global time1\n",
    "            result, K ,conf = check_for_error_condition(data_config[dataset], sess, x, preds, inp, \n",
    "                                               sens_params, input_shape, epsillon)    \n",
    "            if K > max_k:\n",
    "                max_k = K \n",
    "                max_k_time = time.time() - start_time\n",
    "\n",
    "            dis_sample =copy.deepcopy(inp.astype('int').tolist())   \n",
    "            for sens in sens_params:\n",
    "                dis_sample[sens - 1] = 0    \n",
    "            if tuple(dis_sample) not in global_inputs and\\\n",
    "                                    tuple(dis_sample) not in local_inputs:\n",
    "\n",
    "                local_inputs.add(tuple(dis_sample))\n",
    "                local_inputs_list.append(dis_sample + [time.time() - time1])\n",
    "\n",
    "            return (-1 * result)\n",
    "\n",
    "        # select the seed input for fairness testing\n",
    "        inputs = seed_test_input(clusters, min(max_global, len(X)))\n",
    "        global time1\n",
    "        time1 = time.time()  \n",
    "        for num in range(len(inputs)):\n",
    "            \n",
    "            #clear_output(wait=True)\n",
    "            start_time = time.time()\n",
    "            if time.time()-time1 > timeout:\n",
    "                break \n",
    "            print('Input ',seed_num)\n",
    "            index = inputs[num]\n",
    "            sample = X[ index : index + 1]\n",
    "\n",
    "            # start global perturbation\n",
    "            for iter in range( max_iter + 1 ):            \n",
    "                if time.time()-time1 > timeout :\n",
    "                    break\n",
    "                m_sample = m_instance( np.array(sample) , sens_params, data_config[dataset] )\n",
    "                pred = pred_prob( sess, x, preds, m_sample , input_shape )\n",
    "                clus_dic = clustering( pred, m_sample, sens_params, epsillon )\n",
    "                #entropy_min = round(np.log2(len(clus_dic)-1 ),2)#sh_entropy(pred, ent_tresh)\n",
    "                #entropy_sh = sh_entropy(pred, epsillon)\n",
    "                if iter == 0:\n",
    "                    init_k = len(clus_dic) - 1\n",
    "                    max_k = init_k\n",
    "                    #max_k_time = round((time.time() - start_time),4)\n",
    "\n",
    "                if len(clus_dic) - 1 > max_k:\n",
    "                    max_k = len(clus_dic) - 1\n",
    "                    max_k_time = round((time.time() - start_time),4)\n",
    "\n",
    "                sample,n_values = global_sample_select( clus_dic, sens_params )\n",
    "                dis_sample = sample.copy()\n",
    "                for sens in sens_params:\n",
    "                    dis_sample[0][sens  - 1] = 0\n",
    "\n",
    "\n",
    "                if tuple(dis_sample[0].astype('int')) not in global_inputs and\\\n",
    "                                    tuple(dis_sample[0].astype('int')) not in local_inputs:\n",
    "                    dis_flag = True\n",
    "                    global_inputs.add(tuple(dis_sample[0].astype('int')))\n",
    "                    global_inputs_list.append(dis_sample[0].astype('int').tolist())\n",
    "\n",
    "                else:\n",
    "                    dis_flag = False\n",
    "\n",
    "\n",
    "                if dis_flag and (len(clus_dic)-1 >= 2):    \n",
    "\n",
    "                    loc_x,n_values = local_sample_select(clus_dic ,sens_params )                              \n",
    "                    minimizer = {\"method\": \"L-BFGS-B\"}\n",
    "                    local_perturbation = Local_Perturbation(sess, grad_0, x, n_values, \n",
    "                                                            sens_params, input_shape[1], \n",
    "                                                            data_config[dataset])               \n",
    "                    basinhopping(evaluate_local, loc_x, stepsize = 1.0, \n",
    "                                 take_step = local_perturbation, minimizer_kwargs = minimizer, \n",
    "                                 niter = max_local)\n",
    "\n",
    "\n",
    "                if dis_flag :\n",
    "                    global_inputs_list[-1] +=  [time.time() - time1]\n",
    "\n",
    "\n",
    "                clus_dic = {}\n",
    "                if iter == max_iter:\n",
    "                    break\n",
    "\n",
    "                #Making up n_sample\n",
    "                n_sample = sample.copy()\n",
    "                for i in range(len(sens_params)):\n",
    "                    n_sample[0][sens_params[i] - 1] = n_values[i]                \n",
    "\n",
    "                # global perturbation\n",
    "\n",
    "                s_grad = sess.run(tf.sign(grad_0), feed_dict = {x: sample})\n",
    "                n_grad = sess.run(tf.sign(grad_0), feed_dict = {x: n_sample})\n",
    "\n",
    "                # find the feature with same impact\n",
    "                if np.zeros(data_config[dataset].params).tolist() == s_grad[0].tolist():\n",
    "                    g_diff = n_grad[0]\n",
    "                elif np.zeros(data_config[dataset].params).tolist() == n_grad[0].tolist():\n",
    "                    g_diff = s_grad[0]\n",
    "                else:\n",
    "                    g_diff = np.array(s_grad[0] == n_grad[0], dtype = float)                \n",
    "                for sens in sens_params:\n",
    "                    g_diff[sens - 1] = 0 \n",
    "#                         g_diff1[sens - 1] = 0\n",
    "\n",
    "\n",
    "                cal_grad = s_grad * g_diff\n",
    "#                     cal_grad1 = np.array([grad_m_sign[np.random.randint(len(grad_m_sign))] * g_diff1])\n",
    "#                     if np.zeros(input_shape[1]).tolist() == cal_grad1.tolist()[0]:\n",
    "#                         index = np.random.randint(len(cal_grad1[0]) - 1)\n",
    "#                         for i in range(len(sens_params) - 1, -1, -1):\n",
    "#                             if index == sens_params[i] - 1 :\n",
    "#                                 index = index + 1\n",
    "#                         cal_grad1[0][index]  = np.random.choice([1.0, -1.0])\n",
    "\n",
    "                if np.zeros(input_shape[1]).tolist() == cal_grad.tolist()[0]:\n",
    "                    index = np.random.randint(len(cal_grad[0]) - 1)\n",
    "                    for i in range(len(sens_params) - 1, -1, -1):\n",
    "                        if index == sens_params[i] - 1 :\n",
    "                            index = index + 1\n",
    "\n",
    "                    cal_grad[0][index]  = np.random.choice([1.0, -1.0])\n",
    "\n",
    "                sample[0] = clip(sample[0] + perturbation_size * cal_grad[0], data_config[dataset]).astype(\"int\")\n",
    "\n",
    "            seed_num += 1\n",
    "            if max_k > 1:\n",
    "                max_k_time_list.append(max_k_time)\n",
    "                init_k_list.append(init_k)\n",
    "                max_k_list.append(max_k)\n",
    "            \n",
    "        print('Search Time =', time.time()-time1)\n",
    "\n",
    "        # create the folder for storing the fairness testing result\n",
    "        if not os.path.exists('../results/'):\n",
    "            os.makedirs('../results/')\n",
    "        if not os.path.exists('../results/' + dataset + '/'):\n",
    "            os.makedirs('../results/' + dataset + '/')\n",
    "        if not os.path.exists('../results/' + dataset + '/OurTool/'):\n",
    "            os.makedirs('../results/' + dataset + '/OurTool/')\n",
    "        if not os.path.exists('../results/' + dataset + '/OurTool/RQ1&2/'):\n",
    "            os.makedirs('../results/' + dataset + '/OurTool/RQ1&2/')\n",
    "        if not os.path.exists('../results/' + dataset + '/OurTool/RQ1&2/' + ''.join(str(i) for i in sens_params)+'_10runs/'):\n",
    "            os.makedirs('../results/' + dataset + '/OurTool/RQ1&2/' + ''.join(str(i) for i in sens_params)+'_10runs/')\n",
    "        # storing the fairness testing result\n",
    "        np.save('../results/' + dataset + '/OurTool/RQ1&2/'+ ''.join(str(i) for i in sens_params)+'_10runs'+'/global_inputs_'+str(trial)+'.npy', \n",
    "                np.array(global_inputs_list))\n",
    "        np.save('../results/' + dataset + '/OurTool/RQ1&2/'+ ''.join(str(i) for i in sens_params)+'_10runs'+'/local_inputs_'+str(trial)+'.npy', \n",
    "                np.array(local_inputs_list))\n",
    "        total_inputs = np.concatenate((local_inputs_list,global_inputs_list), axis=0)\n",
    "        np.save('../results/' + dataset + '/OurTool/RQ1&2/'+ ''.join(str(i) for i in sens_params)+'_10runs'+'/total_inputs_'+str(trial)+'.npy',\n",
    "                total_inputs)\n",
    "        # RQ1 & RQ2\n",
    "\n",
    "        local_sam = np.array(local_inputs_list).astype('int32')\n",
    "        global_sam = np.array(global_inputs_list).astype('int32')\n",
    "        # Storing result for RQ1 table\n",
    "        print('Analyzing the search results....')\n",
    "\n",
    "        with open('../results/' + dataset + '/OurTool/RQ1&2/'+ ''.join(str(i) for i in sens_params)+'_10runs'+'/global_inputs_msamples_'+str(trial)+'.csv', 'w') as f:\n",
    "            writer = csv.writer(f)\n",
    "            for ind in range(len(global_inputs_list)):\n",
    "                m_sample = m_instance( np.array([global_inputs_list[ind][:input_shape[1]]]) , sens_params, data_config[dataset] ) \n",
    "                rows = m_sample.reshape((len(m_sample),input_shape[1]))\n",
    "                writer.writerows(np.append(rows,[[global_inputs_list[ind][-1]] for i in range(len(m_sample))],axis=1))\n",
    "\n",
    "        with open('../results/' + dataset + '/OurTool/RQ1&2/'+ ''.join(str(i) for i in sens_params)+'_10runs'+'/local_inputs_msamples_'+str(trial)+'.csv', 'w') as f:\n",
    "            writer = csv.writer(f)\n",
    "            for ind in range(len(local_inputs_list)):\n",
    "                m_sample = m_instance( np.array([local_inputs_list[ind][:input_shape[1]]]) , sens_params, data_config[dataset] ) \n",
    "                rows = m_sample.reshape((len(m_sample),input_shape[1]))\n",
    "                writer.writerows(np.append(rows,[[local_inputs_list[ind][-1]] for i in range(len(m_sample))],axis=1))\n",
    "        \n",
    "        df_l = pd.read_csv('../results/' + dataset + '/OurTool/RQ1&2/'+ ''.join(str(i) for i in sens_params)+'_10runs'+'/local_inputs_msamples_'+str(trial)+'.csv',header=None)  \n",
    "        df_g = pd.read_csv('../results/' + dataset + '/OurTool/RQ1&2/'+ ''.join(str(i) for i in sens_params)+'_10runs'+'/global_inputs_msamples_'+str(trial)+'.csv',header=None)\n",
    "\n",
    "        df_g['label'] = model_argmax(sess, x, preds, df_g.to_numpy()[:,:input_shape[1]])\n",
    "        df_l['label'] = model_argmax(sess, x, preds, df_l.to_numpy()[:,:input_shape[1]])\n",
    "        g_pivot = pd.pivot_table(df_g, values=\"label\", index=list(np.setxor1d(df_g.columns[:-1] ,\n",
    "                                                                              np.array(sens_params)-1)), aggfunc=np.sum)\n",
    "        l_pivot = pd.pivot_table(df_l, values=\"label\", index=list(np.setxor1d(df_l.columns[:-1] ,\n",
    "                                                                           np.array(sens_params)-1)), aggfunc=np.sum)\n",
    "\n",
    "        g_time = g_pivot.index[np.where((g_pivot['label'] > 0) & (g_pivot['label'] < len(m_sample)))[0]].get_level_values(input_shape[1]).values\n",
    "        l_time = l_pivot.index[np.where((l_pivot['label'] > 0) & (l_pivot['label'] < len(m_sample)))[0]].get_level_values(input_shape[1]).values\n",
    "        tot_time = np.sort(np.concatenate((l_time, g_time), axis=0 ))\n",
    "        print('Time to 1st ID',tot_time[0])   \n",
    "        print('time to 1000 ID',tot_time[999])\n",
    "        \n",
    "        g_dis = (len(m_sample) - g_pivot.loc[(g_pivot['label'] > 0) & \\\n",
    "                                             (g_pivot['label'] < len(m_sample))]['label'].to_numpy()).sum()\n",
    "        l_dis = (len(m_sample) - l_pivot.loc[(l_pivot['label'] > 0) & \\\n",
    "                                             (l_pivot['label'] < len(m_sample))]['label'].to_numpy()).sum()\n",
    "\n",
    "        g_dis_adf = len(g_time)\n",
    "        l_dis_adf = len(l_time)\n",
    "\n",
    "\n",
    "        global_succ_adf = round((g_dis_adf / len(global_sam)) * 100, 1)\n",
    "        local_succ_adf  = round((l_dis_adf / len(local_sam)) * 100, 1)\n",
    "        global tot_df\n",
    "        tot_df = pd.DataFrame(total_inputs)\n",
    "        tot_df.columns=[i for i in range(input_shape[1])] + ['time']\n",
    "        \n",
    "        #tot_df = tot_df[[i for i in range(input_shape[1])] + [tot_df.columns[-2]]]\n",
    "        #columns = list(tot_df.columns)\n",
    "        #columns[-1] = 'sh_entropy'\n",
    "        #tot_df.columns = columns\n",
    "        k = []\n",
    "        disc = []\n",
    "        tot_df['sh_entropy'] = 0\n",
    "        for sam_ind in range(total_inputs.shape[0]): \n",
    "            \n",
    "            m_sample = m_instance( np.array([total_inputs[:,:input_shape[1]][sam_ind]]) , sens_params, data_config[dataset] )\n",
    "            pred = pred_prob( sess, x, preds, m_sample , input_shape )\n",
    "            clus_dic = clustering( pred, m_sample, sens_params, epsillon )\n",
    "            tot_df.loc[[sam_ind], 'sh_entropy'] = sh_entropy(pred, epsillon)\n",
    "            if pred.max() > 0.5 and  pred.min()< 0.5:\n",
    "                disc.append(1)\n",
    "            else:\n",
    "                disc.append(0)\n",
    "            k.append(len(clus_dic) - 1)\n",
    "            \n",
    "        tot_df['k'] = k\n",
    "        time_K_greater_than_1 = tot_df['time'][np.where(tot_df['k']>1)[0][0]]\n",
    "        tot_df['disc'] = disc\n",
    "        tot_df['min_entropy'] = round(np.log2(tot_df['k'] ),2)\n",
    "        tot_dis = tot_df.loc[tot_df['disc'] == 1]\n",
    "#         min_entropy.append( np.mean(tot_dis['min_entropy']))\n",
    "#         k_mean.append(np.mean(tot_dis['k']))\n",
    "#         sh_entropy_mean.append(np.mean(tot_dis['sh_entropy']))\n",
    "        tot_dis.to_csv('../results/' + dataset + '/OurTool/RQ1&2/'+ ''.join(str(i) for i in sens_params)+'_10runs'+'/total_disc_' + str(trial)+'.csv')\n",
    "        # reseting the TF graph for the next round\n",
    "        sess.close()\n",
    "        tf.reset_default_graph()\n",
    "        haighest_k = np.sort(tot_df['k'].unique())[-3:]\n",
    "        if len(haighest_k)>2:\n",
    "            IK1F = np.where(tot_df['k']==haighest_k[2])[0].shape[0]\n",
    "            IK2F = np.where(tot_df['k']==haighest_k[1])[0].shape[0]\n",
    "            IK3F = np.where(tot_df['k']==haighest_k[0])[0].shape[0]\n",
    "\n",
    "        else:\n",
    "            IK1F = np.where(tot_df['k']==haighest_k[1])[0].shape[0]\n",
    "            IK2F = np.where(tot_df['k']==haighest_k[0])[0].shape[0]\n",
    "            IK3F = 0\n",
    "\n",
    "            \n",
    "        print('Global ID RQ1 =', g_dis)\n",
    "        print('local  ID RQ1  =',  l_dis)\n",
    "        print('Total loc samples  = ', len(local_sam)) \n",
    "        print('Total glob samples = ', len(global_sam)) \n",
    "        print('Total ID = ',g_dis + l_dis)\n",
    "        print('Total ID RQ2  = ',g_dis_adf + l_dis_adf)\n",
    "        print('Global ID RQ2  = ',g_dis_adf)\n",
    "        print('Local  ID RQ2  = ',l_dis_adf)\n",
    "\n",
    "        global_succ = round( g_dis / (len(global_sam) * \\\n",
    "                             len(m_sample)) * 100,1)\n",
    "        local_succ = round(l_dis  / (len(local_sam) * \\\n",
    "                             len(m_sample)) * 100,1)\n",
    "\n",
    "\n",
    "#         df_RQ1 = pd.DataFrame(total_inputs)\n",
    "#         df_RQ1.columns = [i for i in range(input_shape[1])] + ['init_k', 'max_k', 'max_k_time', 'sample_time']\n",
    "        #row = [len(total_inputs)] + list(df_RQ1[['init_k', 'max_k', 'max_k_time',]].mean()+[time_K_greater_than_1])\n",
    "        row = [len(total_inputs)] + [np.mean(init_k_list), np.mean(max_k_list),np.mean(max_k_time_list)] + list(tot_dis[['min_entropy', 'sh_entropy']].mean()) + [time_K_greater_than_1] + [IK1F, IK2F,IK3F] \n",
    "        RQ1_table.append(row)\n",
    "        RQ2_table.append([g_dis_adf, l_dis_adf, g_dis_adf + l_dis_adf ,local_succ_adf, tot_time[0], tot_time[999] ])\n",
    "        print('Local search success rate  = ', local_succ, '%')\n",
    "        print('Global search success rate = ', global_succ, '%')\n",
    "        print('Local search ADF success rate  = ', local_succ_adf, '%')\n",
    "        print('Global search ADF success rate = ', global_succ_adf, '%')\n",
    "        \n",
    "    np.save('../results/' + dataset + '/OurTool/RQ1&2/'+ ''.join(str(i) for i in sens_params)+'_10runs'+'/QID_RQ2_10runs.npy',\n",
    "            RQ2_table)\n",
    "    np.save('../results/' + dataset + '/OurTool/RQ1&2/'+ ''.join(str(i) for i in sens_params)+'_10runs'+'/QID_RQ1_10runs.npy',\n",
    "            RQ1_table)\n",
    "    with open('../results/' + dataset + '/OurTool/RQ1&2/'+ ''.join(str(i) for i in sens_params)+'_10runs'+'/RQ2_table.csv', 'w') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(['g_adf_disc', 'l_adf_disc','tot_adf_disc','local_succ_adf',\n",
    "                             'time_to_first','time_to_1000'])\n",
    "            writer.writerow(np.mean(RQ2_table,axis=0))\n",
    "            writer.writerow(np.std(RQ2_table,axis=0))\n",
    "\n",
    "    with open('../results/' + dataset + '/OurTool/RQ1&2/'+ ''.join(str(i) for i in sens_params)+'_10runs'+'/RQ1_table.csv', 'w') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(['tot_samples','init_k', 'max_k', 'max_k_time','min_entropy',\n",
    "                             'sh_entropy','time_K_greater_than_1', 'IK1F', 'IK2F', 'IK3F'])\n",
    "            writer.writerow(np.mean(RQ1_table,axis=0))\n",
    "            writer.writerow(np.std(RQ1_table,axis=0))\n",
    "\n",
    "#     with open('../results/' + dataset + '/OurTool/RQ3/entropy.csv','w') as f:\n",
    "#         writer = csv.writer(f)\n",
    "#         writer.writerow(['k_mean','min_entropy','sh_entropy'])\n",
    "#         writer.writerow([np.mean(k_mean),np.mean(min_entropy), np.mean(sh_entropy_mean) ])\n",
    "def main(argv=None):\n",
    "    for data_set in [\"credit\"]:#, \"bank\",  \"default\", \"diabetes\"]:#\"heart\" ,\"students\", \"meps15\", \"meps16\"]:\n",
    "        if data_set == 'credit': sens_p = [9]\n",
    "        elif data_set == 'census': sens_p = [9,8,1]\n",
    "        elif data_set == 'bank': sens_p = [1]\n",
    "        elif data_set == 'compas': sens_p = [3,2,1]\n",
    "        elif data_set == 'default': sens_p = [5,2]\n",
    "        elif data_set == 'heart': sens_p = [2,1]\n",
    "        elif data_set == 'diabetes': sens_p = [8]\n",
    "        elif data_set == 'students': sens_p = [3,2]\n",
    "        elif data_set == 'meps15': sens_p = [10,2,1]\n",
    "        elif data_set == 'meps16': sens_p = [10,2,1]\n",
    "    \n",
    "        for sens in sens_p:\n",
    "\n",
    "            dnn_fair_testing(dataset = data_set, \n",
    "                             sens_params = [sens],\n",
    "                             model_path  = FLAGS.model_path,\n",
    "                             cluster_num = FLAGS.cluster_num,\n",
    "                             max_global  = FLAGS.max_global,\n",
    "                             max_local   = FLAGS.max_local,\n",
    "                             max_iter    = FLAGS.max_iter,\n",
    "                             epsillon    = FLAGS.epsillon,\n",
    "                             timeout    = FLAGS.timeout\n",
    "                            )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    flags.DEFINE_string(\"dataset\", \"census\", \"the name of dataset\")\n",
    "    flags.DEFINE_string('model_path', '../models/', 'the path for testing model')\n",
    "    flags.DEFINE_integer('cluster_num', 4, 'the number of clusters to form as well as the number of centroids to generate')\n",
    "    flags.DEFINE_integer('max_global', 1000, 'maximum number of samples for global search')\n",
    "    flags.DEFINE_integer('max_local', 1000, 'maximum number of samples for local search')\n",
    "    flags.DEFINE_integer('max_iter', 10, 'maximum iteration of global perturbation')\n",
    "    flags.DEFINE_integer('timeout', 90, 'search timeout')\n",
    "    #if result for RQ1 table: set this to [9,8,1], if result for RQ2 table: set this one sensitive attribute each time \n",
    "    # e.g. for census dataset, set [9], [8], [1] for sex, race and age respectively\n",
    "    flags.DEFINE_list('sens_params', [9,8,1], 'sensitive parameters index.1 for age, 9 for gender, 8 for race')\n",
    "    flags.DEFINE_float('epsillon', 0.025, 'the value of epsillon for partitioning')\n",
    "    tf.app.run()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "haighest_k = np.sort(tot_df['k'].unique())[-3:]\n",
    "if len(haighest_k)>2:\n",
    "    IK1F = np.where(tot_df['k']==haighest_k[2])[0].shape[0]\n",
    "    IK2F = np.where(tot_df['k']==haighest_k[1])[0].shape[0]\n",
    "    IK3F = np.where(tot_df['k']==haighest_k[0])[0].shape[0]\n",
    "\n",
    "else:\n",
    "    IK1F = np.where(tot_df['k']==haighest_k[1])[0].shape[0]\n",
    "    IK2F = np.where(tot_df['k']==haighest_k[0])[0].shape[0]\n",
    "    IK3F = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4757, 397, 0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IK1F,IK2F,IK3F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
