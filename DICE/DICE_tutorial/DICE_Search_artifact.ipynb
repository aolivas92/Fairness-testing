{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/vmonjezi/.local/lib/python3.8/site-packages/llvmlite/llvmpy/__init__.py:3: UserWarning: The module `llvmlite.llvmpy` is deprecated and will be removed in the future.\n",
      "  warnings.warn(\n",
      "/home/users/vmonjezi/.local/lib/python3.8/site-packages/llvmlite/llvmpy/core.py:8: UserWarning: The module `llvmlite.llvmpy.core` is deprecated and will be removed in the future. Equivalent functionality is provided by `llvmlite.ir`.\n",
      "  warnings.warn(\n",
      "/home/users/vmonjezi/.local/lib/python3.8/site-packages/llvmlite/llvmpy/passes.py:17: UserWarning: The module `llvmlite.llvmpy.passes` is deprecated and will be removed in the future. If you are using this code, it should be inlined into your own project.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../models/credit/test.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0120 09:03:28.113019 140211132315456 saver.py:1399] Restoring parameters from ../models/credit/test.model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/lib/python3/dist-packages/tensorflow/python/util/dispatch.py:1096: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0120 09:03:28.154504 140211132315456 deprecation.py:341] From /usr/lib/python3/dist-packages/tensorflow/python/util/dispatch.py:1096: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "credit [9]\n",
      "Trial 0\n",
      "Input  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trying to unpickle estimator KMeans from version 0.22.2.post1 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input  1\n",
      "Input  2\n",
      "Input  3\n",
      "Input  4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-7e16f1d82fa0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFINE_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sens_params'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sensitive parameters index.1 for age, 9 for gender, 8 for race'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m     \u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFINE_float\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epsillon'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.025\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'the value of epsillon for partitioning'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow/python/platform/app.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, argv)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0mmain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'__main__'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags_parser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_parse_flags_tolerate_undef\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3/dist-packages/absl/app.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, argv, flags_parser)\u001b[0m\n\u001b[1;32m    310\u001b[0m       \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m       \u001b[0m_run_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mUsageError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m       \u001b[0musage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshorthelp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetailed_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexitcode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexitcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/absl/app.py\u001b[0m in \u001b[0;36m_run_main\u001b[0;34m(main, argv)\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-7e16f1d82fa0>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(argv)\u001b[0m\n\u001b[1;32m    646\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msens\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msens_p\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m             dnn_fair_testing(dataset = data_set, \n\u001b[0m\u001b[1;32m    649\u001b[0m                              \u001b[0msens_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m                              \u001b[0mmodel_path\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-7e16f1d82fa0>\u001b[0m in \u001b[0;36mdnn_fair_testing\u001b[0;34m(dataset, sens_params, model_path, cluster_num, max_global, max_local, max_iter, epsillon, timeout)\u001b[0m\n\u001b[1;32m    407\u001b[0m                                                             \u001b[0msens_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m                                                             data_config[dataset])               \n\u001b[0;32m--> 409\u001b[0;31m                     basinhopping(evaluate_local, loc_x, stepsize = 1.0, \n\u001b[0m\u001b[1;32m    410\u001b[0m                                  \u001b[0mtake_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocal_perturbation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminimizer_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m                                  niter = max_local)\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/scipy/optimize/_basinhopping.py\u001b[0m in \u001b[0;36mbasinhopping\u001b[0;34m(func, x0, niter, T, stepsize, minimizer_kwargs, take_step, accept_test, callback, interval, disp, niter_success, seed)\u001b[0m\n\u001b[1;32m    674\u001b[0m                \" successfully\"]\n\u001b[1;32m    675\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mniter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0mnew_global_min\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_cycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/scipy/optimize/_basinhopping.py\u001b[0m in \u001b[0;36mone_cycle\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mnew_global_min\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         \u001b[0maccept\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_monte_carlo_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maccept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/scipy/optimize/_basinhopping.py\u001b[0m in \u001b[0;36m_monte_carlo_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;31m# do a local minimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0mminres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_after_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0mx_after_quench\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0menergy_after_quench\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/scipy/optimize/_basinhopping.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x0)\u001b[0m\n\u001b[1;32m    284\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/scipy/optimize/_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    607\u001b[0m                                   **options)\n\u001b[1;32m    608\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'l-bfgs-b'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 609\u001b[0;31m         return _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[0m\u001b[1;32m    610\u001b[0m                                 callback=callback, **options)\n\u001b[1;32m    611\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tnc'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, **unknown_options)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0;31m# until the completion of the current minimization iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;31m# Overwrite f and g:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb'NEW_X'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             \u001b[0;31m# new iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36mfunc_and_grad\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m             \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_approx_fprime_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36m_approx_fprime_helper\u001b[0;34m(xk, f, epsilon, args, f0)\u001b[0m\n\u001b[1;32m    695\u001b[0m         \u001b[0mei\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m         \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mei\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 697\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxk\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mf0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    698\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36mfunction_wrapper\u001b[0;34m(*wrapper_args)\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mwrapper_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mncalls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper_args\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mncalls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-7e16f1d82fa0>\u001b[0m in \u001b[0;36mevaluate_local\u001b[0;34m(inp)\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0;32mglobal\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0;32mglobal\u001b[0m \u001b[0mtime1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m             result, K ,conf = check_for_error_condition(data_config[dataset], sess, x, preds, inp, \n\u001b[0m\u001b[1;32m    338\u001b[0m                                                sens_params, input_shape, epsillon)    \n\u001b[1;32m    339\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mK\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmax_k\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-7e16f1d82fa0>\u001b[0m in \u001b[0;36mcheck_for_error_condition\u001b[0;34m(conf, sess, x, preds, t, sens_params, input_shape, epsillon)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm_instance\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msens_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0mpartition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclustering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msens_params\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mepsillon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;31m#entropy_min = np.log2(len(partition)-1)#sh_entropy(pred, ent_tresh)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-7e16f1d82fa0>\u001b[0m in \u001b[0;36mpred_prob\u001b[0;34m(sess, x, preds, m_sample, input_shape)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpred_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         probs = model_prediction(sess, x, preds, np.array(m_sample).reshape(len(m_sample),\n\u001b[0m\u001b[1;32m    241\u001b[0m                                                                 input_shape[1]))[:,1:2].reshape(len(m_sample))\n\u001b[1;32m    242\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/Fairness-testing/DICE/DICE_tutorial/../DICE_utils/utils_tf.py\u001b[0m in \u001b[0;36mmodel_prediction\u001b[0;34m(sess, x, predictions, samples, feed, batch_size)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfeed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0mpros\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    968\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    969\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 970\u001b[0;31m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0m\u001b[1;32m    971\u001b[0m                          run_metadata_ptr)\n\u001b[1;32m    972\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1185\u001b[0m     \u001b[0;31m# TODO(yuanbyu, keveman): Revisit whether we should just treat feeding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m     \u001b[0;31m# of a handle from a different device as an error.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1187\u001b[0;31m     \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_with_movers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1188\u001b[0m     \u001b[0mfinal_fetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[0mfinal_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_update_with_movers\u001b[0;34m(self, feed_dict, feed_map)\u001b[0m\n\u001b[1;32m   1432\u001b[0m     \u001b[0mhandle_movers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1433\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfeed_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeed_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1434\u001b[0;31m       \u001b[0mmover\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_handle_mover\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1435\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mmover\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1436\u001b[0m         \u001b[0mhandle_movers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmover\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow/python/ops/session_ops.py\u001b[0m in \u001b[0;36m_get_handle_mover\u001b[0;34m(graph, feeder, handle)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_get_handle_mover\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m   \u001b[0;34m\"\"\"Return a move subgraph for this pair of feeder and handle.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m   \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_handle_feeder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow/python/ops/session_ops.py\u001b[0m in \u001b[0;36m_get_handle_feeder\u001b[0;34m(graph, feeder)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_get_handle_feeder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_feeders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeeder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "from itertools import product\n",
    "import itertools\n",
    "import tensorflow.compat.v1 as tf \n",
    "tf.compat.v1.disable_eager_execution()\n",
    "import sys, os\n",
    "sys.path.append(\"../\")\n",
    "import copy\n",
    "import pandas as pd\n",
    "from tensorflow.python.platform import flags\n",
    "from scipy.optimize import basinhopping\n",
    "from scipy.stats import entropy\n",
    "import time\n",
    "\n",
    "from DICE_data.census import census_data\n",
    "from DICE_data.credit import credit_data\n",
    "from DICE_data.compas import compas_data\n",
    "from DICE_data.default import default_data\n",
    "from DICE_data.bank import bank_data\n",
    "from DICE_data.heart import heart_data\n",
    "from DICE_data.diabetes import diabetes_data\n",
    "from DICE_data.students import students_data\n",
    "from DICE_data.meps15 import meps15_data\n",
    "from DICE_data.meps16 import meps16_data\n",
    "\n",
    "from DICE_model.tutorial_models import dnn\n",
    "from DICE_utils.utils_tf import model_prediction, model_argmax , layer_out\n",
    "from DICE_utils.config import census, credit, bank, compas, default, heart, diabetes, students , meps15, meps16\n",
    "from DICE_tutorial.utils import cluster, gradient_graph\n",
    "#from IPython.display import clear_output\n",
    "\n",
    "# import argparse\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\"--dataset\", help='The name of dataset: census, credit, bank, default, meps21 ')\n",
    "# parser.add_argument(\"--sensitive_index\", help='The index for sensitive feature')\n",
    "# parser.add_argument(\"--time_out\", help='Max. running time', default = 14400, required=False)\n",
    "\n",
    "# args = parser.parse_args()\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "# step size of perturbation\n",
    "perturbation_size = 1\n",
    "\n",
    "def check_for_error_condition(conf, sess, x, preds, t, sens_params, input_shape, epsillon):\n",
    "    \"\"\"\n",
    "    Check whether the test case is an individual discriminatory instance\n",
    "    :param conf: the configuration of dataset\n",
    "    :param sess: TF session\n",
    "    :param x: input placeholder\n",
    "    :param preds: the model's symbolic output\n",
    "    :param t: test case\n",
    "    :param sens: the index of sensitive feature\n",
    "    :return: whether it is an individual discriminatory instance\n",
    "    \"\"\"\n",
    "\n",
    "    t = [t.astype('int')]   \n",
    "    samples = m_instance( np.array(t), sens_params, conf ) \n",
    "    pred = pred_prob(sess, x, preds, samples , input_shape )\n",
    "    partition = clustering(pred,samples, sens_params , epsillon)\n",
    "    #entropy_min = np.log2(len(partition)-1)#sh_entropy(pred, ent_tresh)\n",
    "    #entropy_sh = sh_entropy(pred, epsillon)\n",
    "    \n",
    "\n",
    "    return  max(list(partition.keys())[1:]) - min(list(partition.keys())[1:]), \\\n",
    "                                            len(partition)-1, conf#(len(partition) -1),\n",
    "    \n",
    "def seed_test_input(clusters, limit):\n",
    "    \"\"\"\n",
    "    Select the seed inputs for fairness testing\n",
    "    :param clusters: the results of K-means clustering\n",
    "    :param limit: the size of seed inputs wanted\n",
    "    :return: a sequence of seed inputs\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    rows = []\n",
    "    max_size = max([len(c[0]) for c in clusters])\n",
    "    while i < max_size:\n",
    "        if len(rows) == limit:\n",
    "            break\n",
    "        for c in clusters:\n",
    "            if i >= len(c[0]):\n",
    "                continue\n",
    "            row = c[0][i]\n",
    "            rows.append(row)\n",
    "            if len(rows) == limit:\n",
    "                break\n",
    "        i += 1\n",
    "    return np.array(rows)\n",
    "\n",
    "def clip(input, conf):\n",
    "    \"\"\"\n",
    "    Clip the generating instance with each feature to make sure it is valid\n",
    "    :param input: generating instance\n",
    "    :param conf: the configuration of dataset\n",
    "    :return: a valid generating instance\n",
    "    \"\"\"\n",
    "    for i in range(len(input)):\n",
    "        input[i] = max(input[i], conf.input_bounds[i][0])\n",
    "        input[i] = min(input[i], conf.input_bounds[i][1])\n",
    "    return input\n",
    "\n",
    "class Local_Perturbation(object):\n",
    "    \"\"\"\n",
    "    The  implementation of local perturbation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sess, grad, x, n_values, sens_params, input_shape, conf):\n",
    "        \"\"\"\n",
    "        Initial function of local perturbation\n",
    "        :param sess: TF session\n",
    "        :param grad: the gradient graph\n",
    "        :param x: input placeholder\n",
    "        :param n_value: the discriminatory value of sensitive feature\n",
    "        :param sens_param: the index of sensitive feature\n",
    "        :param input_shape: the shape of dataset\n",
    "        :param conf: the configuration of dataset\n",
    "        \"\"\"\n",
    "        self.sess = sess\n",
    "        self.grad = grad\n",
    "        self.x = x\n",
    "        self.n_values = n_values\n",
    "        self.input_shape = input_shape\n",
    "        self.sens = sens_params\n",
    "        self.conf = conf\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Local perturbation\n",
    "        :param x: input instance for local perturbation\n",
    "        :return: new potential individual discriminatory instance\n",
    "        \"\"\"\n",
    "\n",
    "        # perturbation\n",
    "        s = np.random.choice([1.0, -1.0]) * perturbation_size\n",
    "\n",
    "        n_x = x.copy()\n",
    "        for i in range(len(self.sens)):\n",
    "            n_x[self.sens[i] - 1] = self.n_values[i]\n",
    "       \n",
    "        # compute the gradients of an individual discriminatory instance pairs\n",
    "        ind_grad = self.sess.run(self.grad, feed_dict={self.x:np.array([x])})\n",
    "        n_ind_grad = self.sess.run(self.grad, feed_dict={self.x:np.array([n_x])})\n",
    "\n",
    "        if np.zeros(self.input_shape).tolist() == ind_grad[0].tolist() and \\\n",
    "           np.zeros(self.input_shape).tolist() == n_ind_grad[0].tolist():\n",
    "            \n",
    "            probs = 1.0 / (self.input_shape) * np.ones(self.input_shape)\n",
    "\n",
    "            for sens in self.sens :\n",
    "                probs[sens - 1] = 0\n",
    "\n",
    "                \n",
    "\n",
    "        else:\n",
    "            # nomalize the reciprocal of gradients (prefer the low impactful feature)\n",
    "            grad_sum = 1.0 / (abs(ind_grad[0]) + abs(n_ind_grad[0]))\n",
    "\n",
    "            for sens in self.sens :\n",
    "                grad_sum[ sens - 1 ] = 0\n",
    "\n",
    "            probs = grad_sum / np.sum(grad_sum)\n",
    "        probs = probs / probs.sum()\n",
    "        if True in np.isnan(probs):\n",
    "            probs = 1.0 / (self.input_shape) * np.ones(self.input_shape)\n",
    "\n",
    "            for sens in self.sens :\n",
    "                probs[sens - 1] = 0\n",
    "            probs = probs/probs.sum()\n",
    "\n",
    "\n",
    "        # randomly choose the feature for local perturbation\n",
    "        index = np.random.choice(range(self.input_shape) , p=probs)\n",
    "        local_cal_grad = np.zeros(self.input_shape)\n",
    "        local_cal_grad[index] = 1.0\n",
    "        x = clip(x + s * local_cal_grad, self.conf).astype(\"int\")\n",
    "        return x\n",
    "                \n",
    "#--------------------------------------\n",
    "def m_instance( sample, sens_params, conf):\n",
    "    index = []\n",
    "    m_sample = []\n",
    "    for sens in sens_params:\n",
    "        index.append([i for i in range(conf.input_bounds[sens - 1][0], conf.input_bounds[sens - 1][1] + 1)])\n",
    "      \n",
    "    for ind in list(product(*index)):     \n",
    "        temp = sample.copy()\n",
    "        for i in range(len(sens_params)):\n",
    "            temp[0][sens_params[i] - 1] = ind[i]\n",
    "        m_sample.append(temp)\n",
    "    return np.array(m_sample)\n",
    "\n",
    "def global_sample_select(clus_dic, sens_params):\n",
    "    leng = 0\n",
    "    for key in clus_dic.keys():\n",
    "        if key == 'Seed':\n",
    "            continue\n",
    "        if len(clus_dic[key]) > leng:\n",
    "            leng = len(clus_dic[key])\n",
    "            largest = key\n",
    "    \n",
    "    sample_ind = np.random.randint(len(clus_dic[largest]))\n",
    "    n_sample_ind = np.random.randint(len(clus_dic[largest]))\n",
    "    \n",
    "    sample = clus_dic['Seed']\n",
    "    for i in range(len(sens_params)):\n",
    "        sample[sens_params[i] -1] = clus_dic[largest][sample_ind][i]\n",
    "    # returns one sample of largest partition and its pair\n",
    "    return np.array([sample]),clus_dic[largest][n_sample_ind]\n",
    "\n",
    "\n",
    "def local_sample_select(clus_dic, sens_params):\n",
    "      \n",
    "    k_1 = min(list(clus_dic.keys())[1:])\n",
    "    k_2 = max(list(clus_dic.keys())[1:])\n",
    "    \n",
    "    sample_ind = np.random.randint(len(clus_dic[k_1]))\n",
    "    n_sample_ind = np.random.randint(len(clus_dic[k_2]))\n",
    "\n",
    "    sample = clus_dic['Seed']\n",
    "    for i in range(len(sens_params)):\n",
    "        sample[sens_params[i] -1] = clus_dic[k_1][sample_ind][i]\n",
    "    return np.array([sample]),clus_dic[k_2][n_sample_ind]\n",
    "    \n",
    "\n",
    "def clustering(probs,m_sample, sens_params, epsillon):\n",
    "    cluster_dic = {}\n",
    "    cluster_dic['Seed'] = m_sample[0][0]\n",
    "    bins= np.arange(0, 1, epsillon )\n",
    "    digitized = np.digitize(probs, bins) - 1\n",
    "    for  k in range(len(digitized)):\n",
    "\n",
    "        if digitized[k] not in cluster_dic.keys():        \n",
    "            cluster_dic[digitized[k]]=[ [m_sample[k][0][j - 1] for j in sens_params]]\n",
    "        else:\n",
    "            cluster_dic[digitized[k]].append( [m_sample[k][0][j - 1] for j in sens_params])\n",
    "    return cluster_dic \n",
    "    \n",
    "def pred_prob(sess, x, preds, m_sample, input_shape):\n",
    "        probs = model_prediction(sess, x, preds, np.array(m_sample).reshape(len(m_sample),\n",
    "                                                                input_shape[1]))[:,1:2].reshape(len(m_sample))\n",
    "        return probs\n",
    "\n",
    "def sh_entropy(probs,bin_thresh, base=2 ):\n",
    "    bins = np.arange(0, 1,bin_thresh )\n",
    "    digitized = np.digitize(probs, bins)\n",
    "    value,counts = np.unique(digitized, return_counts=True)   \n",
    "    return entropy(counts, base=base)\n",
    "     \n",
    "    \n",
    "def dnn_fair_testing(dataset, sens_params, model_path, cluster_num, \n",
    "                     max_global, max_local, max_iter, epsillon, timeout):\n",
    "    \"\"\"\n",
    "    \n",
    "    The implementation of ADF\n",
    "    :param dataset: the name of testing dataset\n",
    "    :param sensitive_param: the index of sensitive feature\n",
    "    :param model_path: the path of testing model\n",
    "    :param cluster_num: the number of clusters to form as well as the number of\n",
    "            centroids to generate\n",
    "    :param max_global: the maximum number of samples for global search\n",
    "    :param max_local: the maximum number of samples for local search\n",
    "    :param max_iter: the maximum iteration of global perturbation\n",
    "    \"\"\"\n",
    "    data = {\"census\":census_data, \"credit\":credit_data, \"bank\":bank_data, \"compas\":compas_data, \n",
    "            \"default\": default_data, \"heart\":heart_data, \"diabetes\":diabetes_data, \n",
    "            \"students\":students_data, \"meps15\":meps15_data, \"meps16\":meps16_data}\n",
    "    data_config = {\"census\":census, \"credit\":credit, \"bank\":bank, \"compas\":compas, \"default\":default,\n",
    "                  \"heart\":heart , \"diabetes\":diabetes,\"students\":students, \"meps15\":meps15, \"meps16\":meps16}\n",
    "    # prepare the testing data and model\n",
    "    X, Y, input_shape, nb_classes = data[dataset]()\n",
    "    tf.set_random_seed(1234)\n",
    "\n",
    "    config = tf.ConfigProto(device_count = {'GPU': 0})\n",
    "    config.allow_soft_placement= True\n",
    "\n",
    "    sess = tf.Session(config=config)\n",
    "    x = tf.placeholder(tf.float32, shape=input_shape)\n",
    "    y = tf.placeholder(tf.float32, shape=(None, nb_classes))\n",
    "    model = dnn(input_shape, nb_classes)   \n",
    "\n",
    "    preds = model(x)\n",
    "    saver = tf.train.Saver()\n",
    "    model_path = model_path + dataset + \"/test.model\"\n",
    "    saver.restore(sess, model_path)\n",
    "\n",
    "    # construct the gradient graph\n",
    "    grad_0 = gradient_graph(x, preds)\n",
    "\n",
    "    # build the clustering model\n",
    "    clf = cluster(dataset, cluster_num)\n",
    "    clusters = [np.where(clf.labels_ == i) for i in range(cluster_num)]\n",
    "\n",
    "    # store the result of fairness testing\n",
    "   \n",
    "    global max_k\n",
    "    global start_time\n",
    "    global max_k_time\n",
    "\n",
    "    print(dataset, sens_params)\n",
    "    RQ2_table = []\n",
    "    RQ1_table = []\n",
    "    for trial in range(2):\n",
    "        print('Trial', trial)\n",
    "        if  sess._closed:\n",
    "            sess = tf.Session(config=config)\n",
    "            sess = tf.Session(config=config)\n",
    "            x = tf.placeholder(tf.float32, shape=input_shape)\n",
    "            y = tf.placeholder(tf.float32, shape=(None, nb_classes))\n",
    "            model = dnn(input_shape, nb_classes)   \n",
    "            preds = model(x)\n",
    "            saver = tf.train.Saver()\n",
    "            saver.restore(sess, model_path)\n",
    "            grad_0 = gradient_graph(x, preds)\n",
    "        global_inputs = set()\n",
    "        tot_inputs =set()\n",
    "        global_inputs_list = []\n",
    "        local_inputs = set()\n",
    "        local_inputs_list = []\n",
    "        seed_num = 0          \n",
    "        max_k_time = 0\n",
    "        max_k_time_list = []\n",
    "        init_k_list  = []\n",
    "        max_k_list = []\n",
    "        #-----------------------\n",
    "        def evaluate_local(inp):\n",
    "\n",
    "            \"\"\"\n",
    "            Evaluate whether the test input after local perturbation is an individual discriminatory instance\n",
    "            :param inp: test input\n",
    "            :return: whether it is an individual discriminatory instance\n",
    "            \"\"\" \n",
    "            global max_k\n",
    "            global max_k_time\n",
    "            global start_time\n",
    "            global time1\n",
    "            result, K ,conf = check_for_error_condition(data_config[dataset], sess, x, preds, inp, \n",
    "                                               sens_params, input_shape, epsillon)    \n",
    "            if K > max_k:\n",
    "                max_k = K \n",
    "                max_k_time = time.time() - start_time\n",
    "\n",
    "            dis_sample =copy.deepcopy(inp.astype('int').tolist())   \n",
    "            for sens in sens_params:\n",
    "                dis_sample[sens - 1] = 0    \n",
    "            if tuple(dis_sample) not in global_inputs and\\\n",
    "                                    tuple(dis_sample) not in local_inputs:\n",
    "\n",
    "                local_inputs.add(tuple(dis_sample))\n",
    "                local_inputs_list.append(dis_sample + [time.time() - time1])\n",
    "\n",
    "            return (-1 * result)\n",
    "\n",
    "        # select the seed input for fairness testing\n",
    "        inputs = seed_test_input(clusters, min(max_global, len(X)))\n",
    "        global time1\n",
    "        time1 = time.time()  \n",
    "        for num in range(len(inputs)):\n",
    "            \n",
    "            #clear_output(wait=True)\n",
    "            start_time = time.time()\n",
    "            if time.time()-time1 > timeout:\n",
    "                break \n",
    "            print('Input ',seed_num)\n",
    "            index = inputs[num]\n",
    "            sample = X[ index : index + 1]\n",
    "\n",
    "            # start global perturbation\n",
    "            for iter in range( max_iter + 1 ):            \n",
    "                if time.time()-time1 > timeout :\n",
    "                    break\n",
    "                m_sample = m_instance( np.array(sample) , sens_params, data_config[dataset] )\n",
    "                pred = pred_prob( sess, x, preds, m_sample , input_shape )\n",
    "                clus_dic = clustering( pred, m_sample, sens_params, epsillon )\n",
    "                #entropy_min = round(np.log2(len(clus_dic)-1 ),2)#sh_entropy(pred, ent_tresh)\n",
    "                #entropy_sh = sh_entropy(pred, epsillon)\n",
    "                if iter == 0:\n",
    "                    init_k = len(clus_dic) - 1\n",
    "                    max_k = init_k\n",
    "                    #max_k_time = round((time.time() - start_time),4)\n",
    "\n",
    "                if len(clus_dic) - 1 > max_k:\n",
    "                    max_k = len(clus_dic) - 1\n",
    "                    max_k_time = round((time.time() - start_time),4)\n",
    "\n",
    "                sample,n_values = global_sample_select( clus_dic, sens_params )\n",
    "                dis_sample = sample.copy()\n",
    "                for sens in sens_params:\n",
    "                    dis_sample[0][sens  - 1] = 0\n",
    "\n",
    "\n",
    "                if tuple(dis_sample[0].astype('int')) not in global_inputs and\\\n",
    "                                    tuple(dis_sample[0].astype('int')) not in local_inputs:\n",
    "                    dis_flag = True\n",
    "                    global_inputs.add(tuple(dis_sample[0].astype('int')))\n",
    "                    global_inputs_list.append(dis_sample[0].astype('int').tolist())\n",
    "\n",
    "                else:\n",
    "                    dis_flag = False\n",
    "\n",
    "\n",
    "                if dis_flag and (len(clus_dic)-1 >= 2):    \n",
    "\n",
    "                    loc_x,n_values = local_sample_select(clus_dic ,sens_params )                              \n",
    "                    minimizer = {\"method\": \"L-BFGS-B\"}\n",
    "                    local_perturbation = Local_Perturbation(sess, grad_0, x, n_values, \n",
    "                                                            sens_params, input_shape[1], \n",
    "                                                            data_config[dataset])               \n",
    "                    basinhopping(evaluate_local, loc_x, stepsize = 1.0, \n",
    "                                 take_step = local_perturbation, minimizer_kwargs = minimizer, \n",
    "                                 niter = max_local)\n",
    "\n",
    "\n",
    "                if dis_flag :\n",
    "                    global_inputs_list[-1] +=  [time.time() - time1]\n",
    "\n",
    "\n",
    "                clus_dic = {}\n",
    "                if iter == max_iter:\n",
    "                    break\n",
    "\n",
    "                #Making up n_sample\n",
    "                n_sample = sample.copy()\n",
    "                for i in range(len(sens_params)):\n",
    "                    n_sample[0][sens_params[i] - 1] = n_values[i]                \n",
    "\n",
    "                # global perturbation\n",
    "\n",
    "                s_grad = sess.run(tf.sign(grad_0), feed_dict = {x: sample})\n",
    "                n_grad = sess.run(tf.sign(grad_0), feed_dict = {x: n_sample})\n",
    "\n",
    "                # find the feature with same impact\n",
    "                if np.zeros(data_config[dataset].params).tolist() == s_grad[0].tolist():\n",
    "                    g_diff = n_grad[0]\n",
    "                elif np.zeros(data_config[dataset].params).tolist() == n_grad[0].tolist():\n",
    "                    g_diff = s_grad[0]\n",
    "                else:\n",
    "                    g_diff = np.array(s_grad[0] == n_grad[0], dtype = float)                \n",
    "                for sens in sens_params:\n",
    "                    g_diff[sens - 1] = 0 \n",
    "#                         g_diff1[sens - 1] = 0\n",
    "\n",
    "\n",
    "                cal_grad = s_grad * g_diff\n",
    "#                     cal_grad1 = np.array([grad_m_sign[np.random.randint(len(grad_m_sign))] * g_diff1])\n",
    "#                     if np.zeros(input_shape[1]).tolist() == cal_grad1.tolist()[0]:\n",
    "#                         index = np.random.randint(len(cal_grad1[0]) - 1)\n",
    "#                         for i in range(len(sens_params) - 1, -1, -1):\n",
    "#                             if index == sens_params[i] - 1 :\n",
    "#                                 index = index + 1\n",
    "#                         cal_grad1[0][index]  = np.random.choice([1.0, -1.0])\n",
    "\n",
    "                if np.zeros(input_shape[1]).tolist() == cal_grad.tolist()[0]:\n",
    "                    index = np.random.randint(len(cal_grad[0]) - 1)\n",
    "                    for i in range(len(sens_params) - 1, -1, -1):\n",
    "                        if index == sens_params[i] - 1 :\n",
    "                            index = index + 1\n",
    "\n",
    "                    cal_grad[0][index]  = np.random.choice([1.0, -1.0])\n",
    "\n",
    "                sample[0] = clip(sample[0] + perturbation_size * cal_grad[0], data_config[dataset]).astype(\"int\")\n",
    "\n",
    "            seed_num += 1\n",
    "            if max_k > 1:\n",
    "                max_k_time_list.append(max_k_time)\n",
    "                init_k_list.append(init_k)\n",
    "                max_k_list.append(max_k)\n",
    "            \n",
    "        print('Search Time =', time.time()-time1)\n",
    "\n",
    "        # create the folder for storing the fairness testing result\n",
    "        if not os.path.exists('../results/'):\n",
    "            os.makedirs('../results/')\n",
    "        if not os.path.exists('../results/' + dataset + '/'):\n",
    "            os.makedirs('../results/' + dataset + '/')\n",
    "        if not os.path.exists('../results/' + dataset + '/OurTool/'):\n",
    "            os.makedirs('../results/' + dataset + '/OurTool/')\n",
    "        if not os.path.exists('../results/' + dataset + '/OurTool/RQ1&2/'):\n",
    "            os.makedirs('../results/' + dataset + '/OurTool/RQ1&2/')\n",
    "        if not os.path.exists('../results/' + dataset + '/OurTool/RQ1&2/' + ''.join(str(i) for i in sens_params)+'_10runs/'):\n",
    "            os.makedirs('../results/' + dataset + '/OurTool/RQ1&2/' + ''.join(str(i) for i in sens_params)+'_10runs/')\n",
    "        # storing the fairness testing result\n",
    "        np.save('../results/' + dataset + '/OurTool/RQ1&2/'+ ''.join(str(i) for i in sens_params)+'_10runs'+'/global_inputs_'+str(trial)+'.npy', \n",
    "                np.array(global_inputs_list))\n",
    "        np.save('../results/' + dataset + '/OurTool/RQ1&2/'+ ''.join(str(i) for i in sens_params)+'_10runs'+'/local_inputs_'+str(trial)+'.npy', \n",
    "                np.array(local_inputs_list))\n",
    "        total_inputs = np.concatenate((local_inputs_list,global_inputs_list), axis=0)\n",
    "        np.save('../results/' + dataset + '/OurTool/RQ1&2/'+ ''.join(str(i) for i in sens_params)+'_10runs'+'/total_inputs_'+str(trial)+'.npy',\n",
    "                total_inputs)\n",
    "        # RQ1 & RQ2\n",
    "\n",
    "        local_sam = np.array(local_inputs_list).astype('int32')\n",
    "        global_sam = np.array(global_inputs_list).astype('int32')\n",
    "        # Storing result for RQ1 table\n",
    "        print('Analyzing the search results....')\n",
    "\n",
    "        with open('../results/' + dataset + '/OurTool/RQ1&2/'+ ''.join(str(i) for i in sens_params)+'_10runs'+'/global_inputs_msamples_'+str(trial)+'.csv', 'w') as f:\n",
    "            writer = csv.writer(f)\n",
    "            for ind in range(len(global_inputs_list)):\n",
    "                m_sample = m_instance( np.array([global_inputs_list[ind][:input_shape[1]]]) , sens_params, data_config[dataset] ) \n",
    "                rows = m_sample.reshape((len(m_sample),input_shape[1]))\n",
    "                writer.writerows(np.append(rows,[[global_inputs_list[ind][-1]] for i in range(len(m_sample))],axis=1))\n",
    "\n",
    "        with open('../results/' + dataset + '/OurTool/RQ1&2/'+ ''.join(str(i) for i in sens_params)+'_10runs'+'/local_inputs_msamples_'+str(trial)+'.csv', 'w') as f:\n",
    "            writer = csv.writer(f)\n",
    "            for ind in range(len(local_inputs_list)):\n",
    "                m_sample = m_instance( np.array([local_inputs_list[ind][:input_shape[1]]]) , sens_params, data_config[dataset] ) \n",
    "                rows = m_sample.reshape((len(m_sample),input_shape[1]))\n",
    "                writer.writerows(np.append(rows,[[local_inputs_list[ind][-1]] for i in range(len(m_sample))],axis=1))\n",
    "        \n",
    "        df_l = pd.read_csv('../results/' + dataset + '/OurTool/RQ1&2/'+ ''.join(str(i) for i in sens_params)+'_10runs'+'/local_inputs_msamples_'+str(trial)+'.csv',header=None)  \n",
    "        df_g = pd.read_csv('../results/' + dataset + '/OurTool/RQ1&2/'+ ''.join(str(i) for i in sens_params)+'_10runs'+'/global_inputs_msamples_'+str(trial)+'.csv',header=None)\n",
    "\n",
    "        df_g['label'] = model_argmax(sess, x, preds, df_g.to_numpy()[:,:input_shape[1]])\n",
    "        df_l['label'] = model_argmax(sess, x, preds, df_l.to_numpy()[:,:input_shape[1]])\n",
    "        g_pivot = pd.pivot_table(df_g, values=\"label\", index=list(np.setxor1d(df_g.columns[:-1] ,\n",
    "                                                                              np.array(sens_params)-1)), aggfunc=np.sum)\n",
    "        l_pivot = pd.pivot_table(df_l, values=\"label\", index=list(np.setxor1d(df_l.columns[:-1] ,\n",
    "                                                                           np.array(sens_params)-1)), aggfunc=np.sum)\n",
    "\n",
    "        g_time = g_pivot.index[np.where((g_pivot['label'] > 0) & (g_pivot['label'] < len(m_sample)))[0]].get_level_values(input_shape[1]).values\n",
    "        l_time = l_pivot.index[np.where((l_pivot['label'] > 0) & (l_pivot['label'] < len(m_sample)))[0]].get_level_values(input_shape[1]).values\n",
    "        tot_time = np.sort(np.concatenate((l_time, g_time), axis=0 ))\n",
    "        print('Time to 1st ID',tot_time[0])   \n",
    "        print('time to 1000 ID',tot_time[999])\n",
    "        \n",
    "        g_dis = (len(m_sample) - g_pivot.loc[(g_pivot['label'] > 0) & \\\n",
    "                                             (g_pivot['label'] < len(m_sample))]['label'].to_numpy()).sum()\n",
    "        l_dis = (len(m_sample) - l_pivot.loc[(l_pivot['label'] > 0) & \\\n",
    "                                             (l_pivot['label'] < len(m_sample))]['label'].to_numpy()).sum()\n",
    "\n",
    "        g_dis_adf = len(g_time)\n",
    "        l_dis_adf = len(l_time)\n",
    "\n",
    "\n",
    "        global_succ_adf = round((g_dis_adf / len(global_sam)) * 100, 1)\n",
    "        local_succ_adf  = round((l_dis_adf / len(local_sam)) * 100, 1)\n",
    "        global tot_df\n",
    "        tot_df = pd.DataFrame(total_inputs)\n",
    "        tot_df.columns=[i for i in range(input_shape[1])] + ['time']\n",
    "        \n",
    "        #tot_df = tot_df[[i for i in range(input_shape[1])] + [tot_df.columns[-2]]]\n",
    "        #columns = list(tot_df.columns)\n",
    "        #columns[-1] = 'sh_entropy'\n",
    "        #tot_df.columns = columns\n",
    "        k = []\n",
    "        disc = []\n",
    "        tot_df['sh_entropy'] = 0\n",
    "        for sam_ind in range(total_inputs.shape[0]): \n",
    "            \n",
    "            m_sample = m_instance( np.array([total_inputs[:,:input_shape[1]][sam_ind]]) , sens_params, data_config[dataset] )\n",
    "            pred = pred_prob( sess, x, preds, m_sample , input_shape )\n",
    "            clus_dic = clustering( pred, m_sample, sens_params, epsillon )\n",
    "            tot_df.loc[[sam_ind], 'sh_entropy'] = sh_entropy(pred, epsillon)\n",
    "            if pred.max() > 0.5 and  pred.min()< 0.5:\n",
    "                disc.append(1)\n",
    "            else:\n",
    "                disc.append(0)\n",
    "            k.append(len(clus_dic) - 1)\n",
    "            \n",
    "        tot_df['k'] = k\n",
    "        time_K_greater_than_1 = tot_df['time'][np.where(tot_df['k']>1)[0][0]]\n",
    "        tot_df['disc'] = disc\n",
    "        tot_df['min_entropy'] = round(np.log2(tot_df['k'] ),2)\n",
    "        tot_dis = tot_df.loc[tot_df['disc'] == 1]\n",
    "#         min_entropy.append( np.mean(tot_dis['min_entropy']))\n",
    "#         k_mean.append(np.mean(tot_dis['k']))\n",
    "#         sh_entropy_mean.append(np.mean(tot_dis['sh_entropy']))\n",
    "        tot_dis.to_csv('../results/' + dataset + '/OurTool/RQ1&2/'+ ''.join(str(i) for i in sens_params)+'_10runs'+'/total_disc_' + str(trial)+'.csv')\n",
    "        # reseting the TF graph for the next round\n",
    "        sess.close()\n",
    "        tf.reset_default_graph()\n",
    "        haighest_k = np.sort(tot_df['k'].unique())[-3:]\n",
    "        if len(haighest_k)>2:\n",
    "            IK1F = np.where(tot_df['k']==haighest_k[2])[0].shape[0]\n",
    "            IK2F = np.where(tot_df['k']==haighest_k[1])[0].shape[0]\n",
    "            IK3F = np.where(tot_df['k']==haighest_k[0])[0].shape[0]\n",
    "\n",
    "        else:\n",
    "            IK1F = np.where(tot_df['k']==haighest_k[1])[0].shape[0]\n",
    "            IK2F = np.where(tot_df['k']==haighest_k[0])[0].shape[0]\n",
    "            IK3F = 0\n",
    "\n",
    "            \n",
    "        print('Global ID RQ1 =', g_dis)\n",
    "        print('local  ID RQ1  =',  l_dis)\n",
    "        print('Total loc samples  = ', len(local_sam)) \n",
    "        print('Total glob samples = ', len(global_sam)) \n",
    "        print('Total ID = ',g_dis + l_dis)\n",
    "        print('Total ID RQ2  = ',g_dis_adf + l_dis_adf)\n",
    "        print('Global ID RQ2  = ',g_dis_adf)\n",
    "        print('Local  ID RQ2  = ',l_dis_adf)\n",
    "\n",
    "        global_succ = round( g_dis / (len(global_sam) * \\\n",
    "                             len(m_sample)) * 100,1)\n",
    "        local_succ = round(l_dis  / (len(local_sam) * \\\n",
    "                             len(m_sample)) * 100,1)\n",
    "\n",
    "\n",
    "#         df_RQ1 = pd.DataFrame(total_inputs)\n",
    "#         df_RQ1.columns = [i for i in range(input_shape[1])] + ['init_k', 'max_k', 'max_k_time', 'sample_time']\n",
    "        #row = [len(total_inputs)] + list(df_RQ1[['init_k', 'max_k', 'max_k_time',]].mean()+[time_K_greater_than_1])\n",
    "        row = [len(total_inputs)] + [np.mean(init_k_list), np.mean(max_k_list),np.mean(max_k_time_list)] + list(tot_dis[['min_entropy', 'sh_entropy']].mean()) + [time_K_greater_than_1] + [IK1F, IK2F,IK3F] \n",
    "        RQ1_table.append(row)\n",
    "        RQ2_table.append([g_dis_adf, l_dis_adf, g_dis_adf + l_dis_adf ,local_succ_adf, tot_time[0], tot_time[999] ])\n",
    "        print('Local search success rate  = ', local_succ, '%')\n",
    "        print('Global search success rate = ', global_succ, '%')\n",
    "        print('Local search ADF success rate  = ', local_succ_adf, '%')\n",
    "        print('Global search ADF success rate = ', global_succ_adf, '%')\n",
    "        \n",
    "    np.save('../results/' + dataset + '/OurTool/RQ1&2/'+ ''.join(str(i) for i in sens_params)+'_10runs'+'/QID_RQ2_10runs.npy',\n",
    "            RQ2_table)\n",
    "    np.save('../results/' + dataset + '/OurTool/RQ1&2/'+ ''.join(str(i) for i in sens_params)+'_10runs'+'/QID_RQ1_10runs.npy',\n",
    "            RQ1_table)\n",
    "    with open('../results/' + dataset + '/OurTool/RQ1&2/'+ ''.join(str(i) for i in sens_params)+'_10runs'+'/RQ2_table.csv', 'w') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(['g_adf_disc', 'l_adf_disc','tot_adf_disc','local_succ_adf',\n",
    "                             'time_to_first','time_to_1000'])\n",
    "            writer.writerow(np.mean(RQ2_table,axis=0))\n",
    "            writer.writerow(np.std(RQ2_table,axis=0))\n",
    "\n",
    "    with open('../results/' + dataset + '/OurTool/RQ1&2/'+ ''.join(str(i) for i in sens_params)+'_10runs'+'/RQ1_table.csv', 'w') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(['tot_samples','init_k', 'max_k', 'max_k_time','min_entropy',\n",
    "                             'sh_entropy','time_K_greater_than_1', 'IK1F', 'IK2F', 'IK3F'])\n",
    "            writer.writerow(np.mean(RQ1_table,axis=0))\n",
    "            writer.writerow(np.std(RQ1_table,axis=0))\n",
    "\n",
    "#     with open('../results/' + dataset + '/OurTool/RQ3/entropy.csv','w') as f:\n",
    "#         writer = csv.writer(f)\n",
    "#         writer.writerow(['k_mean','min_entropy','sh_entropy'])\n",
    "#         writer.writerow([np.mean(k_mean),np.mean(min_entropy), np.mean(sh_entropy_mean) ])\n",
    "def main(argv=None):\n",
    "    for data_set in [\"credit\"]:#, \"bank\",  \"default\", \"diabetes\"]:#\"heart\" ,\"students\", \"meps15\", \"meps16\"]:\n",
    "        if data_set == 'credit': sens_p = [9]\n",
    "        elif data_set == 'census': sens_p = [9,8,1]\n",
    "        elif data_set == 'bank': sens_p = [1]\n",
    "        elif data_set == 'compas': sens_p = [3,2,1]\n",
    "        elif data_set == 'default': sens_p = [5,2]\n",
    "        elif data_set == 'heart': sens_p = [2,1]\n",
    "        elif data_set == 'diabetes': sens_p = [8]\n",
    "        elif data_set == 'students': sens_p = [3,2]\n",
    "        elif data_set == 'meps15': sens_p = [10,2,1]\n",
    "        elif data_set == 'meps16': sens_p = [10,2,1]\n",
    "    \n",
    "        for sens in sens_p:\n",
    "\n",
    "            dnn_fair_testing(dataset = data_set, \n",
    "                             sens_params = [sens],\n",
    "                             model_path  = FLAGS.model_path,\n",
    "                             cluster_num = FLAGS.cluster_num,\n",
    "                             max_global  = FLAGS.max_global,\n",
    "                             max_local   = FLAGS.max_local,\n",
    "                             max_iter    = FLAGS.max_iter,\n",
    "                             epsillon    = FLAGS.epsillon,\n",
    "                             timeout    = FLAGS.timeout\n",
    "                            )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    flags.DEFINE_string(\"dataset\", \"census\", \"the name of dataset\")\n",
    "    flags.DEFINE_string('model_path', '../models/', 'the path for testing model')\n",
    "    flags.DEFINE_integer('cluster_num', 4, 'the number of clusters to form as well as the number of centroids to generate')\n",
    "    flags.DEFINE_integer('max_global', 1000, 'maximum number of samples for global search')\n",
    "    flags.DEFINE_integer('max_local', 1000, 'maximum number of samples for local search')\n",
    "    flags.DEFINE_integer('max_iter', 10, 'maximum iteration of global perturbation')\n",
    "    flags.DEFINE_integer('timeout',90 , 'search timeout')\n",
    "    #if result for RQ1 table: set this to [9,8,1], if result for RQ2 table: set this one sensitive attribute each time \n",
    "    # e.g. for census dataset, set [9], [8], [1] for sex, race and age respectively\n",
    "    flags.DEFINE_list('sens_params', [9,8,1], 'sensitive parameters index.1 for age, 9 for gender, 8 for race')\n",
    "    flags.DEFINE_float('epsillon', 0.025, 'the value of epsillon for partitioning')\n",
    "    tf.app.run()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tot_dis' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a63d877e442c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtot_dis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tot_dis' is not defined"
     ]
    }
   ],
   "source": [
    "tot_dis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 8, 27)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IK1F, IK2F,IK3F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
